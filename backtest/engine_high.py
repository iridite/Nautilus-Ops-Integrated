import gc
import json
import logging
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
from nautilus_trader.backtest.config import (
    BacktestDataConfig,
    BacktestRunConfig,
    BacktestVenueConfig,
    ImportableFeeModelConfig,
)
from nautilus_trader.backtest.node import (
    BacktestEngineConfig,
    BacktestNode,
)
from nautilus_trader.backtest.results import BacktestResult
from nautilus_trader.common.config import LoggingConfig
from nautilus_trader.config import ImportableStrategyConfig, RiskEngineConfig
from nautilus_trader.model.data import Bar, BarType
from nautilus_trader.model.instruments import Instrument
from nautilus_trader.persistence.catalog import ParquetDataCatalog
from nautilus_trader.persistence.loaders import CSVBarDataLoader
from nautilus_trader.persistence.wranglers import BarDataWrangler
from pandas import DataFrame

from backtest.exceptions import (
    BacktestEngineError,
    CatalogError,
    DataLoadError,
    InstrumentLoadError,
)
from core.schemas import BacktestConfig
from strategy.core.loader import (
    filter_strategy_params,
    load_strategy_config_class,
)
from utils.data_file_checker import check_single_data_file
from utils.filename_parser import FilenameParser
from utils.instrument_loader import load_instrument
from utils.oi_funding_adapter import OIFundingDataLoader

logger = logging.getLogger(__name__)



# ============================================================
# æ•°æ®åŠ è½½æ¨¡å—
# ============================================================

# ============================================================
# æ•°æ®åŠ è½½æ¨¡å—
# ============================================================

def _load_instruments(cfg: BacktestConfig, base_dir: Path) -> Dict[str, Instrument]:
    """
    åŠ è½½äº¤æ˜“æ ‡çš„ä¿¡æ¯ï¼ˆå¸¦æ•°æ®å¯ç”¨æ€§æ£€æŸ¥ï¼‰

    Args:
        cfg: å›æµ‹é…ç½®
        base_dir: é¡¹ç›®æ ¹ç›®å½•

    Returns:
        Dict[str, Instrument]: instrument_id -> Instrument çš„æ˜ å°„

    Raises:
        InstrumentLoadError: å½“æ ‡çš„åŠ è½½å¤±è´¥æ—¶
    """
    loaded_instruments = {}
    inst_cfg_map = {ic.instrument_id: ic for ic in cfg.instruments}

    # è¿‡æ»¤æœ‰æ•°æ®çš„æ ‡çš„
    instruments_with_data = []

    if cfg.start_date and cfg.end_date:
        for inst_id, inst_cfg in inst_cfg_map.items():
            # æå–ç¬¦å·ç”¨äºæ•°æ®æ–‡ä»¶æ£€æŸ¥
            symbol = inst_id.split("-")[0] if "-" in inst_id else inst_id.split(".")[0]

            # æ„å»ºæ—¶é—´å‘¨æœŸå­—ç¬¦ä¸²
            if cfg.data_feeds:
                first_feed = cfg.data_feeds[0]
                from nautilus_trader.model.enums import BarAggregation
                unit_map = {
                    BarAggregation.MINUTE: "m",
                    BarAggregation.HOUR: "h",
                    BarAggregation.DAY: "d"
                }
                timeframe = f"{first_feed.bar_period}{unit_map.get(first_feed.bar_aggregation, 'h')}"
            else:
                timeframe = "1h"

            # æ£€æŸ¥ä¸»æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            has_data, _ = check_single_data_file(
                symbol=symbol,
                start_date=cfg.start_date,
                end_date=cfg.end_date,
                timeframe=timeframe,
                exchange=inst_cfg.venue_name.lower(),
                base_dir=base_dir,
            )

            if has_data:
                instruments_with_data.append(inst_id)
            else:
                logger.debug(f"â­ï¸ Skipping {inst_id}: no data file")

        if not instruments_with_data:
            raise InstrumentLoadError("No instruments with available data found", "all")

        logger.info(f"ğŸ“Š Found {len(instruments_with_data)}/{len(inst_cfg_map)} instruments with data")
    else:
        instruments_with_data = list(inst_cfg_map.keys())
        logger.warning("âš ï¸ start_date æˆ– end_date æœªé…ç½®ï¼Œè·³è¿‡æ•°æ®å¯ç”¨æ€§æ£€æŸ¥")

    for inst_id in instruments_with_data:
        inst_cfg = inst_cfg_map[inst_id]
        inst_path = inst_cfg.get_json_path()

        if not inst_path.exists():
            raise InstrumentLoadError(
                f"Instrument path not found: {inst_path}", inst_id
            )

        try:
            loaded_instruments[inst_id] = load_instrument(inst_path)
        except Exception as e:
            raise InstrumentLoadError(
                f"Failed to load instrument {inst_id}: {e}", inst_id, e
            )

# ============================================================
# æ•°æ®éªŒè¯æ¨¡å—
# ============================================================


    return loaded_instruments


def _check_parquet_coverage(
    catalog: ParquetDataCatalog,
    bar_type: BarType,
    cfg: BacktestConfig,
) -> Tuple[bool, float]:
    """
    æ£€æŸ¥Parquetæ•°æ®è¦†ç›–ç‡

    Returns:
        Tuple[bool, float]: (æ˜¯å¦å­˜åœ¨, è¦†ç›–ç‡ç™¾åˆ†æ¯”)
    """
    try:
        existing_intervals = catalog.get_intervals(
            data_cls=Bar, identifier=str(bar_type)
        )
        if not existing_intervals:
            return False, 0.0
    except Exception:
        return False, 0.0


# ============================================================
# Parquet æ•°æ®å¤„ç†æ¨¡å—
# ============================================================

    # ç®€åŒ–é€»è¾‘ï¼šå¦‚æœ Parquet æ•°æ®å­˜åœ¨åˆ™è¿”å› True
    return True, 100.0


def _handle_parquet_exists(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    inst: Instrument,
    bar_type: BarType,
    data_cfg,
    cfg: BacktestConfig,
    feed_idx: int,
    total_feeds: int,
) -> str:
    """
    å¤„ç†Parquetæ•°æ®å·²å­˜åœ¨çš„æƒ…å†µ

    Returns:
        str: çŠ¶æ€æ¶ˆæ¯ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    """
    csv_exists = csv_path.exists()

    if csv_exists:
        is_consistent = _verify_data_consistency(csv_path, catalog, bar_type)

        if not is_consistent:
            try:
                catalog_loader(catalog, csv_path, inst, bar_type)
                return f"\rğŸ“– [{feed_idx:3d}/{total_feeds}] Updated: {str(bar_type.instrument_id):<32}"
            except Exception:
                return f"\râš ï¸  [{feed_idx:3d}/{total_feeds}] Using Parquet: {str(bar_type.instrument_id):<28}"
        else:
            return f"\râ© [{feed_idx:3d}/{total_feeds}] Verified: {str(bar_type.instrument_id):<35}"
    else:
        # æ£€æŸ¥Parquetè¦†ç›–ç‡
        parquet_exists, coverage_pct = _check_parquet_coverage(catalog, bar_type, cfg)

        if parquet_exists and coverage_pct < 95.0:
            # æ•°æ®ä¸å®Œæ•´ï¼Œè§¦å‘è‡ªåŠ¨ä¸‹è½½
            download_success = _auto_download_missing_data(data_cfg, cfg)

            if download_success and csv_path.exists():
                try:
                    catalog_loader(catalog, csv_path, inst, bar_type)
                    return f"\rğŸ“– [{feed_idx:3d}/{total_feeds}] Completed: {str(bar_type.instrument_id):<32}"
                except Exception:
                    return f"\râš ï¸  [{feed_idx:3d}/{total_feeds}] Partial Parquet: {str(bar_type.instrument_id):<25} ({coverage_pct:.1f}%)"
            else:
                return f"\râš ï¸  [{feed_idx:3d}/{total_feeds}] Partial Parquet: {str(bar_type.instrument_id):<25} ({coverage_pct:.1f}%)"
        else:
            # å°è¯•å¸¸è§„ä¸‹è½½
            download_success = _auto_download_missing_data(data_cfg, cfg)

            if download_success and csv_path.exists():
                try:
                    catalog_loader(catalog, csv_path, inst, bar_type)
                    return f"\rğŸ“– [{feed_idx:3d}/{total_feeds}] Imported: {str(bar_type.instrument_id):<32}"
                except Exception:
                    return f"\râš ï¸  [{feed_idx:3d}/{total_feeds}] Using Parquet: {str(bar_type.instrument_id):<28}"
            else:
                return f"\râš ï¸  [{feed_idx:3d}/{total_feeds}] Partial Parquet: {str(bar_type.instrument_id):<25}"


def _handle_parquet_missing(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    inst: Instrument,
    bar_type: BarType,
    data_cfg,
    cfg: BacktestConfig,
    feed_idx: int,
    total_feeds: int,
) -> str:
    """
    å¤„ç†Parquetæ•°æ®ä¸å­˜åœ¨çš„æƒ…å†µ

    Returns:
        str: çŠ¶æ€æ¶ˆæ¯

    Raises:
        DataLoadError: å½“æ•°æ®å®Œå…¨ç¼ºå¤±æ—¶
    """
    csv_exists = csv_path.exists()

    if csv_exists:
        try:
            catalog_loader(catalog, csv_path, inst, bar_type)
            return f"\rğŸ“– [{feed_idx:3d}/{total_feeds}] Imported: {str(bar_type.instrument_id):<32}"
        except Exception as e:
            raise DataLoadError(
                f"Error loading {data_cfg.csv_file_name}: {e}", str(csv_path), e
            )
    else:
        download_success = _auto_download_missing_data(data_cfg, cfg)

        if download_success and csv_path.exists():
            try:
                catalog_loader(catalog, csv_path, inst, bar_type)
                return f"\rğŸ“– [{feed_idx:3d}/{total_feeds}] Imported: {str(bar_type.instrument_id):<32}"
            except Exception:
                pass

        # æ•°æ®å®Œå…¨ç¼ºå¤±
        raise DataLoadError(
            f"Critical data missing for {bar_type.instrument_id}. "
            f"CSV file not found: {csv_path}, Parquet data not available, "
            f"and auto-download failed.",
            str(csv_path),
        )


def _auto_download_data(data_cfg, cfg: BacktestConfig) -> bool:
    """
    è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ•°æ®æ–‡ä»¶

    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    try:
        from utils.data_management.data_retrieval import batch_fetch_ohlcv

        parsed = FilenameParser.parse(data_cfg.csv_file_name)
        if not parsed:
            return False

        base_dir = Path(__file__).parent.parent
        configs = batch_fetch_ohlcv(
            symbols=[parsed.symbol],
            start_date=parsed.start_date,
            end_date=parsed.end_date,
            timeframe=parsed.timeframe,
            exchange_id=parsed.exchange,
            base_dir=base_dir,
        )

        if configs:
            csv_path = data_cfg.full_path
            return csv_path.exists() and csv_path.stat().st_size > 1024

        return False
    except Exception:
        return False


def _import_data_to_catalog(
    cfg: BacktestConfig,
    loaded_instruments: Dict[str, Instrument],
    catalog_path: Path,
) -> Tuple[Dict[str, Dict], Dict[str, str], Dict]:
    """
    å°†CSVæ•°æ®å¯¼å…¥Parquetç›®å½•ï¼Œå¹¶ç»„ç»‡æ•°æ®æµ

    æ™ºèƒ½æ•°æ®å¤„ç†ï¼šæ£€æŸ¥Parquetè¦†ç›–ç‡ï¼ŒæŒ‰éœ€å¯¼å…¥CSVæˆ–ä¸‹è½½æ•°æ®
    """
    _catalog = ParquetDataCatalog(catalog_path)
    data_config_by_inst = {}
    feeds_by_inst = defaultdict(dict)
    global_feeds = {}
    total_feeds = len(cfg.data_feeds)

    for feed_idx, data_cfg in enumerate(cfg.data_feeds, 1):
        inst_id = data_cfg.instrument_id or (cfg.instrument.instrument_id if cfg.instrument else None)
        if inst_id not in loaded_instruments:
            continue

        inst = loaded_instruments[inst_id]
        csv_path = data_cfg.full_path
        feed_bar_type_str = f"{inst.id}-{data_cfg.bar_type_str}"
        bar_type = BarType.from_str(feed_bar_type_str)

        # æ£€æŸ¥Parquetæ•°æ®è¦†ç›–ç‡
        parquet_exists, coverage_pct = _check_parquet_coverage(_catalog, bar_type, cfg)

        # å¤„ç†æ•°æ®å¯¼å…¥
        if parquet_exists and coverage_pct >= 80:
            status_msg = f"\râœ… [{feed_idx:3d}/{total_feeds}] Ready: {str(bar_type.instrument_id):<35}"
        elif parquet_exists:
            status_msg = _handle_parquet_exists(
                _catalog, csv_path, inst, bar_type, data_cfg, cfg, feed_idx, total_feeds
            )
        else:
            status_msg = _handle_parquet_missing(
                _catalog, csv_path, inst, bar_type, data_cfg, cfg, feed_idx, total_feeds
            )

        sys.stdout.write(status_msg)
        sys.stdout.flush()

        # å½’ç±»æ•°æ®æµ
        if data_cfg.label == "benchmark":
            global_feeds[data_cfg.label] = feed_bar_type_str
        else:
            feeds_by_inst[inst_id][data_cfg.label] = feed_bar_type_str

        # åˆå¹¶bar_types
        inst_id_str = str(inst.id)
        if inst_id_str not in data_config_by_inst:
            data_config_by_inst[inst_id_str] = {
                "instrument_id": inst.id,
                "bar_types": [],
                "catalog_path": str(catalog_path),
            }

        bar_type_str = str(bar_type)
        if bar_type_str not in data_config_by_inst[inst_id_str]["bar_types"]:
            data_config_by_inst[inst_id_str]["bar_types"].append(bar_type_str)

    logger.info(f"\nâœ… Data import complete. Created {len(data_config_by_inst)} data configs.")
    return data_config_by_inst, global_feeds, feeds_by_inst


def _auto_download_missing_data(data_cfg, backtest_cfg: BacktestConfig) -> bool:
    """è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ•°æ®æ–‡ä»¶"""
    return _auto_download_data(data_cfg, backtest_cfg)


def _clear_parquet_data(catalog: ParquetDataCatalog, bar_type: BarType) -> None:
    """
    æ¸…ç†æŒ‡å®šbar_typeçš„Parquetæ•°æ®

    Args:
        catalog: Parquetæ•°æ®ç›®å½•
        bar_type: è¦æ¸…ç†çš„æ•°æ®ç±»å‹æ ‡è¯†
    """
    try:
        import shutil

        # è·å–catalogçš„æ ¹è·¯å¾„
        catalog_root = Path(catalog.path)

        # NautilusTraderå®é™…çš„æ–‡ä»¶ç»“æ„æ˜¯: data/crypto_perpetual/{instrument_id}/
        instrument_id = str(bar_type.instrument_id)

        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ•°æ®ç›®å½•
        possible_dirs = [
            catalog_root / "data" / "crypto_perpetual" / instrument_id,
            catalog_root / "data" / "Bar" / instrument_id,
            catalog_root / "data" / instrument_id,
        ]

        for data_dir in possible_dirs:
            if data_dir.exists():
                logger.info(f"   ğŸ—‘ï¸ Clearing existing Parquet data: {data_dir}")
                shutil.rmtree(data_dir)
                break
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†ç›®å½•ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«instrument_idçš„ä»»ä½•ç›®å½•
            for root, dirs, files in catalog_root.rglob("*"):
                if instrument_id in str(root) and any(
                    f.endswith(".parquet") for f in files
                ):
                    logger.info(f"   ğŸ—‘ï¸ Clearing found Parquet data: {root}")
                    shutil.rmtree(root)
                    break

    except Exception as e:
        logger.warning(f"   âš ï¸ Warning: Could not clear Parquet data: {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºè¿™ä¸æ˜¯è‡´å‘½é”™è¯¯


def _verify_data_consistency(
    csv_path: Path, catalog: ParquetDataCatalog, bar_type: BarType
) -> bool:
    """
    è½»é‡çº§éªŒè¯CSVå’ŒParquetæ•°æ®çš„ä¸€è‡´æ€§

    ç®€åŒ–æ£€æŸ¥ç­–ç•¥ï¼ˆæ€§èƒ½ä¼˜å…ˆï¼‰ï¼š
    1. æ¯”è¾ƒæ–‡ä»¶ä¿®æ”¹æ—¶é—´
    2. æ¯”è¾ƒæ•°æ®è¡Œæ•°ï¼ˆå¿«é€Ÿä¼°ç®—ï¼‰
    3. æ¯”è¾ƒæ–‡ä»¶å¤§å°èŒƒå›´

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        catalog: Parquetæ•°æ®ç›®å½•
        bar_type: æ•°æ®ç±»å‹æ ‡è¯†

    Returns:
        bool: æ•°æ®æ˜¯å¦ä¸€è‡´
    """
    try:
        # 1. æ£€æŸ¥CSVæ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        if not csv_path.exists() or csv_path.stat().st_size < 1024:
            return False

        # 2. å¿«é€Ÿæ£€æŸ¥Parquetæ•°æ®æ˜¯å¦å­˜åœ¨
        try:
            existing_intervals = catalog.get_intervals(
                data_cls=Bar, identifier=str(bar_type)
            )
            if not existing_intervals:
                return False
        except Exception:
            return False

        # 3. æ¯”è¾ƒæ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼ˆå¦‚æœCSVæ¯”Parquetæ–°ï¼Œè®¤ä¸ºä¸ä¸€è‡´ï¼‰
        csv_mtime = csv_path.stat().st_mtime

        # æŸ¥æ‰¾Parquetæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
        catalog_root = Path(catalog.path)
        instrument_id = str(bar_type.instrument_id)
        parquet_dir = catalog_root / "data" / "crypto_perpetual" / instrument_id

        if parquet_dir.exists():
            parquet_files = list(parquet_dir.glob("*.parquet"))
            if parquet_files:
                parquet_mtime = max(f.stat().st_mtime for f in parquet_files)

                # å¦‚æœCSVæ–‡ä»¶æ¯”Parquetæ–‡ä»¶æ–°è¶…è¿‡1å°æ—¶ï¼Œè®¤ä¸ºä¸ä¸€è‡´
                if csv_mtime - parquet_mtime > 3600:  # 1å°æ—¶
                    return False

        # 4. å¿«é€Ÿæ¯”è¾ƒæ•°æ®é‡ï¼ˆåªè¯»å–CSVè¡Œæ•°ï¼Œä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰
        try:
            csv_line_count = sum(1 for _ in open(csv_path)) - 1  # å‡å»header

            # ä¼°ç®—Parquetæ•°æ®é‡ï¼ˆé€šè¿‡æ—¶é—´é—´éš”ï¼‰
            if existing_intervals:
                interval = existing_intervals[0]
                # å‡è®¾1å°æ—¶æ•°æ®ï¼Œä¼°ç®—æ•°æ®ç‚¹æ•°é‡
                estimated_parquet_count = (interval[1] - interval[0]) // (
                    3600 * 1_000_000_000
                )  # çº³ç§’è½¬å°æ—¶

                # å…è®¸20%çš„å·®å¼‚ï¼ˆæ¯”è¾ƒå®½æ¾çš„æ£€æŸ¥ï¼‰
                if (
                    abs(csv_line_count - estimated_parquet_count)
                    / max(csv_line_count, 1)
                    > 0.2
                ):
                    return False
        except Exception:
            # å¦‚æœå¿«é€Ÿæ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºä¸€è‡´ï¼ˆé¿å…é˜»å¡ï¼‰
            pass

        return True


# ============================================================
# ç­–ç•¥é…ç½®æ¨¡å—
# ============================================================

    except Exception:
        return False


def _create_strategy_configs(
    cfg: BacktestConfig,
    loaded_instruments: Dict[str, Instrument],
    feeds_by_inst: Dict,
    global_feeds: Dict[str, str],
    ConfigClass,
) -> List[ImportableStrategyConfig]:
    """
    åˆ›å»ºç­–ç•¥é…ç½®

    Args:
        cfg: å›æµ‹é…ç½®
        loaded_instruments: å·²åŠ è½½çš„æ ‡çš„æ˜ å°„
        feeds_by_inst: æŒ‰æ ‡çš„åˆ†ç»„çš„æ•°æ®æµ
        global_feeds: å…¨å±€æ•°æ®æµ
        ConfigClass: ç­–ç•¥é…ç½®ç±»

    Returns:
        List[ImportableStrategyConfig]: ç­–ç•¥é…ç½®åˆ—è¡¨
    """
    strategies = []
    strategy_path = f"{cfg.strategy.module_path}:{cfg.strategy.name}"
    config_path = f"{cfg.strategy.module_path}:{cfg.strategy.resolve_config_class()}"

    for inst_id, inst in loaded_instruments.items():
        local_feeds = feeds_by_inst.get(inst_id, {})

        # åªæœ‰æ‹¥æœ‰ 'main' æ•°æ®æµçš„æ ‡çš„æ‰åˆ›å»ºç­–ç•¥å®ä¾‹
        if "main" not in local_feeds:
            continue

        # åˆå¹¶æœ¬åœ°å’Œå…¨å±€æ•°æ®æµ
        combined_feeds = {**local_feeds, **global_feeds}

        # ç”Ÿæˆå¹¶è¿‡æ»¤å‚æ•°
        strat_params = cfg.strategy.resolve_params(
            instrument_id=inst.id,
            leverage=cfg.instrument.leverage if cfg.instrument else 1,
            feed_bar_types=combined_feeds,
        )

        final_params = filter_strategy_params(strat_params, ConfigClass)

        strategies.append(
            ImportableStrategyConfig(
                strategy_path=strategy_path,
                config_path=config_path,
                config=final_params,
            )
        )

# ============================================================
# å›æµ‹æ‰§è¡Œæ¨¡å—
# ============================================================


    return strategies


def run_high_level(cfg: BacktestConfig, base_dir: Path):
    """
    è¿è¡Œé«˜çº§å¼•æ“å›æµ‹ (High-level Engine/BacktestNode)

    Args:
        cfg: å›æµ‹é…ç½®
        base_dir: é¡¹ç›®æ ¹ç›®å½•
    """
    logger.info(f"ğŸš€ Starting High-Level Backtest: {cfg.strategy.name}")

    try:
        # 1. å‡†å¤‡é…ç½®ç±»ï¼ˆç”¨äºå‚æ•°è¿‡æ»¤ï¼‰
        ConfigClass = load_strategy_config_class(
            cfg.strategy.module_path, cfg.strategy.resolve_config_class()
        )

        # 2. åŠ è½½äº¤æ˜“æ ‡çš„ï¼ˆå¸¦æ•°æ®å¯ç”¨æ€§æ£€æŸ¥ï¼‰
        loaded_instruments = _load_instruments(cfg, base_dir)
        logger.info(f"âœ… Loaded {len(loaded_instruments)} instruments")

        # 3. å¯¼å…¥æ•°æ®åˆ°Parquetç›®å½•
        data_path = base_dir / "data"
        catalog_path = data_path / "parquet" / f"{cfg.strategy.name}"

        data_config_by_inst, global_feeds, feeds_by_inst = _import_data_to_catalog(
            cfg, loaded_instruments, catalog_path
        )

        # 4. åˆ›å»ºç­–ç•¥é…ç½®
        strategies = _create_strategy_configs(
            cfg, loaded_instruments, feeds_by_inst, global_feeds, ConfigClass
        )
        logger.info(f"âœ… Created {len(strategies)} strategy configurations")

        # 5. å‡†å¤‡å›æµ‹æ•°æ®é…ç½®
        backtest_data_configs = []
        for inst_id_str, cfg_data in data_config_by_inst.items():
            backtest_data_configs.append(
                BacktestDataConfig(
                    catalog_path=cfg_data["catalog_path"],
                    data_cls="nautilus_trader.model.data:Bar",
                    instrument_id=cfg_data["instrument_id"],
                    bar_types=cfg_data["bar_types"],
                    start_time=cfg.start_date,
                    end_time=cfg.end_date,
                )
            )

        # 6. è¿è¡Œå›æµ‹
        _run_backtest_with_custom_data(
            cfg, base_dir, strategies, backtest_data_configs, loaded_instruments
        )

    except (InstrumentLoadError, DataLoadError, CatalogError) as e:
        logger.error(f"âŒ Backtest failed: {e}")
        raise
    except Exception as e:
        raise BacktestEngineError(f"Unexpected error during backtest: {e}", e)


def _run_backtest_with_custom_data(
    cfg: BacktestConfig,
    base_dir: Path,
    strategies: List[ImportableStrategyConfig],
    backtest_data_configs: List[BacktestDataConfig],
    loaded_instruments: Dict[str, Instrument],
):
    """è¿è¡ŒBacktestNodeå¹¶å¤„ç†è‡ªå®šä¹‰æ•°æ®"""
    if not strategies:
        raise BacktestEngineError(
            "No strategy instances were created. Check config and data paths."
        )

    # é…ç½®äº¤æ˜“æ‰€
    venue_name = (
        cfg.instrument.venue_name
        if cfg.instrument
        else cfg.data_feeds[0].csv_file_name.split("/")[-1].split("-")[0]
    )

    # ä»ç­–ç•¥é…ç½®ä¸­è¯»å– oms_typeï¼ˆå¦‚æœæœ‰å¤šä¸ªç­–ç•¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç­–ç•¥çš„é…ç½®ï¼‰
    oms_type = "HEDGING"  # é»˜è®¤å€¼
    if strategies and hasattr(strategies[0], 'config_path'):
        try:
            import yaml
            config_path = base_dir / strategies[0].config_path
            if config_path.exists():
                with open(config_path, 'r') as f:
                    strategy_config = yaml.safe_load(f)
                    if 'parameters' in strategy_config and 'oms_type' in strategy_config['parameters']:
                        oms_type = strategy_config['parameters']['oms_type']
        except Exception:
            pass  # ä½¿ç”¨é»˜è®¤å€¼

    venue_configs = [
        BacktestVenueConfig(
            name=venue_name,
            oms_type=oms_type,
            account_type="MARGIN",
            base_currency="USDT",
            starting_balances=cfg.initial_balances,
            fee_model=ImportableFeeModelConfig(
                fee_model_path="nautilus_trader.backtest.models:MakerTakerFeeModel",
                config_path="nautilus_trader.backtest.config:MakerTakerFeeModelConfig",
                config={},
            ),
            trade_execution=True,
        )
    ]

    # é…ç½®æ—¥å¿—
    logging_config = None
    if cfg.logging:
        logging_config = LoggingConfig(
            log_level=cfg.logging.log_level,
            log_level_file=cfg.logging.log_level_file,
            log_component_levels=cfg.logging.log_component_levels,
            log_components_only=cfg.logging.log_components_only,
            log_directory=str(base_dir / "log" / "backtest" / "high_level"),
        )

    # æ„å»ºBacktestNodeé…ç½®
    eng_cfg = BacktestEngineConfig(
        strategies=strategies,
        logging=logging_config,
        risk_engine=RiskEngineConfig(bypass=False),
    )

    run_config = BacktestRunConfig(
        engine=eng_cfg,
        data=backtest_data_configs,
        venues=venue_configs,
        raise_exception=True,
    )

    # åˆ›å»ºå¹¶è¿è¡ŒBacktestNode
    gc.collect()
    node = BacktestNode(configs=[run_config])
    logger.info(f"â³ Running BacktestNode with {len(strategies)} strategy instances...")

    # åŠ è½½è‡ªå®šä¹‰æ•°æ®
    _load_custom_data_to_engine(cfg, base_dir, node, run_config, loaded_instruments)

    # è¿è¡Œå›æµ‹
    results = node.run()
    logger.info("âœ… Backtest Complete.")

    # å¤„ç†ç»“æœ

# ============================================================
# è‡ªå®šä¹‰æ•°æ®åŠ è½½æ¨¡å—
# ============================================================

    if results:
        _process_backtest_results(cfg, base_dir, results)


def _check_if_needs_custom_data(strategies: List[ImportableStrategyConfig]) -> bool:
    """æ£€æŸ¥ç­–ç•¥æ˜¯å¦éœ€è¦è‡ªå®šä¹‰æ•°æ®ï¼ˆOI/Fundingï¼‰"""
    for strategy in strategies:
        # æ£€æŸ¥ç­–ç•¥é…ç½®ä¸­çš„ data_dependencies
        if hasattr(strategy, 'config') and isinstance(strategy.config, dict):
            data_deps = strategy.config.get('data_dependencies', [])
            if data_deps:
                for dep in data_deps:
                    if isinstance(dep, dict):
                        data_type = dep.get('data_type', '')
                        if data_type in ['oi', 'funding']:
                            return True
                    elif hasattr(dep, 'data_type'):
                        if dep.data_type in ['oi', 'funding']:
                            return True
    return False


def _load_custom_data_to_engine(
    cfg: BacktestConfig,
    base_dir: Path,
    node: BacktestNode,
    run_config: BacktestRunConfig,
    loaded_instruments: Dict[str, Instrument],
):
    """åŠ è½½è‡ªå®šä¹‰æ•°æ®(OI, Funding Rate)åˆ°å›æµ‹å¼•æ“"""
    if not (cfg.start_date and cfg.end_date):
        logger.warning("âš ï¸ No date range specified, skipping custom data loading")
        return

    # æ³¨æ„ï¼šé«˜çº§å›æµ‹å¼•æ“æš‚ä¸æ”¯æŒ OI/Funding è‡ªå®šä¹‰æ•°æ®åŠ è½½
    # è¿™äº›æ•°æ®çš„åŠ è½½é€»è¾‘ä¸»è¦ä¸ºä½çº§å›æµ‹å¼•æ“è®¾è®¡
    # å¦‚éœ€ä½¿ç”¨ OI/Funding æ•°æ®ï¼Œè¯·ä½¿ç”¨ä½çº§å›æµ‹å¼•æ“ (--type low)
    logger.debug("ğŸ“Š Custom data (OI, Funding Rate) loading skipped in high-level engine")

# ============================================================
# ç»“æœå¤„ç†æ¨¡å—
# ============================================================





def _process_backtest_results(
    cfg: BacktestConfig, base_dir: Path, results: List[BacktestResult]
):
    """å¤„ç†å›æµ‹ç»“æœ"""
    # æ”¶é›†ç­–ç•¥ç»Ÿè®¡æ•°æ®
    filter_stats = None
    trade_metrics = None

    try:
        from strategy.rs_squeeze import RSSqueezeStrategy

        filter_stats = RSSqueezeStrategy.get_filter_stats_summary()
        trade_metrics = RSSqueezeStrategy.get_trade_metrics()
        RSSqueezeStrategy.print_filter_stats_report()
        RSSqueezeStrategy.reset_class_stats()
    except (ImportError, AttributeError):
        pass  # éRS Squeezeç­–ç•¥æ—¶è·³è¿‡

    # æå–ç­–ç•¥é…ç½®å‚æ•°
    strategy_params = cfg.strategy.params
    if hasattr(strategy_params, "model_dump"):
        strategy_config = strategy_params.model_dump()
    elif hasattr(strategy_params, "dict"):
        strategy_config = strategy_params.dict()
    elif isinstance(strategy_params, dict):
        strategy_config = strategy_params
    else:
        strategy_config = {}

    # æ„å»ºå›æµ‹é…ç½®ä¿¡æ¯
    backtest_config = {
        "start_date": str(cfg.start_date),
        "end_date": str(cfg.end_date),
        "initial_balances": cfg.initial_balances,
        "strategies_count": len(results),
    }

    # å¤„ç†æ¯ä¸ªç»“æœ
    for result in results:
        # ç»ˆç«¯è¾“å‡º
        _print_results(result)

        # æ„å»ºå®Œæ•´ç»“æœå­—å…¸
        result_dict = _build_result_dict(
            result=result,
            strategy_name=cfg.strategy.name,
            strategy_config=strategy_config,
            filter_stats=filter_stats,
            trade_metrics=trade_metrics,
            backtest_config=backtest_config,
        )

        # ä¿å­˜JSONæ–‡ä»¶
        json_path = _save_results_to_json(result_dict, cfg.strategy.name, base_dir)
        logger.info(f"ğŸ“ Complete results saved to: {json_path}")


def _build_result_dict(
    result: BacktestResult,
    strategy_name: str,
    strategy_config: Optional[Dict[str, Any]] = None,
    filter_stats: Optional[Dict[str, Dict[str, int]]] = None,
    trade_metrics: Optional[List[Dict]] = None,
    backtest_config: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    æ„å»ºå®Œæ•´çš„å›æµ‹ç»“æœå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¯ç”¨çš„å›æµ‹æ•°æ®ã€‚

    è®¾è®¡ç›®æ ‡ï¼š
    1. åŒ…å«å›æµ‹å¼•æ“èƒ½äº§ç”Ÿçš„æ‰€æœ‰æ•°æ®ï¼Œä¸é—æ¼ä»»ä½•ä¿¡æ¯
    2. ç»“æ„åŒ–ç»„ç»‡ï¼Œä¾¿äº AI å’Œäººå·¥åˆ†æ
    3. åŒæ—¶æ”¯æŒå¿«é€ŸæŸ¥çœ‹ï¼ˆperformanceï¼‰å’Œæ·±åº¦åˆ†æï¼ˆè¯¦ç»†æ•°æ®ï¼‰

    JSON ç»“æ„ï¼š
    - meta: å…ƒæ•°æ®ï¼ˆç­–ç•¥åã€æ—¶é—´æˆ³ã€æ‰§è¡Œæ—¶é•¿ï¼‰
    - summary: æ‘˜è¦ç»Ÿè®¡ï¼ˆè®¢å•æ•°ã€æŒä»“æ•°ï¼‰
    - pnl: ç›ˆäºæ•°æ®ï¼ˆæŒ‰è´§å¸åˆ†ç»„ï¼‰
    - returns: æ”¶ç›Šç‡æ•°æ®ï¼ˆå¤æ™®ç‡ã€æœ€å¤§å›æ’¤ã€æ€»å›æŠ¥ç­‰ï¼‰
    - strategy_config: å®Œæ•´çš„ç­–ç•¥é…ç½®å‚æ•°ï¼ˆä¾› AI åˆ†æä¼˜åŒ–ï¼‰
    - backtest_config: å›æµ‹é…ç½®ä¿¡æ¯ï¼ˆæ—¶é—´èŒƒå›´ã€æ ‡çš„æ•°é‡ç­‰ï¼‰
    - filter_stats: ä¿¡å·è¿‡æ»¤ç»Ÿè®¡
        - by_instrument: æŒ‰æ ‡çš„åˆ†ç»„çš„è¯¦ç»†ç»Ÿè®¡
        - total: æ±‡æ€»ç»Ÿè®¡
        - rates: å„ç§è¿‡æ»¤ç‡ï¼ˆé€šè¿‡ç‡ã€å„è¿‡æ»¤å™¨å æ¯”ï¼‰
        - instrument_count: æ ‡çš„æ•°é‡
    - trade_metrics: äº¤æ˜“æ•°æ®
        - trades: æ‰€æœ‰äº¤æ˜“çš„æ˜ç»†åˆ—è¡¨
        - analysis: åŸºç¡€åˆ†æï¼ˆèƒœç‡ã€ç›ˆäºæ¯”ã€å‡å€¼ç­‰ï¼‰
        - detailed_analysis: è¯¦ç»†åˆ†æï¼ˆèƒœè€… vs è´¥è€…å¯¹æ¯”ï¼‰
    - performance: æ€§èƒ½æ‘˜è¦ï¼ˆæ•´åˆå…³é”®æŒ‡æ ‡ï¼Œä¾¿äºå¿«é€ŸæŸ¥çœ‹ï¼‰

    Args:
        result: BacktestResult å¯¹è±¡
        strategy_name: ç­–ç•¥åç§°
        strategy_config: ç­–ç•¥é…ç½®å‚æ•°ï¼ˆå®Œæ•´ä¿å­˜ï¼Œä¾› AI ä¼˜åŒ–å‚è€ƒï¼‰
        filter_stats: è¿‡æ»¤å™¨ç»Ÿè®¡ï¼ˆæ¥è‡ªç­–ç•¥ç±»å˜é‡ï¼‰
        trade_metrics: äº¤æ˜“æŒ‡æ ‡ï¼ˆæ¥è‡ªç­–ç•¥ç±»å˜é‡ï¼‰
        backtest_config: å›æµ‹é…ç½®ä¿¡æ¯

    Returns:
        å®Œæ•´çš„ç»“æœå­—å…¸ï¼ŒåŒ…å«å›æµ‹å¼•æ“èƒ½è¾“å‡ºçš„æ‰€æœ‰ä¿¡æ¯
    """
    result_dict = {
        "meta": {
            "strategy_name": strategy_name,
            "timestamp": datetime.now().isoformat(),
            "elapsed_time_seconds": result.elapsed_time,
        },
        "summary": {
            "total_orders": result.total_orders,
            "total_positions": result.total_positions,
        },
        "pnl": {},
        "returns": {},
        "strategy_config": strategy_config or {},
        "backtest_config": backtest_config or {},
        "filter_stats": {},
        "trade_metrics": {
            "trades": trade_metrics or [],
            "analysis": {},
            "detailed_analysis": {},  # æ–°å¢ï¼šæ›´è¯¦ç»†çš„äº¤æ˜“åˆ†æ
        },
        "performance": {},  # æ–°å¢ï¼šæ€§èƒ½æ‘˜è¦
    }

    # å¤„ç† PnL æŒ‡æ ‡
    if result.stats_pnls:
        for currency, metrics in result.stats_pnls.items():
            result_dict["pnl"][str(currency)] = {
                k: v if v == v else None
                for k, v in metrics.items()  # NaN -> None
            }

    # å¤„ç†æ”¶ç›Šç‡æŒ‡æ ‡
    if result.stats_returns:
        for currency, val in result.stats_returns.items():
            result_dict["returns"][str(currency)] = val if val == val else None

    # ===== å¤„ç†è¿‡æ»¤å™¨ç»Ÿè®¡ï¼ˆå®Œæ•´ä¿å­˜æ‰€æœ‰è¿‡æ»¤å™¨ç»†èŠ‚ï¼‰ =====
    if filter_stats:
        # æ±‡æ€»æ‰€æœ‰å®ä¾‹çš„ç»Ÿè®¡
        total_stats = {}
        for inst_id, stats in filter_stats.items():
            for key, value in stats.items():
                total_stats[key] = total_stats.get(key, 0) + value

        # è®¡ç®—è¿‡æ»¤ç‡
        signal_checks = total_stats.get("signal_checks", 0)
        all_passed = total_stats.get("all_passed", 0)

        filter_rates = {}
        if signal_checks > 0:
            filter_rates = {
                "pass_rate": (all_passed / signal_checks * 100)
                if signal_checks > 0
                else 0,
                "filter_rate": ((signal_checks - all_passed) / signal_checks * 100)
                if signal_checks > 0
                else 0,
            }
            # è®¡ç®—å„ä¸ªè¿‡æ»¤å™¨çš„å æ¯”
            for key in [
                "fail_trend",
                "fail_squeeze",
                "fail_squeeze_maturity",
                "fail_rs",
                "fail_extension",
                "fail_btc_bear",
                "fail_breakout",
                "fail_volume",
            ]:
                if key in total_stats:
                    filter_rates[f"{key}_rate"] = total_stats[key] / signal_checks * 100

        result_dict["filter_stats"] = {
            "by_instrument": filter_stats,
            "total": total_stats,
            "rates": filter_rates,  # æ–°å¢ï¼šå„ç§è¿‡æ»¤ç‡
            "instrument_count": len(filter_stats),
        }

    # ===== å¤„ç†äº¤æ˜“æŒ‡æ ‡åˆ†æï¼ˆå®Œæ•´ä¿å­˜æ‰€æœ‰äº¤æ˜“ç»Ÿè®¡ï¼‰ =====
    if trade_metrics:
        winners = [t for t in trade_metrics if t.get("result") == "WINNER"]
        losers = [t for t in trade_metrics if t.get("result") == "LOSER"]

        # åŸºç¡€ç»Ÿè®¡
        analysis = {
            "total_trades": len(trade_metrics),
            "winning_trades": len(winners),
            "losing_trades": len(losers),
            "win_rate": len(winners) / len(trade_metrics) if trade_metrics else 0,
        }

        # ç›ˆäºç»Ÿè®¡
        all_pnls = [t.get("pnl_pct", 0) for t in trade_metrics]
        analysis["total_pnl_pct"] = sum(all_pnls)
        analysis["avg_pnl_pct"] = sum(all_pnls) / len(all_pnls) if all_pnls else 0
        analysis["max_winning_trade"] = max(all_pnls) if all_pnls else 0
        analysis["max_losing_trade"] = min(all_pnls) if all_pnls else 0

        # èƒœè€…ç»Ÿè®¡
        if winners:
            winner_pnls = [t.get("pnl_pct", 0) for t in winners]
            analysis["avg_winning_trade"] = sum(winner_pnls) / len(winners)
            analysis["avg_winner_rs_score"] = sum(
                t.get("entry_rs_score", 0) for t in winners
            ) / len(winners)
            analysis["avg_winner_bb_width_pct"] = sum(
                t.get("entry_bb_width_pct", 0) for t in winners
            ) / len(winners)
            analysis["avg_winner_volume_mult"] = sum(
                t.get("entry_volume_mult", 0) for t in winners
            ) / len(winners)

        # è´¥è€…ç»Ÿè®¡
        if losers:
            loser_pnls = [t.get("pnl_pct", 0) for t in losers]
            analysis["avg_losing_trade"] = sum(loser_pnls) / len(losers)
            analysis["avg_loser_rs_score"] = sum(
                t.get("entry_rs_score", 0) for t in losers
            ) / len(losers)
            analysis["avg_loser_bb_width_pct"] = sum(
                t.get("entry_bb_width_pct", 0) for t in losers
            ) / len(losers)
            analysis["avg_loser_volume_mult"] = sum(
                t.get("entry_volume_mult", 0) for t in losers
            ) / len(losers)

        # ç›ˆäºæ¯”å’Œåˆ©æ¶¦å› å­
        if winners and losers and analysis.get("avg_losing_trade", 0) != 0:
            analysis["profit_loss_ratio"] = abs(
                analysis["avg_winning_trade"] / analysis["avg_losing_trade"]
            )

            total_wins = sum([t.get("pnl_pct", 0) for t in winners])
            total_losses = abs(sum([t.get("pnl_pct", 0) for t in losers]))
            if total_losses > 0:
                analysis["profit_factor"] = total_wins / total_losses

        # è¯¦ç»†åˆ†æï¼ˆèƒœè€… vs è´¥è€…å¯¹æ¯”ï¼‰
        detailed_analysis = {}
        if winners and losers:
            detailed_analysis["winners"] = {
                "count": len(winners),
                "avg_pnl_pct": analysis.get("avg_winning_trade", 0),
                "avg_rs_score": analysis.get("avg_winner_rs_score", 0),
                "avg_bb_width_pct": analysis.get("avg_winner_bb_width_pct", 0),
                "avg_volume_mult": analysis.get("avg_winner_volume_mult", 0),
            }
            detailed_analysis["losers"] = {
                "count": len(losers),
                "avg_pnl_pct": analysis.get("avg_losing_trade", 0),
                "avg_rs_score": analysis.get("avg_loser_rs_score", 0),
                "avg_bb_width_pct": analysis.get("avg_loser_bb_width_pct", 0),
                "avg_volume_mult": analysis.get("avg_loser_volume_mult", 0),
            }

        result_dict["trade_metrics"]["analysis"] = analysis
        result_dict["trade_metrics"]["detailed_analysis"] = detailed_analysis

    # ===== æ·»åŠ æ€§èƒ½æ‘˜è¦ï¼ˆæ•´åˆå…³é”®æŒ‡æ ‡ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥çœ‹ï¼‰ =====
    # è¿™ä¸ªéƒ¨åˆ†ä»å„ä¸ªè¯¦ç»†æ•°æ®ä¸­æå–å…³é”®æŒ‡æ ‡ï¼Œä¾¿äºï¼š
    # 1. AI å¿«é€Ÿè¯„ä¼°å›æµ‹ç»“æœ
    # 2. äººå·¥å¿«é€Ÿæµè§ˆ JSON æ—¶äº†è§£æ•´ä½“è¡¨ç°
    # 3. ä¸ current_backtest_result å¯¹åº”ï¼Œç”¨äº LangGraph State
    performance_summary = {
        "total_trades": result.total_positions,
        "total_orders": result.total_orders,
        "elapsed_time_seconds": result.elapsed_time,
    }

    # ä» returns ä¸­æå–å…³é”®æŒ‡æ ‡
    if result.stats_returns:
        for key, val in result.stats_returns.items():
            if "Sharpe Ratio" in str(key):
                performance_summary["sharpe_ratio"] = val if val == val else None
            elif "Max Drawdown" in str(key):
                performance_summary["max_drawdown"] = val if val == val else None
            elif "Total Return" in str(key):
                performance_summary["total_return"] = val if val == val else None

    # ä»äº¤æ˜“åˆ†æä¸­æå–å…³é”®æŒ‡æ ‡
    if trade_metrics:
        analysis = result_dict["trade_metrics"]["analysis"]
        performance_summary["win_rate"] = analysis.get("win_rate", 0)
        performance_summary["profit_loss_ratio"] = analysis.get("profit_loss_ratio", 0)
        performance_summary["avg_winning_trade"] = analysis.get("avg_winning_trade", 0)
        performance_summary["avg_losing_trade"] = analysis.get("avg_losing_trade", 0)
        performance_summary["total_pnl_pct"] = analysis.get("total_pnl_pct", 0)

    # ä»è¿‡æ»¤ç»Ÿè®¡ä¸­æå–ä¿¡æ¯
    if filter_stats:
        filter_data = result_dict["filter_stats"]
        performance_summary["total_signals"] = filter_data["total"].get(
            "signal_checks", 0
        )
        performance_summary["signals_passed"] = filter_data["total"].get(
            "all_passed", 0
        )
        if "rates" in filter_data:
            performance_summary["signal_pass_rate"] = filter_data["rates"].get(
                "pass_rate", 0
            )

    result_dict["performance"] = performance_summary

    return result_dict


def _save_results_to_json(
    result_dict: Dict[str, Any],
    strategy_name: str,
    base_dir: Path,
) -> Path:
    """
    å°†å›æµ‹ç»“æœä¿å­˜ä¸º JSON æ–‡ä»¶ã€‚

    æ–‡ä»¶å‘½åæ ¼å¼: {ç­–ç•¥åç§°}_{YYYY-MM-DD_HH-MM-SS}.json
    å­˜æ”¾è·¯å¾„: output/backtest/result/

    Args:
        result_dict: å®Œæ•´çš„ç»“æœå­—å…¸
        strategy_name: ç­–ç•¥åç§°
        base_dir: é¡¹ç›®æ ¹ç›®å½•

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    # åˆ›å»ºç›®å½•
    result_dir = base_dir / "output" / "backtest" / "result"
    result_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"{strategy_name}_{timestamp}.json"
    # filename = f"{strategy_name}"
    filepath = result_dir / filename

    # ä¿å­˜ JSON
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(result_dict, f, indent=2, ensure_ascii=False, default=str)

    return filepath


def _print_results(
    result: BacktestResult,
    strategy_config: Optional[Dict[str, Any]] = None,
):
    """
    æ‰“å°å›æµ‹ç»“æœæ‘˜è¦åˆ°ç»ˆç«¯ï¼ˆç®€åŒ–ç‰ˆï¼‰

    ç›®æ ‡ï¼šä¿æŒç»ˆç«¯è¾“å‡ºç®€æ´ï¼Œåªæ˜¾ç¤ºæ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡
    å®Œæ•´æ•°æ®ï¼ˆåŒ…æ‹¬ç­–ç•¥é…ç½®ã€äº¤æ˜“æ˜ç»†ç­‰ï¼‰å·²ä¿å­˜åˆ° JSON æ–‡ä»¶ä¸­

    Args:
        result: å›æµ‹ç»“æœå¯¹è±¡
        strategy_config: ç­–ç•¥é…ç½®ï¼ˆå·²ç§»é™¤ï¼Œè®¾ä¸º None ä»¥ç®€åŒ–è¾“å‡ºï¼‰
    """
    logger.info("\n" + "=" * 65)
    logger.info("ğŸ† Backtest Report")
    logger.info("=" * 65)
    logger.info(f"Duration:       {result.elapsed_time:.4f}s")
    logger.info(f"Orders:         {result.total_orders}")
    logger.info(f"Positions:      {result.total_positions}")

    if result.stats_pnls:
        for currency, metrics in result.stats_pnls.items():
            logger.info(f"ğŸ’° PnL ({currency}):")
            for k, v in metrics.items():
                if v == v:  # Check for NaN
                    logger.info(f"   * {k:<25}: {v:,.4f}")

    if result.stats_returns:
        for currency, val in result.stats_returns.items():
            logger.info(f"- {currency:<25} {val:,.4f}")

    # æ³¨æ„ï¼šç­–ç•¥é…ç½®å‚æ•°å·²ä¿å­˜åˆ° JSON æ–‡ä»¶ä¸­ï¼Œç»ˆç«¯ä¸å†æ‰“å°
    # åŸå› ï¼š
    # 1. ä¿æŒç»ˆç«¯è¾“å‡ºç®€æ´ï¼Œåªæ˜¾ç¤ºå…³é”®æ€§èƒ½æŒ‡æ ‡
    # 2. ç­–ç•¥é…ç½®å¯èƒ½æœ‰å‡ åä¸ªå‚æ•°ï¼Œä¼šä½¿ç»ˆç«¯è¾“å‡ºè¿‡é•¿
    # 3. AI å¯ä»¥ä» JSON æ–‡ä»¶ä¸­è¯»å–å®Œæ•´é…ç½®è¿›è¡Œåˆ†æ
    # 4. äººå·¥æŸ¥çœ‹æ—¶ä¹Ÿæ›´å€¾å‘äºæŸ¥çœ‹ JSON æ–‡ä»¶è€Œéç»ˆç«¯æ»šåŠ¨è¾“å‡º

# ============================================================
# å·¥å…·å‡½æ•°æ¨¡å—
# ============================================================


    logger.info("=" * 65 + "\n")


def catalog_loader(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    instrument: Instrument,
    bar_type: BarType,
) -> None:
    """
    å°† CSV æ•°æ®å¯¼å…¥ Catalog (Parquet)ã€‚

    æ³¨æ„ï¼šè°ƒç”¨å‰åº”å…ˆæ£€æŸ¥æ•°æ®æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤è§£æå·¨å¤§çš„ CSVã€‚
    æ£€æŸ¥é€»è¾‘å·²ç§»è‡³ run_high_level ä¸»å¾ªç¯ä¸­ï¼Œä»¥å‡å°‘å†—ä½™å‡½æ•°è°ƒç”¨ã€‚
    """
    # æ—¥å¿—è¾“å‡ºå·²ç§»è‡³è°ƒç”¨æ–¹ï¼Œæ­¤å¤„ä¸å†æ‰“å°

    # 1. è¯»å–å‰å‡ è¡Œæ£€æŸ¥åˆ—åï¼ˆå…¼å®¹ "datetime" å’Œ "timestamp" ä¸¤ç§åˆ—åï¼‰
    sample_df = pd.read_csv(csv_path, nrows=5)

    # ç¡®å®šæ—¶é—´åˆ—åï¼ˆä¼˜å…ˆä½¿ç”¨ timestampï¼Œå…¼å®¹ datetimeï¼‰
    if "timestamp" in sample_df.columns:
        time_col = "timestamp"
    elif "datetime" in sample_df.columns:
        time_col = "datetime"
    else:
        raise ValueError(
            f"No valid time column found in {csv_path}. "
            f"Expected 'datetime' or 'timestamp', got: {list(sample_df.columns)}"
        )

    # 2. é«˜æ•ˆåŠ è½½ CSVï¼ˆä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®åŠ è½½å™¨ï¼‰
    from utils.data_management.data_loader import load_ohlcv_csv

    df: DataFrame = load_ohlcv_csv(csv_path=csv_path)

    # 3. è½¬æ¢å¹¶å†™å…¥
    # Wrangler å°† DataFrame è½¬æ¢ä¸º NT å†…éƒ¨çš„ Bar åºåˆ—
    wrangler = BarDataWrangler(bar_type, instrument)

    # å†™å…¥æ ‡çš„ä¿¡æ¯ (NT çš„å†™å…¥æ˜¯å¹‚ç­‰çš„ï¼Œå¤šæ¬¡å†™å…¥åŒä¸€ä¸ª Instrument ä¸ä¼šæŠ¥é”™)
    catalog.write_data([instrument])

    # å†™å…¥ Bar æ•°æ®
    catalog.write_data(wrangler.process(df))

    # 4. è½»é‡çº§æ ¡éªŒ
    intervals = catalog.get_intervals(data_cls=Bar, identifier=str(bar_type))
    if not intervals:
        raise ValueError(f"âŒ Failed to verify data in catalog for {instrument.id}")


# æ³¨æ„ï¼šè‡ªå®šä¹‰æ•°æ®ï¼ˆOI, Funding Rateï¼‰ç°åœ¨é€šè¿‡ OIFundingDataLoader ç›´æ¥åŠ è½½
# ä¸å†éœ€è¦åºåˆ—åŒ–åˆ° Parquet Catalog
