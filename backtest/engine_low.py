import gc
import json
import logging
import sys
from datetime import datetime
from decimal import Decimal
from pathlib import Path
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)
from nautilus_trader.analysis.tearsheet import create_tearsheet
from nautilus_trader.backtest.config import BacktestEngineConfig
from nautilus_trader.backtest.engine import BacktestEngine
from nautilus_trader.backtest.models.fee import MakerTakerFeeModel
from nautilus_trader.common.config import LoggingConfig
from nautilus_trader.config import RiskEngineConfig
from nautilus_trader.core.nautilus_pyo3 import millis_to_nanos
from nautilus_trader.model import BarType, TraderId
from nautilus_trader.model.currencies import USDT
from nautilus_trader.model.data import FundingRateUpdate
from nautilus_trader.model.enums import (
    AccountType,
    BarAggregation,
    BookType,
    OmsType,
)
from nautilus_trader.model.identifiers import Venue
from nautilus_trader.model.instruments import Instrument
from nautilus_trader.persistence.wranglers import BarDataWrangler
import pandas as pd
from pandas import DataFrame

from core.exceptions import (
    BacktestEngineError,
    CustomDataError,
    DataLoadError,
    InstrumentLoadError,
)
from core.schemas import BacktestConfig, DataConfig
from strategy.core.loader import (
    filter_strategy_params,
    load_strategy_class,
    load_strategy_config_class,
)
from utils.data_file_checker import check_single_data_file
from utils.data_management.data_loader import load_ohlcv_auto
from utils.instrument_loader import load_instrument
from utils.oi_funding_adapter import OIFundingDataLoader


def _load_data_for_feed(
    engine: BacktestEngine,
    base_dir: Path,
    cfg: BacktestConfig,
    data_cfg: DataConfig,
    loaded_instruments: Dict[str, Instrument],
) -> Optional[BarType]:
    """
    åŠ è½½ CSV æ•°æ®å¹¶æ³¨å…¥å›æµ‹å¼•æ“ (Low-Level Engine é£æ ¼)

    Args:
        engine: BacktestEngine å®ä¾‹
        base_dir: é¡¹ç›®åŸºç¡€ç›®å½•
        cfg: å›æµ‹é…ç½®
        data_cfg: æ•°æ®é…ç½®
        loaded_instruments: å·²åŠ è½½çš„æ ‡çš„æ˜ å°„

    Returns:
        Optional[BarType]: æˆåŠŸåŠ è½½åˆ™è¿”å› BarTypeï¼Œå¦åˆ™è¿”å› None

    Raises:
        DataLoadError: å½“æ•°æ®åŠ è½½å¤±è´¥æ—¶
    """
    # ç¡®å®šå…³è”çš„ Instrument ID
    inst_id_str = data_cfg.instrument_id or str(cfg.instrument.instrument_id)
    inst = loaded_instruments.get(inst_id_str)
    if not inst:
        raise DataLoadError(f"Instrument {inst_id_str} not found for feed {data_cfg.csv_file_name}")

    feed_bar_type_str = f"{inst.id}-{data_cfg.bar_type_str}"
    feed_bar_type = BarType.from_str(feed_bar_type_str)

    data_path = data_cfg.full_path
    if not data_path.exists():
        raise DataLoadError(f"Data file not found: {data_path}", str(data_path))

    # è°ƒè¯•æ—¥å¿—ï¼šè¾“å‡ºå®é™…åŠ è½½çš„æ–‡ä»¶è·¯å¾„
    logger.info(f"ğŸ” Loading {inst_id_str} from: {data_path}")

    try:
        # ä½¿ç”¨è‡ªåŠ¨æ ¼å¼æ£€æµ‹åŠ è½½å™¨ï¼ˆæ”¯æŒ CSV å’Œ Parquetï¼‰
        df: DataFrame = load_ohlcv_auto(
            file_path=data_path,
            start_date=cfg.start_date,
            end_date=cfg.end_date,
        )

        if len(df) == 0:
            raise DataLoadError(
                f"No data available in range for {data_cfg.csv_file_name}", str(data_path)
            )

        # ç¡®ä¿ timestamp åˆ—å­˜åœ¨ï¼ˆç”¨äºéªŒè¯ï¼‰ï¼ŒåŒæ—¶ä¿æŒ DatetimeIndexï¼ˆç”¨äº Wranglerï¼‰
        if "timestamp" not in df.columns:
            if isinstance(df.index, pd.DatetimeIndex):
                # å°†ç´¢å¼•è½¬æ¢ä¸º Unix æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰ä½œä¸º timestamp åˆ—
                df["timestamp"] = (df.index.astype("int64") // 10**6).astype("int64")
            elif hasattr(df.index, "name") and df.index.name == "timestamp":
                # ç´¢å¼•åä¸º timestampï¼Œè½¬æ¢ï¿½ï¿½ï¿½æ•°å€¼
                df["timestamp"] = (df.index.astype("int64") // 10**6).astype("int64")

        # éªŒè¯æ•°æ®è´¨é‡ï¼šæ£€æŸ¥å¿…éœ€åˆ—å’Œæ•°æ®æœ‰æ•ˆæ€§
        required_columns = ["timestamp", "open", "high", "low", "close", "volume"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise DataLoadError(
                f"Missing required columns in {data_cfg.csv_file_name}: {missing_columns}",
                str(data_path),
            )

        # æ£€æŸ¥ä»·æ ¼åˆ—æ˜¯å¦åŒ…å« NaN æˆ–æ— æ•ˆå€¼
        price_columns = ["open", "high", "low", "close"]
        for col in price_columns:
            if df[col].isna().any():
                raise DataLoadError(
                    f"Column '{col}' contains NaN values in {data_cfg.csv_file_name}",
                    str(data_path),
                )
            if (df[col] <= 0).any():
                raise DataLoadError(
                    f"Column '{col}' contains non-positive values in {data_cfg.csv_file_name}",
                    str(data_path),
                )

        # æ£€æŸ¥ OHLC é€»è¾‘å…³ç³»ï¼šhigh >= low, high >= open/close, low <= open/close
        if (df["high"] < df["low"]).any():
            raise DataLoadError(
                f"Invalid OHLC data: high < low in {data_cfg.csv_file_name}",
                str(data_path),
            )
        if (df["high"] < df["open"]).any() or (df["high"] < df["close"]).any():
            raise DataLoadError(
                f"Invalid OHLC data: high < open or close in {data_cfg.csv_file_name}",
                str(data_path),
            )
        if (df["low"] > df["open"]).any() or (df["low"] > df["close"]).any():
            raise DataLoadError(
                f"Invalid OHLC data: low > open or close in {data_cfg.csv_file_name}",
                str(data_path),
            )

        # ä½¿ç”¨ Wrangler è½¬æ¢å¹¶æ³¨å…¥å¼•æ“
        wrangler = BarDataWrangler(feed_bar_type, inst)
        bars = wrangler.process(df)
        engine.add_data(bars)

        file_format = data_path.suffix.upper()[1:]  # .csv -> CSV, .parquet -> PARQUET
        logger.info(f"âœ… Loaded {len(bars)} bars for {inst.id} ({data_cfg.label}) [{file_format}]")
        return feed_bar_type

    except Exception as e:
        if isinstance(e, DataLoadError):
            raise
        # ä½¿ç”¨ from e ä¿ç•™åŸå§‹å¼‚å¸¸é“¾å’Œå †æ ˆè·Ÿè¸ª
        raise DataLoadError(
            f"Error loading {data_cfg.csv_file_name}: {e}", str(data_path), e
        ) from e


def _get_symbol_from_instrument(instrument_id) -> str:
    """ä»instrument_idæå–ç¬¦å·ï¼ˆä¿ç•™ -PERP åç¼€ä»¥æ­£ç¡®å®šä½æ•°æ®ç›®å½•ï¼‰"""
    # å¯¹äº PERP æ ‡çš„ï¼Œä¿ç•™å®Œæ•´çš„ symbolï¼ˆåŒ…æ‹¬ -PERPï¼‰
    # å› ä¸ºèµ„é‡‘è´¹ç‡æ•°æ®å­˜å‚¨åœ¨ BTCUSDT-PERP ç›®å½•ä¸‹
    symbol_str = str(instrument_id.symbol)
    return symbol_str  # è¿”å›å®Œæ•´çš„ symbolï¼Œä¾‹å¦‚ "BTCUSDT-PERP" æˆ– "BTCUSDT"


def _find_custom_data_files(symbol_dir: Path) -> tuple[list, list]:
    """æŸ¥æ‰¾OIå’ŒFundingæ•°æ®æ–‡ä»¶"""
    oi_files = list(symbol_dir.glob("*-oi-1h-*.csv"))
    funding_files = list(symbol_dir.glob("*-funding_rate-*.csv"))
    return oi_files, funding_files


def _load_oi_data_for_symbol(
    loader: OIFundingDataLoader, oi_files: list, symbol: str, instrument_id, cfg: BacktestConfig
) -> list:
    """åŠ è½½OIæ•°æ®"""
    oi_data_list = []
    if oi_files:
        for oi_file in oi_files:
            # ç¡®ä¿æ—¥æœŸä¸ä¸º None
            if cfg.start_date and cfg.end_date:
                oi_data_list.extend(
                    loader.load_oi_data(
                        symbol=symbol,
                        instrument_id=instrument_id,
                        start_date=cfg.start_date,
                        end_date=cfg.end_date,
                        exchange=cfg.instrument.venue_name.lower() if cfg.instrument else "binance",
                    )
                )
    return oi_data_list


def _load_funding_data_for_symbol(
    loader: OIFundingDataLoader,
    funding_files: list,
    symbol: str,
    instrument_id,
    cfg: BacktestConfig,
) -> List[FundingRateUpdate]:
    """åŠ è½½Funding Rateæ•°æ®"""
    funding_data_list = []
    if funding_files:
        for funding_file in funding_files:
            try:
                df = pd.read_csv(funding_file)

                # éªŒè¯æ•°æ®æ ¼å¼
                if "timestamp" not in df.columns or "funding_rate" not in df.columns:
                    continue

                # è½¬æ¢ä¸º FundingRateUpdate å¯¹è±¡
                for _, row in df.iterrows():
                    # å¤„ç† timestampï¼šå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•´æ•°
                    ts_value = row["timestamp"]
                    if isinstance(ts_value, str):
                        # å­—ç¬¦ä¸²æ ¼å¼ï¼Œè½¬æ¢ä¸º Unix æ¯«ç§’æ—¶é—´æˆ³
                        ts_ms = int(pd.to_datetime(ts_value).timestamp() * 1000)
                    else:
                        ts_ms = int(ts_value)

                    ts_event = millis_to_nanos(ts_ms)
                    funding_rate = Decimal(str(row["funding_rate"]))

                    next_funding_time = None
                    if "next_funding_time" in df.columns and pd.notna(row["next_funding_time"]):
                        next_ts_value = row["next_funding_time"]
                        if isinstance(next_ts_value, str):
                            next_ts_ms = int(pd.to_datetime(next_ts_value).timestamp() * 1000)
                        else:
                            next_ts_ms = int(next_ts_value)
                        next_funding_time = millis_to_nanos(next_ts_ms)

                    funding_data = FundingRateUpdate(
                        instrument_id=instrument_id,
                        rate=funding_rate,
                        next_funding_ns=next_funding_time,
                        ts_event=ts_event,
                        ts_init=ts_event,
                    )
                    funding_data_list.append(funding_data)

            except Exception as e:
                logger.warning(f"Error loading {funding_file}: {e}")

    return funding_data_list


def _process_instrument_custom_data(
    inst, data_dir: Path, cfg: BacktestConfig, engine: BacktestEngine
) -> int:
    """å¤„ç†å•ä¸ªæ ‡çš„çš„è‡ªå®šä¹‰æ•°æ®"""
    from utils.oi_funding_adapter import merge_custom_data_with_bars

    instrument_id = inst.id
    symbol = _get_symbol_from_instrument(instrument_id)
    symbol_dir = data_dir / symbol

    logger.debug(f"   ğŸ” Checking custom data for {instrument_id} in {symbol_dir}")

    if not symbol_dir.exists():
        logger.debug(f"   âš ï¸ Directory not found: {symbol_dir}")
        return 0

    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    oi_files, funding_files = _find_custom_data_files(symbol_dir)
    logger.debug(f"   ğŸ“ Found {len(oi_files)} OI files, {len(funding_files)} funding files")

    # åŠ è½½æ•°æ®
    loader = OIFundingDataLoader(data_dir)
    oi_data_list = _load_oi_data_for_symbol(loader, oi_files, symbol, instrument_id, cfg)
    funding_data_list = _load_funding_data_for_symbol(
        loader, funding_files, symbol, instrument_id, cfg
    )

    # åˆå¹¶å¹¶æ·»åŠ åˆ°å¼•æ“
    if oi_data_list or funding_data_list:
        merged_data = merge_custom_data_with_bars(oi_data_list, funding_data_list)
        from nautilus_trader.model.identifiers import ClientId

        # é‡è¦ï¼šä½¿ç”¨ add_data() å¹¶ç¡®ä¿æ•°æ®è¢«æ’åº
        # BacktestEngine ä¼šåœ¨ run() æ—¶è‡ªåŠ¨å›æ”¾æ‰€æœ‰æ·»åŠ çš„æ•°æ®
        engine.add_data(merged_data, client_id=ClientId("BINANCE"), sort=True)

        logger.info(f"   âœ… Added {len(merged_data)} custom data points for {symbol}")
        logger.debug(f"   ğŸ“Š Data types: {set(type(d).__name__ for d in merged_data)}")
        logger.debug(f"   â° Time range: {merged_data[0].ts_event} to {merged_data[-1].ts_event}")
        return len(merged_data)

    return 0


def _load_custom_data_to_engine(
    cfg: BacktestConfig,
    base_dir: Path,
    engine: BacktestEngine,
    loaded_instruments: Dict[str, Instrument],
) -> int:
    """
    åŠ è½½è‡ªå®šä¹‰æ•°æ®(OI, Funding Rate)åˆ°å›æµ‹å¼•æ“ (Low-Level Engine é£æ ¼)

    Args:
        cfg: å›æµ‹é…ç½®
        base_dir: é¡¹ç›®åŸºç¡€ç›®å½•
        engine: BacktestEngine å®ä¾‹
        loaded_instruments: å·²åŠ è½½çš„æ ‡çš„æ˜ å°„

    Returns:
        int: åŠ è½½çš„æ•°æ®ç‚¹æ€»æ•°

    Raises:
        CustomDataError: å½“è‡ªå®šä¹‰æ•°æ®åŠ è½½å¤±è´¥æ—¶
    """
    if not (cfg.start_date and cfg.end_date):
        logger.warning("âš ï¸ No date range specified, skipping custom data loading")
        return 0

    logger.info("ğŸ“Š Loading custom data (OI, Funding Rate)...")

    try:
        data_dir = base_dir / "data" / "raw"
        total_loaded = 0

        for inst_id_str, inst in loaded_instruments.items():
            total_loaded += _process_instrument_custom_data(inst, data_dir, cfg, engine)

        logger.info(f"âœ… Total custom data loaded: {total_loaded} points")
        return total_loaded

    except Exception as e:
        raise CustomDataError(f"Custom data loading failed: {e}", cause=e)


def _extract_strategy_config(cfg: BacktestConfig) -> Dict[str, Any]:
    """æå–ç­–ç•¥é…ç½®"""
    strategy_params = cfg.strategy.params
    if hasattr(strategy_params, "model_dump"):
        return strategy_params.model_dump()  # type: ignore[no-any-return]
    elif hasattr(strategy_params, "dict"):
        return strategy_params.dict()  # type: ignore[no-any-return]
    elif isinstance(strategy_params, dict):
        return strategy_params
    return {}


def _get_order_position_stats(engine: BacktestEngine) -> tuple[int, int]:
    """è·å–è®¢å•å’ŒæŒä»“ç»Ÿè®¡"""
    orders_report = engine.trader.generate_order_fills_report()
    positions_report = engine.trader.generate_positions_report()

    total_orders = (
        len(orders_report) if orders_report is not None and hasattr(orders_report, "__len__") else 0
    )
    total_positions = (
        len(positions_report)
        if positions_report is not None and hasattr(positions_report, "__len__")
        else 0
    )

    return total_orders, total_positions


def _find_pnl_column(positions_report):
    """æŸ¥æ‰¾PnLåˆ—å"""
    for col in ["realized_pnl", "pnl", "realized_return", "return"]:
        if col in positions_report.columns:
            return col
    return None


def _calculate_pnl_stats(realized_pnls) -> dict:
    """è®¡ç®—PnLç»Ÿè®¡æŒ‡æ ‡"""
    winners = realized_pnls[realized_pnls > 0]
    losers = realized_pnls[realized_pnls < 0]
    total_pnl = float(realized_pnls.sum())

    return {
        "PnL (total)": total_pnl,
        "PnL% (total)": total_pnl,
        "Max Winner": float(winners.max()) if len(winners) > 0 else None,
        "Avg Winner": float(winners.mean()) if len(winners) > 0 else None,
        "Min Winner": float(winners.min()) if len(winners) > 0 else None,
        "Min Loser": float(losers.min()) if len(losers) > 0 else None,
        "Avg Loser": float(losers.mean()) if len(losers) > 0 else None,
        "Max Loser": float(losers.max()) if len(losers) > 0 else None,
        "Expectancy": float(realized_pnls.mean()) if len(realized_pnls) > 0 else None,
        "Win Rate": float(len(winners) / len(realized_pnls)) if len(realized_pnls) > 0 else None,
    }


def _calculate_basic_stats(returns):
    """è®¡ç®—åŸºç¡€ç»Ÿè®¡æŒ‡æ ‡"""
    import numpy as np

    avg_return = float(np.mean(returns))
    std_return = float(np.std(returns))
    return avg_return, std_return


def _calculate_sharpe_ratio(avg_return: float, std_return: float) -> float:
    """è®¡ç®—å¤æ™®æ¯”ç‡"""
    import numpy as np

    return float(avg_return / std_return * np.sqrt(252)) if std_return > 0 else 0


def _calculate_sortino_ratio(returns, avg_return: float) -> float:
    """è®¡ç®—ç´¢æè¯ºæ¯”ç‡"""
    import numpy as np

    downside_returns = returns[returns < 0]
    downside_std = float(np.std(downside_returns)) if len(downside_returns) > 0 else 0
    return float(avg_return / downside_std * np.sqrt(252)) if downside_std > 0 else 0


def _calculate_profit_factor(winners, losers) -> float:
    """è®¡ç®—ç›ˆåˆ©å› å­"""
    avg_win = float(winners.mean()) if len(winners) > 0 else 0
    avg_loss = abs(float(losers.mean())) if len(losers) > 0 else 0
    return float(avg_win / avg_loss) if avg_loss > 0 else 0


def _calculate_returns_stats(realized_pnls) -> dict:
    """è®¡ç®—æ”¶ç›Šç‡ç»Ÿè®¡æŒ‡æ ‡"""
    if len(realized_pnls) <= 1:
        return {}

    import numpy as np

    returns = realized_pnls.values
    winners = realized_pnls[realized_pnls > 0]
    losers = realized_pnls[realized_pnls < 0]

    avg_return, std_return = _calculate_basic_stats(returns)
    sharpe = _calculate_sharpe_ratio(avg_return, std_return)
    sortino = _calculate_sortino_ratio(returns, avg_return)
    profit_factor = _calculate_profit_factor(winners, losers)

    avg_win = float(winners.mean()) if len(winners) > 0 else 0
    avg_loss = float(losers.mean()) if len(losers) > 0 else None
    risk_return = float(std_return / abs(avg_return)) if avg_return != 0 else None

    return {
        "Returns Volatility (252 days)": std_return * np.sqrt(252),
        "Average (Return)": avg_return,
        "Average Loss (Return)": avg_loss,
        "Average Win (Return)": avg_win,
        "Sharpe Ratio (252 days)": sharpe,
        "Sortino Ratio (252 days)": sortino,
        "Profit Factor": profit_factor,
        "Risk Return Ratio": risk_return,
    }


def _extract_pnl_from_positions(positions_report) -> dict:
    """ä»æŒä»“æŠ¥å‘Šä¸­æå–PnLç»Ÿè®¡"""
    stats_pnls: Dict[str, Any] = {}

    try:
        if (
            positions_report is None
            or not hasattr(positions_report, "empty")
            or positions_report.empty
        ):
            return stats_pnls

        logger.debug(f"æŒä»“æŠ¥å‘Šåˆ—: {positions_report.columns.tolist()}")

        pnl_column = _find_pnl_column(positions_report)
        if not pnl_column:
            logger.warning(f"æŒä»“æŠ¥å‘Šä¸­æœªæ‰¾åˆ° PnL åˆ—ï¼Œå¯ç”¨åˆ—: {positions_report.columns.tolist()}")
            return stats_pnls

        realized_pnls = positions_report[pnl_column].dropna()
        if len(realized_pnls) == 0:
            return stats_pnls

        stats_pnls["USDT"] = _calculate_pnl_stats(realized_pnls)

    except Exception as e:
        logger.warning(f"è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡æ—¶å‡ºé”™: {e}")

    return stats_pnls


def _build_result_dict(
    cfg: BacktestConfig,
    strategy_config: dict,
    total_orders: int,
    total_positions: int,
    stats_pnls: dict,
    engine_pnl: float,
    funding_collected: Decimal,
    initial_capital: float,
) -> dict:
    """æ„å»ºç»“æœå­—å…¸"""
    # è®¡ç®—çœŸå® PnL
    funding_float = float(funding_collected)
    real_pnl = engine_pnl + funding_float
    real_return_pct = (real_pnl / initial_capital * 100) if initial_capital > 0 else 0

    result_dict = {
        "meta": {
            "strategy_name": cfg.strategy.name,
            "timestamp": datetime.now().isoformat(),
            "engine_type": "low_level",
        },
        "summary": {
            "start_date": cfg.start_date,
            "end_date": cfg.end_date,
            "initial_balances": [str(balance) for balance in cfg.initial_balances],
            "total_orders": total_orders,
            "total_positions": total_positions,
            "engine_pnl": engine_pnl,
            "funding_collected": funding_float,
            "real_pnl": real_pnl,
            "real_return_pct": real_return_pct,
        },
        "pnl": {},
        "returns": {},
        "strategy_config": strategy_config,
        "backtest_config": {
            "start_date": str(cfg.start_date),
            "end_date": str(cfg.end_date),
            "initial_balances": [str(b) for b in cfg.initial_balances],
        },
        "performance": {
            "total_orders": total_orders,
            "total_positions": total_positions,
            "engine_pnl": engine_pnl,
            "funding_collected": funding_float,
            "real_pnl": real_pnl,
            "real_return_pct": real_return_pct,
        },
    }

    if stats_pnls:
        pnl_dict: Dict[str, Any] = result_dict["pnl"]  # type: ignore[assignment]
        for currency, metrics in stats_pnls.items():
            pnl_dict[str(currency)] = {str(k): v if v == v else None for k, v in metrics.items()}

    return result_dict


def _add_returns_to_result(engine: BacktestEngine, result_dict: dict) -> None:
    """æ·»åŠ æ”¶ç›Šç‡æŒ‡æ ‡åˆ°ç»“æœå­—å…¸"""
    try:
        returns_stats = engine.trader.generate_returns_report()
        if returns_stats:
            for key, val in returns_stats.items():
                result_dict["returns"][str(key)] = val if val == val else None
    except (AttributeError, KeyError, ValueError) as e:
        logger.warning(f"Failed to generate returns report: {e}")


def _save_result_json(cfg: BacktestConfig, base_dir: Path, result_dict: dict) -> None:
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    result_dir = base_dir / "output" / "backtest" / "result"
    result_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"{cfg.strategy.name}_{timestamp}.json"
    filepath = result_dir / filename

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(result_dict, f, indent=2, ensure_ascii=False, default=str)

    logger.info(f"ğŸ“ Results saved to: {filepath}")


def _extract_funding_collected(engine: BacktestEngine) -> Decimal:
    """ä»ç­–ç•¥å®ä¾‹ä¸­æå–èµ„é‡‘è´¹ç‡æ”¶ç›Š"""
    total_funding = Decimal("0")

    try:
        # éå†æ‰€æœ‰ç­–ç•¥å®ä¾‹
        for strategy in engine.trader.strategies():
            # æ£€æŸ¥ç­–ç•¥æ˜¯å¦æœ‰ _total_funding_collected å±æ€§
            if hasattr(strategy, "_total_funding_collected"):
                funding = strategy._total_funding_collected
                if funding:
                    total_funding += funding
                    logger.info(
                        f"ğŸ’° Extracted funding from {strategy.__class__.__name__}: {float(funding):.2f} USDT"
                    )
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to extract funding collected: {e}")

    return total_funding


def _calculate_engine_pnl(cfg: BacktestConfig, engine: BacktestEngine) -> tuple[float, float]:
    """è®¡ç®—å¼•æ“å±‚é¢çš„ PnL å’Œåˆå§‹èµ„é‡‘"""
    initial_capital = sum(float(b.as_decimal()) for b in cfg.initial_balances)

    # è·å–æœ€ç»ˆè´¦æˆ·ä½™é¢
    venue_name = cfg.instrument.venue_name if cfg.instrument else "BINANCE"
    venue = Venue(venue_name)
    account = engine.trader.generate_account_report(venue)

    if account is not None and not account.empty:
        # ä»è´¦æˆ·æŠ¥å‘Šä¸­æå–æœ€ç»ˆä½™é¢
        final_balance = (
            float(account.iloc[-1]["total"]) if "total" in account.columns else initial_capital
        )
    else:
        final_balance = initial_capital

    engine_pnl = final_balance - initial_capital
    return engine_pnl, initial_capital


def _process_backtest_results(
    cfg: BacktestConfig,
    base_dir: Path,
    engine: BacktestEngine,
) -> None:
    """
    å¤„ç†å›æµ‹ç»“æœ (Low-Level Engine é£æ ¼)

    Args:
        cfg: å›æµ‹é…ç½®
        base_dir: é¡¹ç›®åŸºç¡€ç›®å½•
        engine: BacktestEngine å®ä¾‹
    """
    try:
        strategy_config = _extract_strategy_config(cfg)
        total_orders, total_positions = _get_order_position_stats(engine)

        positions_report = engine.trader.generate_positions_report()
        stats_pnls = _extract_pnl_from_positions(positions_report)

        # æå–èµ„é‡‘è´¹ç‡æ”¶ç›Š
        funding_collected = _extract_funding_collected(engine)

        # è®¡ç®—å¼•æ“ PnL
        engine_pnl, initial_capital = _calculate_engine_pnl(cfg, engine)

        result_dict = _build_result_dict(
            cfg,
            strategy_config,
            total_orders,
            total_positions,
            stats_pnls,
            engine_pnl,
            funding_collected,
            initial_capital,
        )
        _add_returns_to_result(engine, result_dict)
        _save_result_json(cfg, base_dir, result_dict)

        # è¾“å‡ºçœŸå® PnL æ‘˜è¦
        real_pnl = engine_pnl + float(funding_collected)
        real_return_pct = (real_pnl / initial_capital * 100) if initial_capital > 0 else 0

        logger.info(
            f"\n{'=' * 60}\n"
            f"ğŸ“Š Backtest Results Summary\n"
            f"{'=' * 60}\n"
            f"Initial Capital: {initial_capital:.2f} USDT\n"
            f"Engine PnL: {engine_pnl:.2f} USDT\n"
            f"Funding Collected: {float(funding_collected):.2f} USDT\n"
            f"Real PnL: {real_pnl:.2f} USDT ({real_return_pct:+.2f}%)\n"
            f"{'=' * 60}"
        )

    except Exception as e:
        logger.warning(f"âš ï¸ Error saving results: {e}")


def _setup_engine(cfg: BacktestConfig, base_dir: Path) -> BacktestEngine:
    """åˆå§‹åŒ–å›æµ‹å¼•æ“"""
    logging_config = None
    if cfg.logging:
        logging_config = LoggingConfig(
            log_level=cfg.logging.log_level,
            log_level_file=cfg.logging.log_level_file,
            log_directory=str(base_dir / "log" / "backtest" / "low_level"),
        )

    engine = BacktestEngine(
        BacktestEngineConfig(
            trader_id=TraderId("BACKTESTER-001"),
            logging=logging_config,
            risk_engine=RiskEngineConfig(bypass=False),
        )
    )

    venue_name = cfg.instrument.venue_name if cfg.instrument else "BINANCE"
    oms_type_str = getattr(
        cfg.strategy.params if hasattr(cfg.strategy, "params") else {}, "oms_type", "HEDGING"
    )
    oms_type = OmsType.HEDGING if oms_type_str == "HEDGING" else OmsType.NETTING

    engine.add_venue(
        venue=Venue(venue_name),
        oms_type=oms_type,
        book_type=BookType.L1_MBP,
        account_type=AccountType.MARGIN,
        base_currency=USDT,
        starting_balances=cfg.initial_balances,
        trade_execution=True,
        fee_model=MakerTakerFeeModel(),
    )

    return engine


def _filter_instruments_with_data(cfg: BacktestConfig, base_dir: Path) -> List:
    """è¿‡æ»¤æœ‰æ•°æ®çš„æ ‡çš„"""
    from core.schemas import InstrumentConfig, InstrumentType

    instruments_with_data = []

    if not cfg.start_date or not cfg.end_date:
        logger.warning("âš ï¸ start_date æˆ– end_date æœªé…ç½®ï¼Œè·³è¿‡æ•°æ®å¯ç”¨æ€§æ£€æŸ¥")
        return list(cfg.instruments)

    for inst_cfg in cfg.instruments:
        symbol = (
            inst_cfg.instrument_id.split("-")[0]
            if "-" in inst_cfg.instrument_id
            else inst_cfg.instrument_id.split(".")[0]
        )

        if cfg.data_feeds:
            first_feed = cfg.data_feeds[0]
            unit_map = {
                BarAggregation.MINUTE: "m",
                BarAggregation.HOUR: "h",
                BarAggregation.DAY: "d",
            }
            timeframe = f"{first_feed.bar_period}{unit_map.get(first_feed.bar_aggregation, 'h')}"
        else:
            timeframe = "1h"

        has_data, _ = check_single_data_file(
            symbol=symbol,
            start_date=cfg.start_date,
            end_date=cfg.end_date,
            timeframe=timeframe,
            exchange=inst_cfg.venue_name.lower(),
            base_dir=base_dir,
        )

        if has_data:
            instruments_with_data.append(inst_cfg)
        else:
            logger.debug(f"â­ï¸ Skipping {inst_cfg.instrument_id}: no data file")

    # è‡ªåŠ¨æ·»åŠ  SPOT æ ‡çš„ï¼ˆç”¨äºèµ„é‡‘è´¹ç‡å¥—åˆ©ç­–ç•¥ï¼‰
    perp_instruments = [cfg for cfg in instruments_with_data if "-PERP" in cfg.instrument_id]

    for perp_cfg in perp_instruments:
        # æ¨å¯¼ SPOT symbol: BTCUSDT-PERP -> BTCUSDT
        spot_symbol = perp_cfg.instrument_id.split("-")[0]
        spot_id = spot_symbol + "." + perp_cfg.venue_name

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if not any(cfg.instrument_id == spot_id for cfg in instruments_with_data):
            # æ£€æŸ¥ SPOT æ•°æ®æ˜¯å¦å­˜åœ¨
            has_spot_data, _ = check_single_data_file(
                symbol=spot_symbol,
                start_date=cfg.start_date,
                end_date=cfg.end_date,
                timeframe=timeframe,
                exchange=perp_cfg.venue_name.lower(),
                base_dir=base_dir,
            )

            if has_spot_data:
                # åˆ›å»º SPOT æ ‡çš„é…ç½®
                spot_cfg = InstrumentConfig(
                    type=InstrumentType.SPOT,
                    venue_name=perp_cfg.venue_name,
                    base_currency=perp_cfg.base_currency,
                    quote_currency=perp_cfg.quote_currency,
                    leverage=1,
                )
                instruments_with_data.append(spot_cfg)
                logger.info(f"ğŸ”„ Auto-added SPOT instrument for funding arbitrage: {spot_id}")
            else:
                logger.warning(
                    f"âš ï¸ SPOT data not found for {spot_id}, funding arbitrage may not work"
                )

    if not instruments_with_data:
        raise BacktestEngineError("No instruments with available data found")

    logger.info(
        f"ğŸ“Š Found {len(instruments_with_data)}/{len(cfg.instruments)} instruments with data"
    )
    return instruments_with_data


def _load_instruments(engine: BacktestEngine, instruments_with_data: List) -> Dict:
    """åŠ è½½æ ‡çš„å®šä¹‰ï¼ˆè‡ªåŠ¨æ·»åŠ  SPOT æ ‡çš„ç”¨äºèµ„é‡‘è´¹ç‡å¥—åˆ©ï¼‰"""
    from core.schemas import InstrumentConfig, InstrumentType

    # è‡ªåŠ¨æ·»åŠ  SPOT æ ‡çš„ï¼ˆç”¨äºèµ„é‡‘è´¹ç‡å¥—åˆ©ç­–ç•¥ï¼‰
    # å¦‚æœå‘ç° PERP æ ‡çš„ï¼Œè‡ªåŠ¨æ·»åŠ å¯¹åº”çš„ SPOT æ ‡çš„
    inst_cfg_list = list(instruments_with_data)  # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿ä¿®æ”¹
    perp_instruments = [cfg for cfg in inst_cfg_list if "-PERP" in cfg.instrument_id]

    for perp_cfg in perp_instruments:
        # æ¨å¯¼ SPOT ID: BTCUSDT-PERP.BINANCE -> BTCUSDT.BINANCE
        spot_id = perp_cfg.instrument_id.replace("-PERP", "")

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if not any(cfg.instrument_id == spot_id for cfg in inst_cfg_list):
            # åˆ›å»º SPOT æ ‡çš„é…ç½®
            spot_cfg = InstrumentConfig(
                type=InstrumentType.SPOT,
                venue_name=perp_cfg.venue_name,
                base_currency=perp_cfg.base_currency,
                quote_currency=perp_cfg.quote_currency,
                leverage=1,  # SPOT ä¸ä½¿ç”¨æ æ†
            )
            inst_cfg_list.append(spot_cfg)
            logger.info(f"ğŸ”„ Auto-added SPOT instrument for funding arbitrage: {spot_id}")

    loaded_instruments = {}

    for inst_cfg in inst_cfg_list:
        inst_path = inst_cfg.get_json_path()
        if not inst_path.exists():
            raise InstrumentLoadError(
                f"Instrument path not found: {inst_path}", inst_cfg.instrument_id
            )

        try:
            inst = load_instrument(inst_path)
            engine.add_instrument(inst)
            loaded_instruments[str(inst.id)] = inst
            logger.info(f"âœ… Loaded instrument: {inst.id}")
        except Exception as e:
            raise InstrumentLoadError(
                f"Failed to load instrument {inst_cfg.instrument_id}: {e}",
                inst_cfg.instrument_id,
                e,
            )

    return loaded_instruments


def _load_data_feeds(
    engine: BacktestEngine, cfg: BacktestConfig, base_dir: Path, loaded_instruments: Dict
) -> Dict:
    """åŠ è½½å›æµ‹æ•°æ®"""
    from core.schemas import DataConfig

    all_feeds = {}

    # ä¸ºè‡ªåŠ¨æ·»åŠ çš„ SPOT æ ‡çš„åˆ›å»ºæ•°æ®æºé…ç½®
    data_feeds_to_load = list(cfg.data_feeds)

    logger.info(f"ğŸ” Initial data_feeds_to_load: {[f.instrument_id for f in data_feeds_to_load]}")

    # æ£€æŸ¥æ˜¯å¦æœ‰ SPOT æ ‡çš„éœ€è¦åŠ è½½æ•°æ®
    for inst_id_str in loaded_instruments.keys():
        # å¦‚æœæ˜¯ SPOT æ ‡çš„ä¸”ä¸åœ¨ç°æœ‰ data_feeds ä¸­
        is_spot = "-PERP" not in inst_id_str
        has_feed = any(df.instrument_id == inst_id_str for df in data_feeds_to_load)
        logger.info(f"ğŸ” Checking {inst_id_str}: is_spot={is_spot}, has_feed={has_feed}")

        if is_spot and not has_feed:
            # æŸ¥æ‰¾å¯¹åº”çš„ PERP é…ç½®ä½œä¸ºæ¨¡æ¿
            perp_id = inst_id_str.replace(".BINANCE", "-PERP.BINANCE")
            perp_feed = next((df for df in cfg.data_feeds if perp_id in df.instrument_id), None)

            if perp_feed:
                # åˆ›å»º SPOT æ•°æ®æºé…ç½®ï¼ˆå¤åˆ¶ PERP çš„é…ç½®ï¼‰
                spot_symbol = inst_id_str.split(".")[0]
                spot_feed = DataConfig(
                    instrument_id=inst_id_str,
                    bar_aggregation=perp_feed.bar_aggregation,
                    bar_period=perp_feed.bar_period,
                    price_type=perp_feed.price_type,
                    origination=perp_feed.origination,
                    csv_file_name=f"{spot_symbol}/binance-{spot_symbol}-1h-{cfg.start_date}_{cfg.end_date}.csv",
                    label="main",
                )
                data_feeds_to_load.append(spot_feed)
                logger.info(f"ğŸ”„ Auto-created data feed for SPOT: {inst_id_str}")

    total_feeds = len(data_feeds_to_load)

    for feed_idx, data_cfg in enumerate(data_feeds_to_load, 1):
        sys.stdout.write(f"\rğŸ“– [{feed_idx}/{total_feeds}] Loading: {data_cfg.csv_file_name}")
        sys.stdout.flush()

        try:
            bt = _load_data_for_feed(engine, base_dir, cfg, data_cfg, loaded_instruments)
            if bt:
                inst_id = data_cfg.instrument_id or str(cfg.instrument.instrument_id)
                all_feeds[(inst_id, data_cfg.label)] = str(bt)
        except DataLoadError as e:
            logger.error(f"\nâŒ Failed to load {data_cfg.csv_file_name}: {e}")
            continue

    logger.info(f"\nâœ… Loaded {len(all_feeds)} data feeds")
    return all_feeds


def _add_strategies(
    engine: BacktestEngine, cfg: BacktestConfig, all_feeds: Dict, loaded_instruments: Dict
) -> int:
    """é…ç½®ä¸æ·»åŠ ç­–ç•¥"""
    StrategyClass = load_strategy_class(cfg.strategy.module_path, cfg.strategy.name)
    ConfigClass = load_strategy_config_class(
        cfg.strategy.module_path, cfg.strategy.resolve_config_class()
    )

    global_feeds = {label: bt for (iid, label), bt in all_feeds.items() if label == "benchmark"}
    strategies_count = 0

    # ç‰¹æ®Šå¤„ç†ï¼šèµ„é‡‘è´¹ç‡å¥—åˆ©ç­–ç•¥åªéœ€è¦ä¸€ä¸ªå®ä¾‹ï¼ˆåŸºäº PERP æ ‡çš„ï¼‰
    is_funding_arbitrage = cfg.strategy.name == "FundingArbitrageStrategy"

    for inst_id, inst in loaded_instruments.items():
        # å¦‚æœæ˜¯èµ„é‡‘è´¹ç‡å¥—åˆ©ç­–ç•¥ï¼Œè·³è¿‡ SPOT æ ‡çš„
        if is_funding_arbitrage and "-PERP" not in inst_id:
            continue

        local_feeds = {
            label: bt
            for (iid, label), bt in all_feeds.items()
            if iid == inst_id and label != "benchmark"
        }

        if "main" not in local_feeds:
            continue

        strat_params = cfg.strategy.resolve_params(
            instrument_id=inst.id,
            leverage=cfg.instrument.leverage if cfg.instrument else 1,
            feed_bar_types={**local_feeds, **global_feeds},
        )

        final_params = filter_strategy_params(strat_params, ConfigClass)
        strat_config = ConfigClass(**final_params)
        engine.add_strategy(StrategyClass(config=strat_config))
        strategies_count += 1

    return strategies_count


def _generate_report(cfg: BacktestConfig, base_dir: Path, engine: BacktestEngine) -> None:
    """ç”ŸæˆHTMLæŠ¥å‘Š"""
    if cfg.output_html_report:
        output_dir = base_dir / "output"
        output_dir.mkdir(parents=True, exist_ok=True)
        report_path = (
            output_dir / f"low_{cfg.strategy.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        )
        create_tearsheet(engine=engine, output_path=str(report_path))
        logger.info(f"ğŸ“Š HTML Report generated: {report_path}")


def run_low_level(cfg: BacktestConfig, base_dir: Path):
    """
    è¿è¡Œä½çº§å¼•æ“å›æµ‹ (Low-level Engine)

    ä¿æŒ BacktestEngine ç›´æ¥ API çš„ç®€æ´ç‰¹æ€§ï¼Œä½†å¢å¼ºé”™è¯¯å¤„ç†å’ŒåŠŸèƒ½å®Œæ•´æ€§ã€‚

    Args:
        cfg: å›æµ‹é…ç½®
        base_dir: é¡¹ç›®åŸºç¡€ç›®å½•

    Raises:
        BacktestEngineError: å½“å›æµ‹æ‰§è¡Œå¤±è´¥æ—¶
        InstrumentLoadError: å½“æ ‡çš„åŠ è½½å¤±è´¥æ—¶
        DataLoadError: å½“æ•°æ®åŠ è½½å¤±è´¥æ—¶
    """
    logger.info(f"ğŸš€ Starting Low-Level Backtest: {cfg.strategy.name}")

    try:
        engine = _setup_engine(cfg, base_dir)
        instruments_with_data = _filter_instruments_with_data(cfg, base_dir)
        loaded_instruments = _load_instruments(engine, instruments_with_data)
        all_feeds = _load_data_feeds(engine, cfg, base_dir, loaded_instruments)

        # åœ¨ bar æ•°æ®åŠ è½½ä¹‹åã€ç­–ç•¥æ·»åŠ ä¹‹å‰åŠ è½½è‡ªå®šä¹‰æ•°æ®
        try:
            _load_custom_data_to_engine(cfg, base_dir, engine, loaded_instruments)
        except CustomDataError as e:
            logger.warning(f"âš ï¸ Custom data loading failed: {e}")

        strategies_count = _add_strategies(engine, cfg, all_feeds, loaded_instruments)

        if strategies_count == 0:
            raise BacktestEngineError(
                "No strategy instances were created. Check config and data paths."
            )

        # è°ƒè¯•ï¼šæ£€æŸ¥å¼•æ“ä¸­çš„æ•°æ®

        logger.info(f"â³ Running engine with {strategies_count} strategy instances...")
        engine.run()
        logger.info("âœ… Backtest Complete.")

        _process_backtest_results(cfg, base_dir, engine)
        _generate_report(cfg, base_dir, engine)

        gc.collect()
        engine.reset()
        engine.dispose()
        logger.info("ğŸ§¹ Engine resources cleaned up")

    except (InstrumentLoadError, DataLoadError, CustomDataError) as e:
        logger.error(f"âŒ Backtest failed: {e}")
        raise
    except Exception as e:
        raise BacktestEngineError(f"Unexpected error during backtest: {e}", e)
