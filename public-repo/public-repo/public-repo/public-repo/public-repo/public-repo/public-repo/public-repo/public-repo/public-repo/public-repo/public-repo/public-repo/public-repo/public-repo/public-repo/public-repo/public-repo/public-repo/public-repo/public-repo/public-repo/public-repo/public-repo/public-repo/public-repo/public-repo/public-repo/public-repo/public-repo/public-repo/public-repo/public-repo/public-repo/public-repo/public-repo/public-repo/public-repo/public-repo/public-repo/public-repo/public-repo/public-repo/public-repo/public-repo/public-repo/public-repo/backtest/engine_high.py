import gc
import json
import logging
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import yaml
from nautilus_trader.backtest.config import (
    BacktestDataConfig,
    BacktestRunConfig,
    BacktestVenueConfig,
    ImportableFeeModelConfig,
)
from nautilus_trader.backtest.node import (
    BacktestEngineConfig,
    BacktestNode,
)
from nautilus_trader.backtest.results import BacktestResult
from nautilus_trader.common.config import LoggingConfig
from nautilus_trader.config import ImportableStrategyConfig, RiskEngineConfig
from nautilus_trader.model.data import Bar, BarType
from nautilus_trader.model.instruments import Instrument
from nautilus_trader.persistence.catalog import ParquetDataCatalog
from nautilus_trader.persistence.wranglers import BarDataWrangler
from pandas import DataFrame

from backtest.exceptions import (
    BacktestEngineError,
    CatalogError,
    DataLoadError,
    InstrumentLoadError,
)
from core.schemas import BacktestConfig
from strategy.core.loader import (
    filter_strategy_params,
    load_strategy_config_class,
)
from utils.data_file_checker import check_single_data_file
from utils.filename_parser import FilenameParser
from utils.instrument_loader import load_instrument

logger = logging.getLogger(__name__)



# ============================================================
# æ•°æ®åŠ è½½æ¨¡å—
# ============================================================

# ============================================================
# æ•°æ®åŠ è½½æ¨¡å—
# ============================================================

def _extract_symbol_from_instrument_id(inst_id: str) -> str:
    """ä»instrument_idæå–ç¬¦å·"""
    return inst_id.split("-")[0] if "-" in inst_id else inst_id.split(".")[0]


def _build_timeframe_string(cfg: BacktestConfig) -> str:
    """æ„å»ºæ—¶é—´å‘¨æœŸå­—ç¬¦ä¸²"""
    if not cfg.data_feeds:
        return "1h"

    first_feed = cfg.data_feeds[0]
    from nautilus_trader.model.enums import BarAggregation
    unit_map = {
        BarAggregation.MINUTE: "m",
        BarAggregation.HOUR: "h",
        BarAggregation.DAY: "d"
    }
    return f"{first_feed.bar_period}{unit_map.get(first_feed.bar_aggregation, 'h')}"


def _check_instrument_data_availability(
    inst_id: str,
    inst_cfg,
    cfg: BacktestConfig,
    base_dir: Path,
    timeframe: str
) -> bool:
    """æ£€æŸ¥å•ä¸ªæ ‡çš„çš„æ•°æ®å¯ç”¨æ€§"""
    symbol = _extract_symbol_from_instrument_id(inst_id)

    has_data, _ = check_single_data_file(
        symbol=symbol,
        start_date=cfg.start_date,
        end_date=cfg.end_date,
        timeframe=timeframe,
        exchange=inst_cfg.venue_name.lower(),
        base_dir=base_dir,
    )

    if not has_data:
        logger.debug(f"â­ï¸ Skipping {inst_id}: no data file")

    return has_data


def _filter_instruments_with_data(
    cfg: BacktestConfig,
    inst_cfg_map: Dict[str, any],
    base_dir: Path
) -> list[str]:
    """è¿‡æ»¤æœ‰æ•°æ®çš„æ ‡çš„"""
    if not cfg.start_date or not cfg.end_date:
        logger.warning("âš ï¸ start_date æˆ– end_date æœªé…ç½®ï¼Œè·³è¿‡æ•°æ®å¯ç”¨æ€§æ£€æŸ¥")
        return list(inst_cfg_map.keys())

    timeframe = _build_timeframe_string(cfg)
    instruments_with_data = []

    for inst_id, inst_cfg in inst_cfg_map.items():
        if _check_instrument_data_availability(inst_id, inst_cfg, cfg, base_dir, timeframe):
            instruments_with_data.append(inst_id)

    if not instruments_with_data:
        raise InstrumentLoadError("No instruments with available data found", "all")

    logger.info(f"ğŸ“Š Found {len(instruments_with_data)}/{len(inst_cfg_map)} instruments with data")
    return instruments_with_data


def _load_single_instrument(inst_id: str, inst_cfg) -> Instrument:
    """åŠ è½½å•ä¸ªæ ‡çš„"""
    inst_path = inst_cfg.get_json_path()

    if not inst_path.exists():
        raise InstrumentLoadError(
            f"Instrument path not found: {inst_path}", inst_id
        )

    try:
        return load_instrument(inst_path)
    except Exception as e:
        raise InstrumentLoadError(
            f"Failed to load instrument {inst_id}: {e}", inst_id, e
        )


def _load_instruments(cfg: BacktestConfig, base_dir: Path) -> Dict[str, Instrument]:
    """
    åŠ è½½äº¤æ˜“æ ‡çš„ä¿¡æ¯ï¼ˆå¸¦æ•°æ®å¯ç”¨æ€§æ£€æŸ¥ï¼‰

    Args:
        cfg: å›æµ‹é…ç½®
        base_dir: é¡¹ç›®æ ¹ç›®å½•

    Returns:
        Dict[str, Instrument]: instrument_id -> Instrument çš„æ˜ å°„

    Raises:
        InstrumentLoadError: å½“æ ‡çš„åŠ è½½å¤±è´¥æ—¶
    """
    inst_cfg_map = {ic.instrument_id: ic for ic in cfg.instruments}

    # è¿‡æ»¤æœ‰æ•°æ®çš„æ ‡çš„
    instruments_with_data = _filter_instruments_with_data(cfg, inst_cfg_map, base_dir)

    # åŠ è½½æ ‡çš„
    loaded_instruments = {}
    for inst_id in instruments_with_data:
        inst_cfg = inst_cfg_map[inst_id]
        loaded_instruments[inst_id] = _load_single_instrument(inst_id, inst_cfg)

    return loaded_instruments


# ============================================================
# æ•°æ®éªŒè¯æ¨¡å—
# ============================================================


def _check_parquet_coverage(
    catalog: ParquetDataCatalog,
    bar_type: BarType,
    cfg: BacktestConfig,
) -> Tuple[bool, float]:
    """
    æ£€æŸ¥Parquetæ•°æ®è¦†ç›–ç‡

    Returns:
        Tuple[bool, float]: (æ˜¯å¦å­˜åœ¨, è¦†ç›–ç‡ç™¾åˆ†æ¯”)
    """
    try:
        existing_intervals = catalog.get_intervals(
            data_cls=Bar, identifier=str(bar_type)
        )
        if not existing_intervals:
            return False, 0.0
    except (KeyError, AttributeError) as e:
        logger.warning(f"Failed to get intervals for {bar_type}: {e}")
        return False, 0.0

    # ç®€åŒ–é€»è¾‘ï¼šå¦‚æœ Parquet æ•°æ®å­˜åœ¨åˆ™è¿”å› True
    return True, 100.0


# ============================================================
# Parquet æ•°æ®å¤„ç†æ¨¡å—
# ============================================================


def _format_status_message(feed_idx: int, total_feeds: int, status: str, inst_id: str, extra: str = "") -> str:
    """æ ¼å¼åŒ–çŠ¶æ€æ¶ˆæ¯"""
    return f"\r{status} [{feed_idx:3d}/{total_feeds}] {extra}: {str(inst_id):<32}"


def _update_parquet_from_csv(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    inst: Instrument,
    bar_type: BarType,
    feed_idx: int,
    total_feeds: int,
    action: str
) -> str:
    """ä»CSVæ›´æ–°Parquetæ•°æ®"""
    try:
        catalog_loader(catalog, csv_path, inst, bar_type)
        return _format_status_message(feed_idx, total_feeds, "ğŸ“–", bar_type.instrument_id, action)
    except (OSError, ValueError) as e:
        logger.warning(f"Failed to {action.lower()} Parquet for {bar_type.instrument_id}: {e}")
        return _format_status_message(feed_idx, total_feeds, "âš ï¸ ", bar_type.instrument_id, "Using Parquet")


def _handle_csv_exists(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    inst: Instrument,
    bar_type: BarType,
    feed_idx: int,
    total_feeds: int
) -> str:
    """å¤„ç†CSVæ–‡ä»¶å­˜åœ¨çš„æƒ…å†µ"""
    is_consistent = _verify_data_consistency(csv_path, catalog, bar_type)

    if not is_consistent:
        return _update_parquet_from_csv(catalog, csv_path, inst, bar_type, feed_idx, total_feeds, "Updated")
    else:
        return _format_status_message(feed_idx, total_feeds, "â©", bar_type.instrument_id, "Verified")


def _handle_incomplete_parquet(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    inst: Instrument,
    bar_type: BarType,
    data_cfg,
    cfg: BacktestConfig,
    feed_idx: int,
    total_feeds: int,
    coverage_pct: float
) -> str:
    """å¤„ç†Parquetæ•°æ®ä¸å®Œæ•´çš„æƒ…å†µ"""
    download_success = _auto_download_missing_data(data_cfg, cfg)

    if download_success and csv_path.exists():
        return _update_parquet_from_csv(catalog, csv_path, inst, bar_type, feed_idx, total_feeds, "Completed")
    else:
        return f"\râš ï¸  [{feed_idx:3d}/{total_feeds}] Partial Parquet: {str(bar_type.instrument_id):<25} ({coverage_pct:.1f}%)"


def _handle_csv_missing(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    inst: Instrument,
    bar_type: BarType,
    data_cfg,
    cfg: BacktestConfig,
    feed_idx: int,
    total_feeds: int
) -> str:
    """å¤„ç†CSVæ–‡ä»¶ä¸å­˜åœ¨çš„æƒ…å†µ"""
    # æ£€æŸ¥Parquetè¦†ç›–ç‡
    parquet_exists, coverage_pct = _check_parquet_coverage(catalog, bar_type, cfg)

    if parquet_exists and coverage_pct < 95.0:
        return _handle_incomplete_parquet(
            catalog, csv_path, inst, bar_type, data_cfg, cfg, feed_idx, total_feeds, coverage_pct
        )
    else:
        # å°è¯•å¸¸è§„ä¸‹è½½
        download_success = _auto_download_missing_data(data_cfg, cfg)

        if download_success and csv_path.exists():
            return _update_parquet_from_csv(catalog, csv_path, inst, bar_type, feed_idx, total_feeds, "Imported")
        else:
            return f"\râš ï¸  [{feed_idx:3d}/{total_feeds}] Partial Parquet: {str(bar_type.instrument_id):<25}"


def _handle_parquet_exists(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    inst: Instrument,
    bar_type: BarType,
    data_cfg,
    cfg: BacktestConfig,
    feed_idx: int,
    total_feeds: int,
) -> str:
    """
    å¤„ç†Parquetæ•°æ®å·²å­˜åœ¨çš„æƒ…å†µ

    Returns:
        str: çŠ¶æ€æ¶ˆæ¯ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    """
    csv_exists = csv_path.exists()

    if csv_exists:
        return _handle_csv_exists(catalog, csv_path, inst, bar_type, feed_idx, total_feeds)
    else:
        return _handle_csv_missing(catalog, csv_path, inst, bar_type, data_cfg, cfg, feed_idx, total_feeds)


def _import_csv_to_parquet(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    inst: Instrument,
    bar_type: BarType,
    feed_idx: int,
    total_feeds: int
) -> str:
    """å¯¼å…¥CSVåˆ°Parquet"""
    catalog_loader(catalog, csv_path, inst, bar_type)
    return f"\rğŸ“– [{feed_idx:3d}/{total_feeds}] Imported: {str(bar_type.instrument_id):<32}"


def _handle_csv_exists_for_missing_parquet(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    inst: Instrument,
    bar_type: BarType,
    data_cfg,
    feed_idx: int,
    total_feeds: int
) -> str:
    """å¤„ç†CSVå­˜åœ¨ä½†Parquetç¼ºå¤±çš„æƒ…å†µ"""
    try:
        return _import_csv_to_parquet(catalog, csv_path, inst, bar_type, feed_idx, total_feeds)
    except Exception as e:
        raise DataLoadError(
            f"Error loading {data_cfg.csv_file_name}: {e}", str(csv_path), e
        )


def _handle_csv_missing_for_missing_parquet(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    inst: Instrument,
    bar_type: BarType,
    data_cfg,
    cfg: BacktestConfig,
    feed_idx: int,
    total_feeds: int
) -> str:
    """å¤„ç†CSVå’ŒParquetéƒ½ç¼ºå¤±çš„æƒ…å†µ"""
    download_success = _auto_download_missing_data(data_cfg, cfg)

    if download_success and csv_path.exists():
        try:
            return _import_csv_to_parquet(catalog, csv_path, inst, bar_type, feed_idx, total_feeds)
        except (OSError, ValueError, KeyError) as e:
            logger.warning(f"Failed to import {bar_type.instrument_id}: {e}")

    # æ•°æ®å®Œå…¨ç¼ºå¤±
    raise DataLoadError(
        f"Critical data missing for {bar_type.instrument_id}. "
        f"CSV file not found: {csv_path}, Parquet data not available, "
        f"and auto-download failed.",
        str(csv_path),
    )


def _handle_parquet_missing(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    inst: Instrument,
    bar_type: BarType,
    data_cfg,
    cfg: BacktestConfig,
    feed_idx: int,
    total_feeds: int,
) -> str:
    """
    å¤„ç†Parquetæ•°æ®ä¸å­˜åœ¨çš„æƒ…å†µ

    Returns:
        str: çŠ¶æ€æ¶ˆæ¯

    Raises:
        DataLoadError: å½“æ•°æ®å®Œå…¨ç¼ºå¤±æ—¶
    """
    csv_exists = csv_path.exists()

    if csv_exists:
        return _handle_csv_exists_for_missing_parquet(
            catalog, csv_path, inst, bar_type, data_cfg, feed_idx, total_feeds
        )
    else:
        return _handle_csv_missing_for_missing_parquet(
            catalog, csv_path, inst, bar_type, data_cfg, cfg, feed_idx, total_feeds
        )


def _auto_download_data(data_cfg, cfg: BacktestConfig) -> bool:
    """
    è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ•°æ®æ–‡ä»¶

    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    try:
        from utils.data_management.data_retrieval import batch_fetch_ohlcv

        parsed = FilenameParser.parse(data_cfg.csv_file_name)
        if not parsed:
            return False

        base_dir = Path(__file__).parent.parent
        configs = batch_fetch_ohlcv(
            symbols=[parsed.symbol],
            start_date=parsed.start_date,
            end_date=parsed.end_date,
            timeframe=parsed.timeframe,
            exchange_id=parsed.exchange,
            base_dir=base_dir,
        )

        if configs:
            csv_path = data_cfg.full_path
            return csv_path.exists() and csv_path.stat().st_size > 1024

        return False
    except (IOError, ImportError) as e:
        logger.error(f"Failed to auto-download data: {e}")
        return False


def _process_data_feed(
    feed_idx: int,
    total_feeds: int,
    data_cfg,
    cfg: BacktestConfig,
    loaded_instruments: Dict[str, Instrument],
    _catalog,
) -> Tuple[Optional[str], Optional[str], Optional[str], str]:
    """
    å¤„ç†å•ä¸ªæ•°æ®æµçš„å¯¼å…¥
    
    Returns
    -------
    Tuple[Optional[str], Optional[str], Optional[str], str]
        (inst_id, feed_bar_type_str, bar_type_str, status_msg)
    """
    inst_id = data_cfg.instrument_id or (cfg.instrument.instrument_id if cfg.instrument else None)
    if inst_id not in loaded_instruments:
        return None, None, None, ""

    inst = loaded_instruments[inst_id]
    csv_path = data_cfg.full_path
    feed_bar_type_str = f"{inst.id}-{data_cfg.bar_type_str}"
    bar_type = BarType.from_str(feed_bar_type_str)

    # æ£€æŸ¥Parquetæ•°æ®è¦†ç›–ç‡
    parquet_exists, coverage_pct = _check_parquet_coverage(_catalog, bar_type, cfg)

    # å¤„ç†æ•°æ®å¯¼å…¥
    if parquet_exists and coverage_pct >= 80:
        status_msg = f"\râœ… [{feed_idx:3d}/{total_feeds}] Ready: {str(bar_type.instrument_id):<35}"
    elif parquet_exists:
        status_msg = _handle_parquet_exists(
            _catalog, csv_path, inst, bar_type, data_cfg, cfg, feed_idx, total_feeds
        )
    else:
        status_msg = _handle_parquet_missing(
            _catalog, csv_path, inst, bar_type, data_cfg, cfg, feed_idx, total_feeds
        )

    return inst_id, feed_bar_type_str, str(bar_type), status_msg


def _categorize_data_feed(
    inst_id: str,
    feed_bar_type_str: str,
    data_cfg,
    global_feeds: Dict,
    feeds_by_inst: Dict,
):
    """å°†æ•°æ®æµå½’ç±»åˆ°å…¨å±€æˆ–æŒ‰instrumentåˆ†ç±»"""
    if data_cfg.label == "benchmark":
        global_feeds[data_cfg.label] = feed_bar_type_str
    else:
        feeds_by_inst[inst_id][data_cfg.label] = feed_bar_type_str


def _update_data_config(
    inst_id_str: str,
    inst,
    bar_type_str: str,
    catalog_path: Path,
    data_config_by_inst: Dict,
):
    """æ›´æ–°instrumentçš„æ•°æ®é…ç½®"""
    if inst_id_str not in data_config_by_inst:
        data_config_by_inst[inst_id_str] = {
            "instrument_id": inst.id,
            "bar_types": [],
            "catalog_path": str(catalog_path),
        }

    if bar_type_str not in data_config_by_inst[inst_id_str]["bar_types"]:
        data_config_by_inst[inst_id_str]["bar_types"].append(bar_type_str)


def _import_data_to_catalog(
    cfg: BacktestConfig,
    loaded_instruments: Dict[str, Instrument],
    catalog_path: Path,
) -> Tuple[Dict[str, Dict], Dict[str, str], Dict]:
    """
    å°†CSVæ•°æ®å¯¼å…¥Parquetç›®å½•ï¼Œå¹¶ç»„ç»‡æ•°æ®æµ

    æ™ºèƒ½æ•°æ®å¤„ç†ï¼šæ£€æŸ¥Parquetè¦†ç›–ç‡ï¼ŒæŒ‰éœ€å¯¼å…¥CSVæˆ–ä¸‹è½½æ•°æ®
    """
    _catalog = ParquetDataCatalog(catalog_path)
    data_config_by_inst = {}
    feeds_by_inst = defaultdict(dict)
    global_feeds = {}
    total_feeds = len(cfg.data_feeds)

    for feed_idx, data_cfg in enumerate(cfg.data_feeds, 1):
        inst_id, feed_bar_type_str, bar_type_str, status_msg = _process_data_feed(
            feed_idx, total_feeds, data_cfg, cfg, loaded_instruments, _catalog
        )

        if inst_id is None:
            continue

        sys.stdout.write(status_msg)
        sys.stdout.flush()

        # å½’ç±»æ•°æ®æµ
        _categorize_data_feed(inst_id, feed_bar_type_str, data_cfg, global_feeds, feeds_by_inst)

        # æ›´æ–°æ•°æ®é…ç½®
        inst = loaded_instruments[inst_id]
        _update_data_config(str(inst.id), inst, bar_type_str, catalog_path, data_config_by_inst)

    logger.info(f"\nâœ… Data import complete. Created {len(data_config_by_inst)} data configs.")
    return data_config_by_inst, global_feeds, feeds_by_inst


def _auto_download_missing_data(data_cfg, backtest_cfg: BacktestConfig) -> bool:
    """è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ•°æ®æ–‡ä»¶"""
    return _auto_download_data(data_cfg, backtest_cfg)


def _clear_parquet_data(catalog: ParquetDataCatalog, bar_type: BarType) -> None:
    """
    æ¸…ç†æŒ‡å®šbar_typeçš„Parquetæ•°æ®

    Args:
        catalog: Parquetæ•°æ®ç›®å½•
        bar_type: è¦æ¸…ç†çš„æ•°æ®ç±»å‹æ ‡è¯†
    """
    try:
        import shutil

        # è·å–catalogçš„æ ¹è·¯å¾„
        catalog_root = Path(catalog.path)

        # NautilusTraderå®é™…çš„æ–‡ä»¶ç»“æ„æ˜¯: data/crypto_perpetual/{instrument_id}/
        instrument_id = str(bar_type.instrument_id)

        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ•°æ®ç›®å½•
        possible_dirs = [
            catalog_root / "data" / "crypto_perpetual" / instrument_id,
            catalog_root / "data" / "Bar" / instrument_id,
            catalog_root / "data" / instrument_id,
        ]

        for data_dir in possible_dirs:
            if data_dir.exists():
                logger.info(f"   ğŸ—‘ï¸ Clearing existing Parquet data: {data_dir}")
                shutil.rmtree(data_dir)
                break
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†ç›®å½•ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«instrument_idçš„ä»»ä½•ç›®å½•
            import os
            for root, dirs, files in os.walk(catalog_root):
                if instrument_id in root and any(
                    f.endswith(".parquet") for f in files
                ):
                    logger.info(f"   ğŸ—‘ï¸ Clearing found Parquet data: {root}")
                    shutil.rmtree(root)
                    break

    except Exception as e:
        logger.warning(f"   âš ï¸ Warning: Could not clear Parquet data: {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºè¿™ä¸æ˜¯è‡´å‘½é”™è¯¯


def _check_csv_file_validity(csv_path: Path) -> bool:
    """æ£€æŸ¥CSVæ–‡ä»¶åŸºæœ¬æœ‰æ•ˆæ€§"""
    return csv_path.exists() and csv_path.stat().st_size >= 1024


def _get_parquet_intervals(catalog: ParquetDataCatalog, bar_type: BarType) -> list:
    """è·å–Parquetæ•°æ®æ—¶é—´é—´éš”"""
    try:
        existing_intervals = catalog.get_intervals(
            data_cls=Bar, identifier=str(bar_type)
        )
        return existing_intervals if existing_intervals else []
    except (KeyError, AttributeError) as e:
        logger.warning(f"Failed to get intervals in verification: {e}")
        return []


def _get_parquet_mtime(catalog: ParquetDataCatalog, bar_type: BarType) -> float | None:
    """è·å–Parquetæ–‡ä»¶çš„æœ€æ–°ä¿®æ”¹æ—¶é—´"""
    catalog_root = Path(catalog.path)
    instrument_id = str(bar_type.instrument_id)
    parquet_dir = catalog_root / "data" / "crypto_perpetual" / instrument_id

    if parquet_dir.exists():
        parquet_files = list(parquet_dir.glob("*.parquet"))
        if parquet_files:
            return max(f.stat().st_mtime for f in parquet_files)
    return None


def _check_file_freshness(csv_mtime: float, parquet_mtime: float | None) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ–°é²œåº¦ï¼ˆCSVæ˜¯å¦æ¯”Parquetæ–°å¤ªå¤šï¼‰"""
    if parquet_mtime is None:
        return True
    # å¦‚æœCSVæ–‡ä»¶æ¯”Parquetæ–‡ä»¶æ–°è¶…è¿‡1å°æ—¶ï¼Œè®¤ä¸ºä¸ä¸€è‡´
    return csv_mtime - parquet_mtime <= 3600


def _count_csv_lines(csv_path: Path) -> int:
    """å¿«é€Ÿç»Ÿè®¡CSVè¡Œæ•°"""
    try:
        with open(csv_path) as f:
            return sum(1 for _ in f) - 1  # å‡å»header
    except (IOError, ValueError):
        return 0


def _estimate_parquet_count(existing_intervals: list) -> int:
    """ä¼°ç®—Parquetæ•°æ®é‡"""
    if not existing_intervals:
        return 0
    interval = existing_intervals[0]
    # å‡è®¾1å°æ—¶æ•°æ®ï¼Œä¼°ç®—æ•°æ®ç‚¹æ•°é‡
    return (interval[1] - interval[0]) // (3600 * 1_000_000_000)  # çº³ç§’è½¬å°æ—¶


def _check_data_count_consistency(csv_line_count: int, estimated_parquet_count: int) -> bool:
    """æ£€æŸ¥æ•°æ®é‡ä¸€è‡´æ€§ï¼ˆå…è®¸20%å·®å¼‚ï¼‰"""
    if csv_line_count == 0:
        return False
    diff_ratio = abs(csv_line_count - estimated_parquet_count) / max(csv_line_count, 1)
    return diff_ratio <= 0.2


def _verify_data_consistency(
    csv_path: Path, catalog: ParquetDataCatalog, bar_type: BarType
) -> bool:
    """
    è½»é‡çº§éªŒè¯CSVå’ŒParquetæ•°æ®çš„ä¸€è‡´æ€§

    ç®€åŒ–æ£€æŸ¥ç­–ç•¥ï¼ˆæ€§èƒ½ä¼˜å…ˆï¼‰ï¼š
    1. æ¯”è¾ƒæ–‡ä»¶ä¿®æ”¹æ—¶é—´
    2. æ¯”è¾ƒæ•°æ®è¡Œæ•°ï¼ˆå¿«é€Ÿä¼°ç®—ï¼‰
    3. æ¯”è¾ƒæ–‡ä»¶å¤§å°èŒƒå›´

    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        catalog: Parquetæ•°æ®ç›®å½•
        bar_type: æ•°æ®ç±»å‹æ ‡è¯†

    Returns:
        bool: æ•°æ®æ˜¯å¦ä¸€è‡´
    """
    try:
        # 1. æ£€æŸ¥CSVæ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        if not _check_csv_file_validity(csv_path):
            return False

        # 2. å¿«é€Ÿæ£€æŸ¥Parquetæ•°æ®æ˜¯å¦å­˜åœ¨
        existing_intervals = _get_parquet_intervals(catalog, bar_type)
        if not existing_intervals:
            return False

        # 3. æ¯”è¾ƒæ–‡ä»¶ä¿®æ”¹æ—¶é—´
        csv_mtime = csv_path.stat().st_mtime
        parquet_mtime = _get_parquet_mtime(catalog, bar_type)

        if not _check_file_freshness(csv_mtime, parquet_mtime):
            return False

        # 4. å¿«é€Ÿæ¯”è¾ƒæ•°æ®é‡
        try:
            csv_line_count = _count_csv_lines(csv_path)
            estimated_parquet_count = _estimate_parquet_count(existing_intervals)

            if not _check_data_count_consistency(csv_line_count, estimated_parquet_count):
                return False
        except (IOError, ValueError) as e:
            # å¦‚æœå¿«é€Ÿæ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºä¸€è‡´ï¼ˆé¿å…é˜»å¡ï¼‰
            logger.debug(f"Quick check failed, assuming consistent: {e}")

        return True

    except (IOError, OSError) as e:
        logger.warning(f"Data consistency verification failed: {e}")
        return False


# ============================================================
# ç­–ç•¥é…ç½®æ¨¡å—
# ============================================================


def _create_strategy_configs(
    cfg: BacktestConfig,
    loaded_instruments: Dict[str, Instrument],
    feeds_by_inst: Dict,
    global_feeds: Dict[str, str],
    ConfigClass,
) -> List[ImportableStrategyConfig]:
    """
    åˆ›å»ºç­–ç•¥é…ç½®

    Args:
        cfg: å›æµ‹é…ç½®
        loaded_instruments: å·²åŠ è½½çš„æ ‡çš„æ˜ å°„
        feeds_by_inst: æŒ‰æ ‡çš„åˆ†ç»„çš„æ•°æ®æµ
        global_feeds: å…¨å±€æ•°æ®æµ
        ConfigClass: ç­–ç•¥é…ç½®ç±»

    Returns:
        List[ImportableStrategyConfig]: ç­–ç•¥é…ç½®åˆ—è¡¨
    """
    strategies = []
    strategy_path = f"{cfg.strategy.module_path}:{cfg.strategy.name}"
    config_path = f"{cfg.strategy.module_path}:{cfg.strategy.resolve_config_class()}"

    for inst_id, inst in loaded_instruments.items():
        local_feeds = feeds_by_inst.get(inst_id, {})

        # åªæœ‰æ‹¥æœ‰ 'main' æ•°æ®æµçš„æ ‡çš„æ‰åˆ›å»ºç­–ç•¥å®ä¾‹
        if "main" not in local_feeds:
            continue

        # åˆå¹¶æœ¬åœ°å’Œå…¨å±€æ•°æ®æµ
        combined_feeds = {**local_feeds, **global_feeds}

        # ç”Ÿæˆå¹¶è¿‡æ»¤å‚æ•°
        strat_params = cfg.strategy.resolve_params(
            instrument_id=inst.id,
            leverage=cfg.instrument.leverage if cfg.instrument else 1,
            feed_bar_types=combined_feeds,
        )

        final_params = filter_strategy_params(strat_params, ConfigClass)

        strategies.append(
            ImportableStrategyConfig(
                strategy_path=strategy_path,
                config_path=config_path,
                config=final_params,
            )
        )

    return strategies


# ============================================================
# å›æµ‹æ‰§è¡Œæ¨¡å—
# ============================================================


def run_high_level(cfg: BacktestConfig, base_dir: Path):
    """
    è¿è¡Œé«˜çº§å¼•æ“å›æµ‹ (High-level Engine/BacktestNode)

    Args:
        cfg: å›æµ‹é…ç½®
        base_dir: é¡¹ç›®æ ¹ç›®å½•
    """
    logger.info(f"ğŸš€ Starting High-Level Backtest: {cfg.strategy.name}")

    try:
        # 1. å‡†å¤‡é…ç½®ç±»ï¼ˆç”¨äºå‚æ•°è¿‡æ»¤ï¼‰
        ConfigClass = load_strategy_config_class(
            cfg.strategy.module_path, cfg.strategy.resolve_config_class()
        )

        # 2. åŠ è½½äº¤æ˜“æ ‡çš„ï¼ˆå¸¦æ•°æ®å¯ç”¨æ€§æ£€æŸ¥ï¼‰
        loaded_instruments = _load_instruments(cfg, base_dir)
        logger.info(f"âœ… Loaded {len(loaded_instruments)} instruments")

        # 3. å¯¼å…¥æ•°æ®åˆ°Parquetç›®å½•
        data_path = base_dir / "data"
        catalog_path = data_path / "parquet" / f"{cfg.strategy.name}"

        data_config_by_inst, global_feeds, feeds_by_inst = _import_data_to_catalog(
            cfg, loaded_instruments, catalog_path
        )

        # 4. åˆ›å»ºç­–ç•¥é…ç½®
        strategies = _create_strategy_configs(
            cfg, loaded_instruments, feeds_by_inst, global_feeds, ConfigClass
        )
        logger.info(f"âœ… Created {len(strategies)} strategy configurations")

        # 5. å‡†å¤‡å›æµ‹æ•°æ®é…ç½®
        backtest_data_configs = []
        for inst_id_str, cfg_data in data_config_by_inst.items():
            backtest_data_configs.append(
                BacktestDataConfig(
                    catalog_path=cfg_data["catalog_path"],
                    data_cls="nautilus_trader.model.data:Bar",
                    instrument_id=cfg_data["instrument_id"],
                    bar_types=cfg_data["bar_types"],
                    start_time=cfg.start_date,
                    end_time=cfg.end_date,
                )
            )

        # 6. è¿è¡Œå›æµ‹
        _run_backtest_with_custom_data(
            cfg, base_dir, strategies, backtest_data_configs, loaded_instruments
        )

    except (InstrumentLoadError, DataLoadError, CatalogError) as e:
        logger.error(f"âŒ Backtest failed: {e}")
        raise
    except Exception as e:
        raise BacktestEngineError(f"Unexpected error during backtest: {e}", e)


def _get_venue_name(cfg: BacktestConfig) -> str:
    """è·å–äº¤æ˜“æ‰€åç§°"""
    if cfg.instrument:
        return cfg.instrument.venue_name
    return cfg.data_feeds[0].csv_file_name.split("/")[-1].split("-")[0]


def _load_oms_type_from_config(strategies: List, base_dir: Path) -> str:
    """ä»ç­–ç•¥é…ç½®æ–‡ä»¶åŠ è½½OMSç±»å‹"""
    if not strategies or not hasattr(strategies[0], 'config_path'):
        return "HEDGING"

    try:
        config_path = base_dir / strategies[0].config_path
        if config_path.exists():
            with open(config_path, 'r') as f:
                strategy_config = yaml.safe_load(f)
                if 'parameters' in strategy_config and 'oms_type' in strategy_config['parameters']:
                    return strategy_config['parameters']['oms_type']
    except (IOError, yaml.YAMLError) as e:
        logger.debug(f"Failed to load OMS type from config, using default: {e}")

    return "HEDGING"


def _create_venue_configs(cfg: BacktestConfig, venue_name: str, oms_type: str) -> List:
    """åˆ›å»ºäº¤æ˜“æ‰€é…ç½®"""
    return [
        BacktestVenueConfig(
            name=venue_name,
            oms_type=oms_type,
            account_type="MARGIN",
            base_currency="USDT",
            starting_balances=cfg.initial_balances,
            fee_model=ImportableFeeModelConfig(
                fee_model_path="nautilus_trader.backtest.models:MakerTakerFeeModel",
                config_path="nautilus_trader.backtest.config:MakerTakerFeeModelConfig",
                config={},
            ),
            trade_execution=True,
        )
    ]


def _create_logging_config(cfg: BacktestConfig, base_dir: Path) -> Optional[LoggingConfig]:
    """åˆ›å»ºæ—¥å¿—é…ç½®"""
    if not cfg.logging:
        return None

    return LoggingConfig(
        log_level=cfg.logging.log_level,
        log_level_file=cfg.logging.log_level_file,
        log_component_levels=cfg.logging.log_component_levels,
        log_components_only=cfg.logging.log_components_only,
        log_directory=str(base_dir / "log" / "backtest" / "high_level"),
    )


def _run_backtest_with_custom_data(
    cfg: BacktestConfig,
    base_dir: Path,
    strategies: List[ImportableStrategyConfig],
    backtest_data_configs: List[BacktestDataConfig],
    loaded_instruments: Dict[str, Instrument],
):
    """è¿è¡ŒBacktestNodeå¹¶å¤„ç†è‡ªå®šä¹‰æ•°æ®"""
    if not strategies:
        raise BacktestEngineError(
            "No strategy instances were created. Check config and data paths."
        )

    # é…ç½®äº¤æ˜“æ‰€
    venue_name = _get_venue_name(cfg)
    oms_type = _load_oms_type_from_config(strategies, base_dir)
    venue_configs = _create_venue_configs(cfg, venue_name, oms_type)

    # é…ç½®æ—¥å¿—
    logging_config = _create_logging_config(cfg, base_dir)

    # æ„å»ºBacktestNodeé…ç½®
    eng_cfg = BacktestEngineConfig(
        strategies=strategies,
        logging=logging_config,
        risk_engine=RiskEngineConfig(bypass=False),
    )

    run_config = BacktestRunConfig(
        engine=eng_cfg,
        data=backtest_data_configs,
        venues=venue_configs,
        raise_exception=True,
    )

    # åˆ›å»ºå¹¶è¿è¡ŒBacktestNode
    gc.collect()
    node = BacktestNode(configs=[run_config])
    logger.info(f"â³ Running BacktestNode with {len(strategies)} strategy instances...")

    # åŠ è½½è‡ªå®šä¹‰æ•°æ®
    _load_custom_data_to_engine(cfg, base_dir, node, run_config, loaded_instruments)

    # è¿è¡Œå›æµ‹
    results = node.run()
    logger.info("âœ… Backtest Complete.")

    # å¤„ç†ç»“æœ
    if results:
        _process_backtest_results(cfg, base_dir, results)


# ============================================================
# è‡ªå®šä¹‰æ•°æ®åŠ è½½æ¨¡å—
# ============================================================


def _is_custom_data_type(data_type: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºè‡ªå®šä¹‰æ•°æ®ç±»å‹"""
    return data_type in ['oi', 'funding']


def _check_dict_dependency(dep: dict) -> bool:
    """æ£€æŸ¥å­—å…¸ç±»å‹çš„ä¾èµ–"""
    data_type = dep.get('data_type', '')
    return _is_custom_data_type(data_type)


def _check_object_dependency(dep) -> bool:
    """æ£€æŸ¥å¯¹è±¡ç±»å‹çš„ä¾èµ–"""
    if hasattr(dep, 'data_type'):
        return _is_custom_data_type(dep.data_type)
    return False


def _check_strategy_dependencies(strategy: ImportableStrategyConfig) -> bool:
    """æ£€æŸ¥å•ä¸ªç­–ç•¥çš„æ•°æ®ä¾èµ–"""
    if not (hasattr(strategy, 'config') and isinstance(strategy.config, dict)):
        return False

    data_deps = strategy.config.get('data_dependencies', [])
    if not data_deps:
        return False

    for dep in data_deps:
        if isinstance(dep, dict):
            if _check_dict_dependency(dep):
                return True
        elif _check_object_dependency(dep):
            return True

    return False


def _check_if_needs_custom_data(strategies: List[ImportableStrategyConfig]) -> bool:
    """æ£€æŸ¥ç­–ç•¥æ˜¯å¦éœ€è¦è‡ªå®šä¹‰æ•°æ®ï¼ˆOI/Fundingï¼‰"""
    return any(_check_strategy_dependencies(strategy) for strategy in strategies)


def _load_custom_data_to_engine(
    cfg: BacktestConfig,
    base_dir: Path,
    node: BacktestNode,
    run_config: BacktestRunConfig,
    loaded_instruments: Dict[str, Instrument],
):
    """åŠ è½½è‡ªå®šä¹‰æ•°æ®(OI, Funding Rate)åˆ°å›æµ‹å¼•æ“"""
    if not (cfg.start_date and cfg.end_date):
        logger.warning("âš ï¸ No date range specified, skipping custom data loading")
        return

    # æ³¨æ„ï¼šé«˜çº§å›æµ‹å¼•æ“æš‚ä¸æ”¯æŒ OI/Funding è‡ªå®šä¹‰æ•°æ®åŠ è½½
    # è¿™äº›æ•°æ®çš„åŠ è½½é€»è¾‘ä¸»è¦ä¸ºä½çº§å›æµ‹å¼•æ“è®¾è®¡
    # å¦‚éœ€ä½¿ç”¨ OI/Funding æ•°æ®ï¼Œè¯·ä½¿ç”¨ä½çº§å›æµ‹å¼•æ“ (--type low)
    logger.debug("ğŸ“Š Custom data (OI, Funding Rate) loading skipped in high-level engine")


# ============================================================
# ç»“æœå¤„ç†æ¨¡å—
# ============================================================





def _process_backtest_results(
    cfg: BacktestConfig, base_dir: Path, results: List[BacktestResult]
):
    """å¤„ç†å›æµ‹ç»“æœ"""
    # æ”¶é›†ç­–ç•¥ç»Ÿè®¡æ•°æ®
    filter_stats = None
    trade_metrics = None

    try:
        from strategy.keltner_rs_breakout import KeltnerRSBreakoutStrategy

        filter_stats = KeltnerRSBreakoutStrategy.get_filter_stats_summary()
        trade_metrics = KeltnerRSBreakoutStrategy.get_trade_metrics()
        KeltnerRSBreakoutStrategy.print_filter_stats_report()
        KeltnerRSBreakoutStrategy.reset_class_stats()
    except (ImportError, AttributeError):
        pass  # éKeltner RS Breakoutç­–ç•¥æ—¶è·³è¿‡

    # æå–ç­–ç•¥é…ç½®å‚æ•°
    strategy_params = cfg.strategy.params
    if hasattr(strategy_params, "model_dump"):
        strategy_config = strategy_params.model_dump()
    elif hasattr(strategy_params, "dict"):
        strategy_config = strategy_params.dict()
    elif isinstance(strategy_params, dict):
        strategy_config = strategy_params
    else:
        strategy_config = {}

    # æ„å»ºå›æµ‹é…ç½®ä¿¡æ¯
    backtest_config = {
        "start_date": str(cfg.start_date),
        "end_date": str(cfg.end_date),
        "initial_balances": cfg.initial_balances,
        "strategies_count": len(results),
    }

    # å¤„ç†æ¯ä¸ªç»“æœ
    for result in results:
        # ç»ˆç«¯è¾“å‡º
        _print_results(result)

        # æ„å»ºå®Œæ•´ç»“æœå­—å…¸
        result_dict = _build_result_dict(
            result=result,
            strategy_name=cfg.strategy.name,
            strategy_config=strategy_config,
            filter_stats=filter_stats,
            trade_metrics=trade_metrics,
            backtest_config=backtest_config,
        )

        # ä¿å­˜JSONæ–‡ä»¶
        json_path = _save_results_to_json(result_dict, cfg.strategy.name, base_dir)
        logger.info(f"ğŸ“ Complete results saved to: {json_path}")


def _build_pnl_metrics(result: BacktestResult) -> Dict[str, Any]:
    """æ„å»ºPnLæŒ‡æ ‡"""
    pnl_dict = {}
    if result.stats_pnls:
        for currency, metrics in result.stats_pnls.items():
            pnl_dict[str(currency)] = {
                k: v if v == v else None
                for k, v in metrics.items()
            }
    return pnl_dict


def _build_returns_metrics(result: BacktestResult) -> Dict[str, Any]:
    """æ„å»ºæ”¶ç›Šç‡æŒ‡æ ‡"""
    returns_dict = {}
    if result.stats_returns:
        for currency, val in result.stats_returns.items():
            returns_dict[str(currency)] = val if val == val else None
    return returns_dict


def _build_filter_stats(filter_stats: Optional[Dict[str, Dict[str, int]]]) -> Dict[str, Any]:
    """æ„å»ºè¿‡æ»¤å™¨ç»Ÿè®¡"""
    if not filter_stats:
        return {}

    total_stats = {}
    for inst_id, stats in filter_stats.items():
        for key, value in stats.items():
            total_stats[key] = total_stats.get(key, 0) + value

    signal_checks = total_stats.get("signal_checks", 0)
    all_passed = total_stats.get("all_passed", 0)

    filter_rates = {}
    if signal_checks > 0:
        filter_rates = {
            "pass_rate": (all_passed / signal_checks * 100),
            "filter_rate": ((signal_checks - all_passed) / signal_checks * 100),
        }
        for key in [
            "fail_trend", "fail_squeeze", "fail_squeeze_maturity",
            "fail_rs", "fail_extension", "fail_btc_bear",
            "fail_breakout", "fail_volume",
        ]:
            if key in total_stats:
                filter_rates[f"{key}_rate"] = total_stats[key] / signal_checks * 100

    return {
        "by_instrument": filter_stats,
        "total": total_stats,
        "rates": filter_rates,
        "instrument_count": len(filter_stats),
    }


def _calculate_basic_stats(trade_metrics: List[Dict], winners: List[Dict], losers: List[Dict]) -> Dict[str, Any]:
    """è®¡ç®—åŸºç¡€ç»Ÿè®¡æ•°æ®"""
    return {
        "total_trades": len(trade_metrics),
        "winning_trades": len(winners),
        "losing_trades": len(losers),
        "win_rate": len(winners) / len(trade_metrics) if trade_metrics else 0,
    }


def _calculate_pnl_stats(trade_metrics: List[Dict]) -> Dict[str, Any]:
    """è®¡ç®—PnLç»Ÿè®¡æ•°æ®"""
    all_pnls = [t.get("pnl_pct", 0) for t in trade_metrics]
    return {
        "total_pnl_pct": sum(all_pnls),
        "avg_pnl_pct": sum(all_pnls) / len(all_pnls) if all_pnls else 0,
        "max_winning_trade": max(all_pnls) if all_pnls else 0,
        "max_losing_trade": min(all_pnls) if all_pnls else 0,
    }


def _calculate_winner_stats(winners: List[Dict]) -> Dict[str, Any]:
    """è®¡ç®—ç›ˆåˆ©äº¤æ˜“ç»Ÿè®¡"""
    if not winners:
        return {}

    winner_pnls = [t.get("pnl_pct", 0) for t in winners]
    return {
        "avg_winning_trade": sum(winner_pnls) / len(winners),
        "avg_winner_rs_score": sum(t.get("entry_rs_score", 0) for t in winners) / len(winners),
        "avg_winner_bb_width_pct": sum(t.get("entry_bb_width_pct", 0) for t in winners) / len(winners),
        "avg_winner_volume_mult": sum(t.get("entry_volume_mult", 0) for t in winners) / len(winners),
    }


def _calculate_loser_stats(losers: List[Dict]) -> Dict[str, Any]:
    """è®¡ç®—äºæŸäº¤æ˜“ç»Ÿè®¡"""
    if not losers:
        return {}

    loser_pnls = [t.get("pnl_pct", 0) for t in losers]
    return {
        "avg_losing_trade": sum(loser_pnls) / len(losers),
        "avg_loser_rs_score": sum(t.get("entry_rs_score", 0) for t in losers) / len(losers),
        "avg_loser_bb_width_pct": sum(t.get("entry_bb_width_pct", 0) for t in losers) / len(losers),
        "avg_loser_volume_mult": sum(t.get("entry_volume_mult", 0) for t in losers) / len(losers),
    }


def _calculate_profit_ratios(winners: List[Dict], losers: List[Dict], analysis: Dict[str, Any]) -> Dict[str, Any]:
    """è®¡ç®—ç›ˆäºæ¯”å’Œç›ˆåˆ©å› å­"""
    if not winners or not losers or analysis.get("avg_losing_trade", 0) == 0:
        return {}

    ratios = {
        "profit_loss_ratio": abs(analysis["avg_winning_trade"] / analysis["avg_losing_trade"])
    }

    total_wins = sum([t.get("pnl_pct", 0) for t in winners])
    total_losses = abs(sum([t.get("pnl_pct", 0) for t in losers]))
    if total_losses > 0:
        ratios["profit_factor"] = total_wins / total_losses

    return ratios


def _calculate_trade_analysis(trade_metrics: List[Dict]) -> Dict[str, Any]:
    """è®¡ç®—äº¤æ˜“åŸºç¡€åˆ†æ"""
    winners = [t for t in trade_metrics if t.get("result") == "WINNER"]
    losers = [t for t in trade_metrics if t.get("result") == "LOSER"]

    # åˆå¹¶æ‰€æœ‰ç»Ÿè®¡æ•°æ®
    analysis = {}
    analysis.update(_calculate_basic_stats(trade_metrics, winners, losers))
    analysis.update(_calculate_pnl_stats(trade_metrics))
    analysis.update(_calculate_winner_stats(winners))
    analysis.update(_calculate_loser_stats(losers))
    analysis.update(_calculate_profit_ratios(winners, losers, analysis))

    return analysis


def _build_detailed_analysis(trade_metrics: List[Dict], analysis: Dict[str, Any]) -> Dict[str, Any]:
    """æ„å»ºè¯¦ç»†äº¤æ˜“åˆ†æ"""
    winners = [t for t in trade_metrics if t.get("result") == "WINNER"]
    losers = [t for t in trade_metrics if t.get("result") == "LOSER"]

    if not (winners and losers):
        return {}

    return {
        "winners": {
            "count": len(winners),
            "avg_pnl_pct": analysis.get("avg_winning_trade", 0),
            "avg_rs_score": analysis.get("avg_winner_rs_score", 0),
            "avg_bb_width_pct": analysis.get("avg_winner_bb_width_pct", 0),
            "avg_volume_mult": analysis.get("avg_winner_volume_mult", 0),
        },
        "losers": {
            "count": len(losers),
            "avg_pnl_pct": analysis.get("avg_losing_trade", 0),
            "avg_rs_score": analysis.get("avg_loser_rs_score", 0),
            "avg_bb_width_pct": analysis.get("avg_loser_bb_width_pct", 0),
            "avg_volume_mult": analysis.get("avg_loser_volume_mult", 0),
        }
    }


def _extract_returns_metrics(result: BacktestResult) -> Dict[str, Any]:
    """ä»stats_returnsæå–å…³é”®æŒ‡æ ‡"""
    metrics = {}

    if not result.stats_returns:
        return metrics

    for key, val in result.stats_returns.items():
        key_str = str(key)
        # å¤„ç†NaNå€¼
        clean_val = val if val == val else None

        if "Sharpe Ratio" in key_str:
            metrics["sharpe_ratio"] = clean_val
        elif "Max Drawdown" in key_str:
            metrics["max_drawdown"] = clean_val
        elif "Total Return" in key_str:
            metrics["total_return"] = clean_val

    return metrics


def _extract_trade_analysis(result_dict: Dict[str, Any]) -> Dict[str, Any]:
    """ä»trade_metricsæå–äº¤æ˜“åˆ†ææ•°æ®"""
    analysis = result_dict["trade_metrics"]["analysis"]
    return {
        "win_rate": analysis.get("win_rate", 0),
        "profit_loss_ratio": analysis.get("profit_loss_ratio", 0),
        "avg_winning_trade": analysis.get("avg_winning_trade", 0),
        "avg_losing_trade": analysis.get("avg_losing_trade", 0),
        "total_pnl_pct": analysis.get("total_pnl_pct", 0),
    }


def _extract_filter_statistics(result_dict: Dict[str, Any]) -> Dict[str, Any]:
    """ä»filter_statsæå–è¿‡æ»¤å™¨ç»Ÿè®¡æ•°æ®"""
    filter_data = result_dict["filter_stats"]
    stats = {
        "total_signals": filter_data["total"].get("signal_checks", 0),
        "signals_passed": filter_data["total"].get("all_passed", 0),
    }

    if "rates" in filter_data:
        stats["signal_pass_rate"] = filter_data["rates"].get("pass_rate", 0)

    return stats


def _build_performance_summary(
    result: BacktestResult,
    result_dict: Dict[str, Any],
    trade_metrics: Optional[List[Dict]],
    filter_stats: Optional[Dict[str, Dict[str, int]]],
) -> Dict[str, Any]:
    """æ„å»ºæ€§èƒ½æ‘˜è¦"""
    performance_summary = {
        "total_trades": result.total_positions,
        "total_orders": result.total_orders,
        "elapsed_time_seconds": result.elapsed_time,
    }

    # æ·»åŠ æ”¶ç›Šç‡æŒ‡æ ‡
    performance_summary.update(_extract_returns_metrics(result))

    # æ·»åŠ äº¤æ˜“åˆ†ææŒ‡æ ‡
    if trade_metrics:
        performance_summary.update(_extract_trade_analysis(result_dict))

    # æ·»åŠ è¿‡æ»¤å™¨ç»Ÿè®¡
    if filter_stats:
        performance_summary.update(_extract_filter_statistics(result_dict))

    return performance_summary


def _build_result_dict(
    result: BacktestResult,
    strategy_name: str,
    strategy_config: Optional[Dict[str, Any]] = None,
    filter_stats: Optional[Dict[str, Dict[str, int]]] = None,
    trade_metrics: Optional[List[Dict]] = None,
    backtest_config: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    æ„å»ºå®Œæ•´çš„å›æµ‹ç»“æœå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¯ç”¨çš„å›æµ‹æ•°æ®ã€‚

    è®¾è®¡ç›®æ ‡ï¼š
    1. åŒ…å«å›æµ‹å¼•æ“èƒ½äº§ç”Ÿçš„æ‰€æœ‰æ•°æ®ï¼Œä¸é—æ¼ä»»ä½•ä¿¡æ¯
    2. ç»“æ„åŒ–ç»„ç»‡ï¼Œä¾¿äº AI å’Œäººå·¥åˆ†æ
    3. åŒæ—¶æ”¯æŒå¿«é€ŸæŸ¥çœ‹ï¼ˆperformanceï¼‰å’Œæ·±åº¦åˆ†æï¼ˆè¯¦ç»†æ•°æ®ï¼‰

    JSON ç»“æ„ï¼š
    - meta: å…ƒæ•°æ®ï¼ˆç­–ç•¥åã€æ—¶é—´æˆ³ã€æ‰§è¡Œæ—¶é•¿ï¼‰
    - summary: æ‘˜è¦ç»Ÿè®¡ï¼ˆè®¢å•æ•°ã€æŒä»“æ•°ï¼‰
    - pnl: ç›ˆäºæ•°æ®ï¼ˆæŒ‰è´§å¸åˆ†ç»„ï¼‰
    - returns: æ”¶ç›Šç‡æ•°æ®ï¼ˆå¤æ™®ç‡ã€æœ€å¤§å›æ’¤ã€æ€»å›æŠ¥ç­‰ï¼‰
    - strategy_config: å®Œæ•´çš„ç­–ç•¥é…ç½®å‚æ•°ï¼ˆä¾› AI åˆ†æä¼˜åŒ–ï¼‰
    - backtest_config: å›æµ‹é…ç½®ä¿¡æ¯ï¼ˆæ—¶é—´èŒƒå›´ã€æ ‡çš„æ•°é‡ç­‰ï¼‰
    - filter_stats: ä¿¡å·è¿‡æ»¤ç»Ÿè®¡
    - trade_metrics: äº¤æ˜“æ•°æ®
    - performance: æ€§èƒ½æ‘˜è¦ï¼ˆæ•´åˆå…³é”®æŒ‡æ ‡ï¼Œä¾¿äºå¿«é€ŸæŸ¥çœ‹ï¼‰

    Args:
        result: BacktestResult å¯¹è±¡
        strategy_name: ç­–ç•¥åç§°
        strategy_config: ç­–ç•¥é…ç½®å‚æ•°ï¼ˆå®Œæ•´ä¿å­˜ï¼Œä¾› AI ä¼˜åŒ–å‚è€ƒï¼‰
        filter_stats: è¿‡æ»¤å™¨ç»Ÿè®¡ï¼ˆæ¥è‡ªç­–ç•¥ç±»å˜é‡ï¼‰
        trade_metrics: äº¤æ˜“æŒ‡æ ‡ï¼ˆæ¥è‡ªç­–ç•¥ç±»å˜é‡ï¼‰
        backtest_config: å›æµ‹é…ç½®ä¿¡æ¯

    Returns:
        å®Œæ•´çš„ç»“æœå­—å…¸ï¼ŒåŒ…å«å›æµ‹å¼•æ“èƒ½è¾“å‡ºçš„æ‰€æœ‰ä¿¡æ¯
    """
    result_dict = {
        "meta": {
            "strategy_name": strategy_name,
            "timestamp": datetime.now().isoformat(),
            "elapsed_time_seconds": result.elapsed_time,
        },
        "summary": {
            "total_orders": result.total_orders,
            "total_positions": result.total_positions,
        },
        "pnl": _build_pnl_metrics(result),
        "returns": _build_returns_metrics(result),
        "strategy_config": strategy_config or {},
        "backtest_config": backtest_config or {},
        "filter_stats": _build_filter_stats(filter_stats),
        "trade_metrics": {
            "trades": trade_metrics or [],
            "analysis": {},
            "detailed_analysis": {},
        },
        "performance": {},
    }

    if trade_metrics:
        analysis = _calculate_trade_analysis(trade_metrics)
        result_dict["trade_metrics"]["analysis"] = analysis
        result_dict["trade_metrics"]["detailed_analysis"] = _build_detailed_analysis(trade_metrics, analysis)

    result_dict["performance"] = _build_performance_summary(result, result_dict, trade_metrics, filter_stats)

    return result_dict


def _save_results_to_json(
    result_dict: Dict[str, Any],
    strategy_name: str,
    base_dir: Path,
) -> Path:
    """
    å°†å›æµ‹ç»“æœä¿å­˜ä¸º JSON æ–‡ä»¶ã€‚

    æ–‡ä»¶å‘½åæ ¼å¼: {ç­–ç•¥åç§°}_{YYYY-MM-DD_HH-MM-SS}.json
    å­˜æ”¾è·¯å¾„: output/backtest/result/

    Args:
        result_dict: å®Œæ•´çš„ç»“æœå­—å…¸
        strategy_name: ç­–ç•¥åç§°
        base_dir: é¡¹ç›®æ ¹ç›®å½•

    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    # åˆ›å»ºç›®å½•
    result_dir = base_dir / "output" / "backtest" / "result"
    result_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"{strategy_name}_{timestamp}.json"
    # filename = f"{strategy_name}"
    filepath = result_dir / filename

    # ä¿å­˜ JSON
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(result_dict, f, indent=2, ensure_ascii=False, default=str)

    return filepath


def _print_results(
    result: BacktestResult,
    strategy_config: Optional[Dict[str, Any]] = None,
):
    """
    æ‰“å°å›æµ‹ç»“æœæ‘˜è¦åˆ°ç»ˆç«¯ï¼ˆç®€åŒ–ç‰ˆï¼‰

    ç›®æ ‡ï¼šä¿æŒç»ˆç«¯è¾“å‡ºç®€æ´ï¼Œåªæ˜¾ç¤ºæ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡
    å®Œæ•´æ•°æ®ï¼ˆåŒ…æ‹¬ç­–ç•¥é…ç½®ã€äº¤æ˜“æ˜ç»†ç­‰ï¼‰å·²ä¿å­˜åˆ° JSON æ–‡ä»¶ä¸­

    Args:
        result: å›æµ‹ç»“æœå¯¹è±¡
        strategy_config: ç­–ç•¥é…ç½®ï¼ˆå·²ç§»é™¤ï¼Œè®¾ä¸º None ä»¥ç®€åŒ–è¾“å‡ºï¼‰
    """
    logger.info("\n" + "=" * 65)
    logger.info("ğŸ† Backtest Report")
    logger.info("=" * 65)
    logger.info(f"Duration:       {result.elapsed_time:.4f}s")
    logger.info(f"Orders:         {result.total_orders}")
    logger.info(f"Positions:      {result.total_positions}")

    if result.stats_pnls:
        for currency, metrics in result.stats_pnls.items():
            logger.info(f"ğŸ’° PnL ({currency}):")
            for k, v in metrics.items():
                if v == v:  # Check for NaN
                    logger.info(f"   * {k:<25}: {v:,.4f}")

    if result.stats_returns:
        for currency, val in result.stats_returns.items():
            logger.info(f"- {currency:<25} {val:,.4f}")

    # æ³¨æ„ï¼šç­–ç•¥é…ç½®å‚æ•°å·²ä¿å­˜åˆ° JSON æ–‡ä»¶ä¸­ï¼Œç»ˆç«¯ä¸å†æ‰“å°
    # åŸå› ï¼š
    # 1. ä¿æŒç»ˆç«¯è¾“å‡ºç®€æ´ï¼Œåªæ˜¾ç¤ºå…³é”®æ€§èƒ½æŒ‡æ ‡
    # 2. ç­–ç•¥é…ç½®å¯èƒ½æœ‰å‡ åä¸ªå‚æ•°ï¼Œä¼šä½¿ç»ˆç«¯è¾“å‡ºè¿‡é•¿
    # 3. AI å¯ä»¥ä» JSON æ–‡ä»¶ä¸­è¯»å–å®Œæ•´é…ç½®è¿›è¡Œåˆ†æ
    # 4. äººå·¥æŸ¥çœ‹æ—¶ä¹Ÿæ›´å€¾å‘äºæŸ¥çœ‹ JSON æ–‡ä»¶è€Œéç»ˆç«¯æ»šåŠ¨è¾“å‡º

    logger.info("=" * 65 + "\n")


# ============================================================
# å·¥å…·å‡½æ•°æ¨¡å—
# ============================================================


def catalog_loader(
    catalog: ParquetDataCatalog,
    csv_path: Path,
    instrument: Instrument,
    bar_type: BarType,
) -> None:
    """
    å°† CSV æ•°æ®å¯¼å…¥ Catalog (Parquet)ã€‚

    æ³¨æ„ï¼šè°ƒç”¨å‰åº”å…ˆæ£€æŸ¥æ•°æ®æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤è§£æå·¨å¤§çš„ CSVã€‚
    æ£€æŸ¥é€»è¾‘å·²ç§»è‡³ run_high_level ä¸»å¾ªç¯ä¸­ï¼Œä»¥å‡å°‘å†—ä½™å‡½æ•°è°ƒç”¨ã€‚
    """
    # æ—¥å¿—è¾“å‡ºå·²ç§»è‡³è°ƒç”¨æ–¹ï¼Œæ­¤å¤„ä¸å†æ‰“å°

    # 1. è¯»å–å‰å‡ è¡Œæ£€æŸ¥åˆ—åï¼ˆå…¼å®¹ "datetime" å’Œ "timestamp" ä¸¤ç§åˆ—åï¼‰
    sample_df = pd.read_csv(csv_path, nrows=5)

    # ç¡®å®šæ—¶é—´åˆ—åï¼ˆä¼˜å…ˆä½¿ç”¨ timestampï¼Œå…¼å®¹ datetimeï¼‰
    if "timestamp" in sample_df.columns:
        pass
    elif "datetime" in sample_df.columns:
        pass
    else:
        raise ValueError(
            f"No valid time column found in {csv_path}. "
            f"Expected 'datetime' or 'timestamp', got: {list(sample_df.columns)}"
        )

    # 2. é«˜æ•ˆåŠ è½½ CSVï¼ˆä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®åŠ è½½å™¨ï¼‰
    from utils.data_management.data_loader import load_ohlcv_csv

    df: DataFrame = load_ohlcv_csv(csv_path=csv_path)

    # 3. è½¬æ¢å¹¶å†™å…¥
    # Wrangler å°† DataFrame è½¬æ¢ä¸º NT å†…éƒ¨çš„ Bar åºåˆ—
    wrangler = BarDataWrangler(bar_type, instrument)

    # å†™å…¥æ ‡çš„ä¿¡æ¯ (NT çš„å†™å…¥æ˜¯å¹‚ç­‰çš„ï¼Œå¤šæ¬¡å†™å…¥åŒä¸€ä¸ª Instrument ä¸ä¼šæŠ¥é”™)
    catalog.write_data([instrument])

    # å†™å…¥ Bar æ•°æ®
    catalog.write_data(wrangler.process(df))

    # 4. è½»é‡çº§æ ¡éªŒ
    intervals = catalog.get_intervals(data_cls=Bar, identifier=str(bar_type))
    if not intervals:
        raise ValueError(f"âŒ Failed to verify data in catalog for {instrument.id}")


# æ³¨æ„ï¼šè‡ªå®šä¹‰æ•°æ®ï¼ˆOI, Funding Rateï¼‰ç°åœ¨é€šè¿‡ OIFundingDataLoader ç›´æ¥åŠ è½½
# ä¸å†éœ€è¦åºåˆ—åŒ–åˆ° Parquet Catalog
