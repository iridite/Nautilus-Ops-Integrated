"""
æ•°æ®åŠ è½½å·¥å…·æ¨¡å—

æä¾›ç»Ÿä¸€çš„CSVæ•°æ®åŠ è½½ã€é¢„å¤„ç†å’ŒéªŒè¯åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§æ•°æ®æ ¼å¼å’Œè‡ªåŠ¨æ—¶é—´åˆ—æ£€æµ‹ã€‚

åŠŸèƒ½åŒ…æ‹¬ï¼š
- æ™ºèƒ½æ—¶é—´åˆ—æ£€æµ‹ï¼ˆdatetime/timestampï¼‰
- CSVæ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- æ—¶é—´èŒƒå›´è¿‡æ»¤å’Œæ•°æ®éªŒè¯
- è‡ªå®šä¹‰æ•°æ®ç±»å‹åŠ è½½ï¼ˆOI, Funding Rateï¼‰
- NautilusTrader æ•°æ®æ ¼å¼è½¬æ¢
- æ•°æ®æ¸…æ´—å’Œå»é‡
"""

import logging
from decimal import Decimal
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import pandas as pd
from nautilus_trader.core.nautilus_pyo3 import millis_to_nanos
from nautilus_trader.model.data import Bar
from nautilus_trader.model.identifiers import InstrumentId
from nautilus_trader.persistence.loaders import CSVBarDataLoader
from nautilus_trader.persistence.wranglers import BarDataWrangler

from utils.custom_data import FundingRateData, OpenInterestData
from utils.time_helpers import parse_date_to_timestamp

logger = logging.getLogger(__name__)

# å¯¼å…¥ç¼“å­˜æ¨¡å—
from .data_cache import get_cache

# å¯¼å…¥ç»Ÿä¸€å¼‚å¸¸
from core.exceptions import DataLoadError, TimeColumnError


def detect_time_column(csv_path: Union[str, Path], sample_rows: int = 5) -> str:
    """
    æ™ºèƒ½æ£€æµ‹CSVæ–‡ä»¶çš„æ—¶é—´åˆ—åç§°

    Parameters
    ----------
    csv_path : Union[str, Path]
        CSVæ–‡ä»¶è·¯å¾„
    sample_rows : int, optional
        ç”¨äºæ£€æµ‹çš„æ ·æœ¬è¡Œæ•°ï¼Œé»˜è®¤ 5

    Returns
    -------
    str
        æ£€æµ‹åˆ°çš„æ—¶é—´åˆ—åç§°

    Raises
    ------
    TimeColumnError
        å½“æ— æ³•æ£€æµ‹åˆ°æœ‰æ•ˆçš„æ—¶é—´åˆ—æ—¶

    Examples
    --------
    >>> time_col = detect_time_column("data.csv")
    >>> print(time_col)  # "datetime" æˆ– "timestamp"
    """
    try:
        sample_df = pd.read_csv(csv_path, nrows=sample_rows)

        # ä¼˜å…ˆçº§ï¼štimestamp > datetime > å…¶ä»–å¯èƒ½çš„æ—¶é—´åˆ—å
        time_candidates = ["timestamp", "datetime", "time", "date", "ts"]

        for candidate in time_candidates:
            if candidate in sample_df.columns:
                # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ—¶é—´æ ¼å¼
                if _validate_time_column(sample_df[candidate]):
                    return candidate

        # å¦‚æœæ²¡æœ‰æ ‡å‡†æ—¶é—´åˆ—åï¼Œå°è¯•æ£€æµ‹å¯èƒ½çš„æ—¶é—´åˆ—
        for col in sample_df.columns:
            if _looks_like_time_column(sample_df[col]):
                return col

        # æŠ›å‡ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
        available_cols = list(sample_df.columns)
        raise TimeColumnError(
            f"No valid time column found in {csv_path}. "
            f"Expected one of {time_candidates}, got: {available_cols}"
        )

    except pd.errors.EmptyDataError:
        raise TimeColumnError(f"CSV file is empty: {csv_path}")
    except Exception as e:
        raise TimeColumnError(f"Error detecting time column in {csv_path}: {e}")


def _validate_time_column(series: pd.Series) -> bool:
    """éªŒè¯åºåˆ—æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ—¶é—´åˆ—"""
    if len(series) == 0:
        return False

    try:
        # å°è¯•è§£æç¬¬ä¸€ä¸ªéç©ºå€¼
        sample_value = series.dropna().iloc[0] if not series.dropna().empty else None
        if sample_value is None:
            return False

        # æ£€æŸ¥æ˜¯å¦ä¸ºæ—¶é—´æˆ³ï¼ˆæ•°å€¼ï¼‰æˆ–æ—¥æœŸå­—ç¬¦ä¸²
        if isinstance(sample_value, (int, float)):
            # å¯èƒ½æ˜¯æ—¶é—´æˆ³
            return True
        elif isinstance(sample_value, str):
            # å°è¯•è§£æä¸ºæ—¥æœŸ
            pd.to_datetime(sample_value)
            return True
        else:
            return False
    except (ValueError, TypeError) as e:
        logger.debug(f"Failed to validate time column: {e}")
        return False


def _looks_like_time_column(series: pd.Series) -> bool:
    """æ£€æŸ¥åºåˆ—æ˜¯å¦çœ‹èµ·æ¥åƒæ—¶é—´åˆ—"""
    if len(series) == 0:
        return False

    # æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«æ—¶é—´ç›¸å…³å…³é”®è¯
    col_name = str(series.name).lower() if series.name else ""
    time_keywords = ["time", "date", "ts", "stamp"]

    return any(keyword in col_name for keyword in time_keywords)


def _check_data_range_mismatch(
    df: pd.DataFrame, start_date: str, end_date: str
) -> tuple[bool, pd.Timestamp, pd.Timestamp, pd.Timestamp, pd.Timestamp]:
    """æ£€æŸ¥æ•°æ®èŒƒå›´æ˜¯å¦åŒ¹é…"""
    actual_start = df.index.min()
    actual_end = df.index.max()
    config_start = pd.Timestamp(start_date)
    config_end = pd.Timestamp(end_date)

    has_mismatch = actual_end > config_end
    return has_mismatch, actual_start, actual_end, config_start, config_end


def _log_range_mismatch(csv_path: Path, actual_start, actual_end, config_start, config_end, logger):
    """è®°å½•èŒƒå›´ä¸åŒ¹é…è­¦å‘Š"""
    logger.warning(
        f"âš ï¸ Data range mismatch detected in {csv_path.name}:\n"
        f"   CSV data: {actual_start.date()} to {actual_end.date()}\n"
        f"   Config range: {config_start.date()} to {config_end.date()}\n"
        f"   Data exceeds config by {(actual_end - config_end).days} days"
    )


def _cleanup_csv_file(csv_path: Path, logger):
    """æ¸…ç†CSVæ–‡ä»¶"""
    logger.warning(f"ğŸ§¹ Auto-cleanup enabled: removing {csv_path.name}")
    csv_path.unlink()


def _cleanup_parquet_files(csv_path: Path, logger):
    """æ¸…ç†ç›¸å…³çš„Parquetæ–‡ä»¶"""
    import shutil

    symbol = csv_path.stem.split("-")[1] if "-" in csv_path.stem else None
    if not symbol:
        return

    parquet_base = csv_path.parent.parent / "parquet"
    if not parquet_base.exists():
        return

    for strategy_dir in parquet_base.iterdir():
        if strategy_dir.is_dir():
            for symbol_dir in strategy_dir.rglob(f"*{symbol}*"):
                if symbol_dir.is_dir():
                    logger.warning(f"   Removing Parquet: {symbol_dir}")
                    shutil.rmtree(symbol_dir)


def _check_and_cleanup_data_range(
    df: pd.DataFrame, csv_path: Path, start_date: str, end_date: str, auto_cleanup: bool, logger
) -> None:
    """æ£€æŸ¥æ•°æ®èŒƒå›´å¹¶æ‰§è¡Œæ¸…ç†"""
    has_mismatch, actual_start, actual_end, config_start, config_end = _check_data_range_mismatch(
        df, start_date, end_date
    )

    if not has_mismatch:
        return

    _log_range_mismatch(csv_path, actual_start, actual_end, config_start, config_end, logger)

    if not auto_cleanup:
        return

    _cleanup_csv_file(csv_path, logger)
    _cleanup_parquet_files(csv_path, logger)

    raise DataLoadError(
        "Data file removed due to range mismatch. Please re-run to download correct data range."
    )


def _filter_by_date_range(
    df: pd.DataFrame, start_date: str | None, end_date: str | None
) -> pd.DataFrame:
    """æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤æ•°æ®"""
    if start_date:
        start_ts = parse_date_to_timestamp(start_date)
        df = df[df.index >= start_ts]

    if end_date:
        end_ts = parse_date_to_timestamp(end_date)
        df = df[df.index <= end_ts]

    return df


def _try_get_cached_csv_data(
    csv_path: Path, start_date: str, end_date: str, use_cache: bool
) -> Optional[pd.DataFrame]:
    """å°è¯•ä»ç¼“å­˜è·å–CSVæ•°æ®"""
    if use_cache and start_date and end_date:
        cache = get_cache()
        cached_df = cache.get(csv_path, start_date, end_date)
        if cached_df is not None:
            return cached_df
    return None


def _load_csv_with_time_column(
    csv_path: Path, time_column: Optional[str]
) -> tuple[pd.DataFrame, str]:
    """åŠ è½½CSVæ•°æ®å¹¶æ£€æµ‹æ—¶é—´åˆ—"""
    if time_column is None:
        time_column = detect_time_column(csv_path)

    required_columns = [time_column, "open", "high", "low", "close", "volume"]

    df_full = CSVBarDataLoader.load(
        file_path=csv_path,
        index_col=time_column,
        usecols=required_columns,
        parse_dates=True,
    )

    df_full.sort_index(inplace=True)
    return df_full, time_column


def _process_and_validate_csv_data(
    df_full: pd.DataFrame,
    csv_path: Path,
    start_date: Optional[str],
    end_date: Optional[str],
    validate_data: bool,
    auto_cleanup: bool,
    logger,
) -> pd.DataFrame:
    """å¤„ç†å’ŒéªŒè¯CSVæ•°æ®"""
    if start_date and end_date and len(df_full) > 0:
        _check_and_cleanup_data_range(df_full, csv_path, start_date, end_date, auto_cleanup, logger)

    df = _filter_by_date_range(df_full, start_date, end_date)

    if validate_data:
        _validate_ohlcv_data(df)

    if len(df) == 0:
        raise DataLoadError(f"No data available in specified range for {csv_path}")

    return df


def _cache_csv_data(
    csv_path: Path,
    start_date: Optional[str],
    end_date: Optional[str],
    df: pd.DataFrame,
    use_cache: bool,
):
    """ç¼“å­˜CSVæ•°æ®"""
    if use_cache and start_date and end_date:
        cache = get_cache()
        cache.put(csv_path, start_date, end_date, df)


def load_ohlcv_csv(
    csv_path: Union[str, Path],
    time_column: str | None = None,
    start_date: str | None = None,
    end_date: str | None = None,
    validate_data: bool = True,
    auto_cleanup: bool = True,
    use_cache: bool = True,
) -> pd.DataFrame:
    """
    åŠ è½½OHLCV CSVæ•°æ®ï¼Œæ”¯æŒè‡ªåŠ¨æ—¶é—´åˆ—æ£€æµ‹å’Œæ—¶é—´èŒƒå›´è¿‡æ»¤

    Parameters
    ----------
    csv_path : Union[str, Path]
        CSVæ–‡ä»¶è·¯å¾„
    time_column : str | None, optional
        æŒ‡å®šæ—¶é—´åˆ—åç§°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
    start_date : str | None, optional
        å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    end_date : str | None, optional
        ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    validate_data : bool, optional
        æ˜¯å¦éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼Œé»˜è®¤ True
    auto_cleanup : bool, optional
        å½“æ£€æµ‹åˆ°æ•°æ®èŒƒå›´å¼‚å¸¸æ—¶æ˜¯å¦è‡ªåŠ¨æ¸…ç†ï¼Œé»˜è®¤ True
    use_cache : bool, optional
        æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ True

    Returns
    -------
    pd.DataFrame
        åŠ è½½å¹¶å¤„ç†åçš„OHLCVæ•°æ®ï¼Œä»¥æ—¶é—´ä¸ºç´¢å¼•

    Raises
    ------
    DataLoadError
        å½“æ•°æ®åŠ è½½æˆ–å¤„ç†å¤±è´¥æ—¶
    """
    import logging

    logger = logging.getLogger(__name__)
    csv_path = Path(csv_path)

    if not csv_path.exists():
        raise DataLoadError(f"CSV file not found: {csv_path}")

    # å°è¯•ä»ç¼“å­˜è·å–
    cached_df = _try_get_cached_csv_data(csv_path, start_date, end_date, use_cache)
    if cached_df is not None:
        return cached_df

    try:
        # åŠ è½½CSVæ•°æ®
        df_full, time_column = _load_csv_with_time_column(csv_path, time_column)

        # å¤„ç†å’ŒéªŒè¯æ•°æ®
        df = _process_and_validate_csv_data(
            df_full, csv_path, start_date, end_date, validate_data, auto_cleanup, logger
        )

        # æ”¾å…¥ç¼“å­˜
        _cache_csv_data(csv_path, start_date, end_date, df, use_cache)

        return df

    except Exception as e:
        if isinstance(e, DataLoadError):
            raise
        raise DataLoadError(f"Error loading OHLCV data from {csv_path}: {e}")


def _validate_ohlcv_data(df: pd.DataFrame) -> None:
    """éªŒè¯OHLCVæ•°æ®å®Œæ•´æ€§"""
    required_cols = ["open", "high", "low", "close", "volume"]

    # æ£€æŸ¥å¿…éœ€åˆ—
    missing_cols = set(required_cols) - set(df.columns)
    if missing_cols:
        raise DataLoadError(f"Missing required columns: {missing_cols}")

    # æ£€æŸ¥æ•°å€¼ç±»å‹
    for col in required_cols:
        if not pd.api.types.is_numeric_dtype(df[col]):
            raise DataLoadError(f"Column '{col}' must be numeric")

    # æ£€æŸ¥ä»·æ ¼é€»è¾‘
    invalid_rows = (
        (df["high"] < df["low"])
        | (df["high"] < df["open"])
        | (df["high"] < df["close"])
        | (df["low"] > df["open"])
        | (df["low"] > df["close"])
    )

    if invalid_rows.any():
        first_invalid = df[invalid_rows].index[0]
        raise DataLoadError(f"Invalid OHLC data detected at {first_invalid}")


def create_nautilus_bars(df: pd.DataFrame, bar_type: Any, instrument: Any) -> List[Bar]:
    """
    å°†pandas DataFrameè½¬æ¢ä¸ºNautilusTrader Barå¯¹è±¡åˆ—è¡¨

    Parameters
    ----------
    df : pd.DataFrame
        OHLCVæ•°æ®æ¡†ï¼ˆä»¥æ—¶é—´ä¸ºç´¢å¼•ï¼‰
    bar_type : BarType
        NautilusTrader BarTypeå¯¹è±¡
    instrument : Instrument
        NautilusTrader Instrumentå¯¹è±¡

    Returns
    -------
    List[Bar]
        NautilusTrader Barå¯¹è±¡åˆ—è¡¨
    """
    try:
        wrangler = BarDataWrangler(bar_type, instrument)
        bars = wrangler.process(df)
        return bars
    except Exception as e:
        raise DataLoadError(f"Error creating Nautilus bars: {e}")


def _validate_custom_csv_columns(
    df: pd.DataFrame, time_column: str, data_type: str, csv_path: Path
):
    """éªŒè¯CSVåˆ—æ˜¯å¦å­˜åœ¨"""
    if time_column not in df.columns:
        raise DataLoadError(f"Time column '{time_column}' not found in {csv_path}")

    if data_type.lower() == "oi":
        if "open_interest" not in df.columns:
            raise DataLoadError(f"'open_interest' column not found in {csv_path}")
    elif data_type.lower() == "funding":
        if "funding_rate" not in df.columns:
            raise DataLoadError(f"'funding_rate' column not found in {csv_path}")
    else:
        raise DataLoadError(f"Unsupported data type: {data_type}")


def _create_oi_data(row, time_column: str, instrument_id: InstrumentId) -> OpenInterestData:
    """åˆ›å»ºOIæ•°æ®å¯¹è±¡"""
    ts_ms = int(row[time_column])
    ts_event = millis_to_nanos(ts_ms)
    ts_init = ts_event

    return OpenInterestData(
        instrument_id=instrument_id,
        open_interest=Decimal(str(row["open_interest"])),
        ts_event=ts_event,
        ts_init=ts_init,
    )


def _create_funding_data(
    row, time_column: str, instrument_id: InstrumentId, df: pd.DataFrame
) -> FundingRateData:
    """åˆ›å»ºFunding Rateæ•°æ®å¯¹è±¡"""
    ts_ms = int(row[time_column])
    ts_event = millis_to_nanos(ts_ms)
    ts_init = ts_event

    # å¯é€‰çš„ä¸‹æ¬¡èµ„é‡‘è´¹ç‡æ—¶é—´
    next_funding_time = None
    if "next_funding_time" in df.columns and pd.notna(row["next_funding_time"]):
        next_funding_time = millis_to_nanos(int(row["next_funding_time"]))

    return FundingRateData(
        instrument_id=instrument_id,
        funding_rate=Decimal(str(row["funding_rate"])),
        next_funding_time=next_funding_time,
        ts_event=ts_event,
        ts_init=ts_init,
    )


def _parse_custom_data(
    df: pd.DataFrame, data_type: str, time_column: str, instrument_id: InstrumentId
) -> List:
    """è§£æè‡ªå®šä¹‰æ•°æ®"""
    data_list = []

    if data_type.lower() == "oi":
        for _, row in df.iterrows():
            data_list.append(_create_oi_data(row, time_column, instrument_id))
    elif data_type.lower() == "funding":
        for _, row in df.iterrows():
            data_list.append(_create_funding_data(row, time_column, instrument_id, df))

    return data_list


def load_custom_csv_data(
    csv_path: Union[str, Path],
    data_type: str,
    instrument_id: InstrumentId,
    time_column: str | None = None,
) -> List[Union[OpenInterestData, FundingRateData]]:
    """
    åŠ è½½è‡ªå®šä¹‰CSVæ•°æ®ï¼ˆOIæˆ–Funding Rateï¼‰

    Parameters
    ----------
    csv_path : Union[str, Path]
        CSVæ–‡ä»¶è·¯å¾„
    data_type : str
        æ•°æ®ç±»å‹ï¼ˆ"oi" æˆ– "funding"ï¼‰
    instrument_id : InstrumentId
        NautilusTrader åˆçº¦æ ‡è¯†ç¬¦
    time_column : str | None, optional
        æ—¶é—´åˆ—åç§°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹

    Returns
    -------
    List[Union[OpenInterestData, FundingRateData]]
        è‡ªå®šä¹‰æ•°æ®å¯¹è±¡åˆ—è¡¨
    """
    csv_path = Path(csv_path)

    if not csv_path.exists():
        raise DataLoadError(f"Custom data file not found: {csv_path}")

    try:
        # è‡ªåŠ¨æ£€æµ‹æ—¶é—´åˆ—
        if time_column is None:
            time_column = detect_time_column(csv_path)

        df = pd.read_csv(csv_path)

        # éªŒè¯åˆ—
        _validate_custom_csv_columns(df, time_column, data_type, csv_path)

        # è§£ææ•°æ®
        return _parse_custom_data(df, data_type, time_column, instrument_id)

    except Exception as e:
        if isinstance(e, DataLoadError):
            raise
        raise DataLoadError(f"Error loading custom data from {csv_path}: {e}")


def batch_load_custom_data(
    data_dir: Path,
    symbol: str,
    instrument_id: InstrumentId,
    data_types: List[str] = ["oi", "funding"],
) -> Dict[str, List]:
    """
    æ‰¹é‡åŠ è½½æŒ‡å®šç¬¦å·çš„æ‰€æœ‰è‡ªå®šä¹‰æ•°æ®

    Parameters
    ----------
    data_dir : Path
        æ•°æ®ç›®å½•è·¯å¾„
    symbol : str
        äº¤æ˜“å¯¹ç¬¦å·ï¼ˆå¦‚ "BTCUSDT"ï¼‰
    instrument_id : InstrumentId
        NautilusTrader åˆçº¦æ ‡è¯†ç¬¦
    data_types : List[str], optional
        è¦åŠ è½½çš„æ•°æ®ç±»å‹åˆ—è¡¨ï¼Œé»˜è®¤ ["oi", "funding"]

    Returns
    -------
    Dict[str, List]
        æŒ‰æ•°æ®ç±»å‹åˆ†ç»„çš„æ•°æ®å­—å…¸
    """
    symbol_dir = data_dir / symbol
    result = {}

    for data_type in data_types:
        result[data_type] = []

        if not symbol_dir.exists():
            continue

        # æŸ¥æ‰¾åŒ¹é…çš„æ•°æ®æ–‡ä»¶
        if data_type == "oi":
            pattern = "*-oi-*.csv"
        elif data_type == "funding":
            pattern = "*-funding-*.csv"
        else:
            continue

        data_files = list(symbol_dir.glob(pattern))

        for file_path in data_files:
            try:
                custom_data = load_custom_csv_data(file_path, data_type, instrument_id)
                result[data_type].extend(custom_data)
            except DataLoadError as e:
                logger.error(f"âš ï¸ Failed to load {file_path}: {e}")
                continue

    return result


def clean_and_deduplicate_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    æ¸…æ´—å’Œå»é‡æ•°æ®

    Parameters
    ----------
    df : pd.DataFrame
        åŸå§‹æ•°æ®æ¡†

    Returns
    -------
    pd.DataFrame
        æ¸…æ´—åçš„æ•°æ®æ¡†
    """
    # å»é™¤é‡å¤è¡Œ
    df_clean = df.drop_duplicates()

    # å»é™¤åŒ…å«NaNçš„è¡Œ
    df_clean = df_clean.dropna()

    # å¦‚æœæœ‰æ—¶é—´ç´¢å¼•ï¼Œå»é™¤é‡å¤çš„æ—¶é—´æˆ³
    if hasattr(df_clean.index, "duplicated"):
        df_clean = df_clean[~df_clean.index.duplicated(keep="first")]

    return df_clean


def get_data_summary(df: pd.DataFrame) -> Dict[str, Any]:
    """
    è·å–æ•°æ®æ‘˜è¦ä¿¡æ¯

    Parameters
    ----------
    df : pd.DataFrame
        æ•°æ®æ¡†

    Returns
    -------
    Dict[str, Any]
        æ•°æ®æ‘˜è¦ä¿¡æ¯
    """
    # æ€§èƒ½ä¼˜åŒ–ï¼šç§»é™¤ len(df) è°ƒç”¨ï¼Œé¿å…è§¦å‘å¤§å‹ CSV å®Œæ•´åŠ è½½
    summary = {
        "columns": list(df.columns),
        "start_time": df.index.min() if len(df) > 0 else None,
        "end_time": df.index.max() if len(df) > 0 else None,
        "missing_values": df.isnull().sum().to_dict(),
        "data_types": df.dtypes.to_dict(),
    }

    # å¦‚æœåŒ…å«OHLCVæ•°æ®ï¼Œæ·»åŠ ä»·æ ¼ç»Ÿè®¡
    if all(col in df.columns for col in ["open", "high", "low", "close"]):
        summary["price_stats"] = {
            "min_price": float(df[["open", "high", "low", "close"]].min().min()),
            "max_price": float(df[["open", "high", "low", "close"]].max().max()),
            "avg_close": float(df["close"].mean()),
        }

    # å¦‚æœåŒ…å«æˆäº¤é‡æ•°æ®
    if "volume" in df.columns:
        summary["volume_stats"] = {
            "total_volume": float(df["volume"].sum()),
            "avg_volume": float(df["volume"].mean()),
            "max_volume": float(df["volume"].max()),
        }

    return summary


class DataLoader:
    """
    ç»Ÿä¸€æ•°æ®åŠ è½½å™¨ç±»

    æä¾›ä¾¿æ·çš„æ•°æ®åŠ è½½å’Œç®¡ç†åŠŸèƒ½
    """

    def __init__(self, base_dir: Path):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨

        Parameters
        ----------
        base_dir : Path
            æ•°æ®åŸºç¡€ç›®å½•
        """
        self.base_dir = base_dir
        self.raw_data_dir = base_dir / "data" / "raw"

    def load_ohlcv(
        self, symbol: str, timeframe: str, start_date: str, end_date: str, exchange: str = "binance"
    ) -> pd.DataFrame:
        """
        åŠ è½½æŒ‡å®šç¬¦å·çš„OHLCVæ•°æ®

        Parameters
        ----------
        symbol : str
            äº¤æ˜“å¯¹ç¬¦å·
        timeframe : str
            æ—¶é—´å‘¨æœŸ
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        exchange : str, optional
            äº¤æ˜“æ‰€åç§°ï¼Œé»˜è®¤ "binance"

        Returns
        -------
        pd.DataFrame
            OHLCVæ•°æ®æ¡†
        """
        safe_symbol = symbol.replace("/", "")
        filename = f"{exchange.lower()}-{safe_symbol}-{timeframe}-{start_date}_{end_date}.csv"
        file_path = self.raw_data_dir / safe_symbol / filename

        return load_ohlcv_csv(file_path, start_date=start_date, end_date=end_date)

    def load_custom_data(
        self, symbol: str, instrument_id: InstrumentId, data_types: List[str] = ["oi", "funding"]
    ) -> Dict[str, List]:
        """
        åŠ è½½æŒ‡å®šç¬¦å·çš„è‡ªå®šä¹‰æ•°æ®

        Parameters
        ----------
        symbol : str
            äº¤æ˜“å¯¹ç¬¦å·
        instrument_id : InstrumentId
            åˆçº¦æ ‡è¯†ç¬¦
        data_types : List[str], optional
            æ•°æ®ç±»å‹åˆ—è¡¨ï¼Œé»˜è®¤ ["oi", "funding"]

        Returns
        -------
        Dict[str, List]
            è‡ªå®šä¹‰æ•°æ®å­—å…¸
        """
        return batch_load_custom_data(self.raw_data_dir, symbol, instrument_id, data_types)

    def validate_data_availability(
        self, symbol: str, timeframe: str, start_date: str, end_date: str, exchange: str = "binance"
    ) -> Tuple[bool, str | None]:
        """
        éªŒè¯æ•°æ®å¯ç”¨æ€§

        Parameters
        ----------
        symbol : str
            äº¤æ˜“å¯¹ç¬¦å·
        timeframe : str
            æ—¶é—´å‘¨æœŸ
        start_date : str
            å¼€å§‹æ—¥æœŸ
        end_date : str
            ç»“æŸæ—¥æœŸ
        exchange : str, optional
            äº¤æ˜“æ‰€åç§°ï¼Œé»˜è®¤ "binance"

        Returns
        -------
        Tuple[bool, str | None]
            (æ•°æ®æ˜¯å¦å¯ç”¨, é”™è¯¯ä¿¡æ¯)
        """
        from utils.data_file_checker import check_single_data_file

        exists, _ = check_single_data_file(
            symbol=symbol,
            start_date=start_date,
            end_date=end_date,
            timeframe=timeframe,
            exchange=exchange,
            base_dir=self.base_dir,
        )
        return (exists, None if exists else "Data file not found")


def load_csv_with_time_detection(
    csv_path: Union[str, Path], time_column: str | None = None
) -> pd.DataFrame:
    """
    ç®€åŒ–çš„CSVæ•°æ®åŠ è½½å‡½æ•°ï¼Œæ”¯æŒè‡ªåŠ¨æ—¶é—´åˆ—æ£€æµ‹

    è¿™æ˜¯ load_ohlcv_csv çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸»è¦ç”¨äºæµ‹è¯•å’Œå¿«é€Ÿæ•°æ®åŠ è½½ã€‚

    Parameters
    ----------
    csv_path : Union[str, Path]
        CSVæ–‡ä»¶è·¯å¾„
    time_column : str | None, optional
        æŒ‡å®šæ—¶é—´åˆ—åç§°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹

    Returns
    -------
    pd.DataFrame
        åŠ è½½çš„æ•°æ®æ¡†ï¼Œå¦‚æœæ£€æµ‹åˆ°æ—¶é—´åˆ—ä¼šè¿›è¡Œç›¸åº”å¤„ç†

    Raises
    ------
    DataLoadError
        å½“æ•°æ®åŠ è½½å¤±è´¥æ—¶
    """
    csv_path = Path(csv_path)

    if not csv_path.exists():
        logger.warning(f"Warning: CSV file not found: {csv_path}")
        return pd.DataFrame()

    try:
        # å…ˆå°è¯•åŸºæœ¬åŠ è½½
        df = pd.read_csv(csv_path)

        if df.empty:
            return df

        # è‡ªåŠ¨æ£€æµ‹æ—¶é—´åˆ—ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if time_column is None:
            try:
                time_column = detect_time_column(csv_path)
            except TimeColumnError:
                # æ²¡æœ‰æ—¶é—´åˆ—ï¼Œç›´æ¥è¿”å›åŸå§‹æ•°æ®
                return df

        # å¦‚æœæ‰¾åˆ°æ—¶é—´åˆ—ï¼Œè¿›è¡Œç›¸åº”å¤„ç†
        if time_column and time_column in df.columns:
            # å°è¯•è½¬æ¢datetimeåˆ—
            if time_column.lower() == "datetime":
                try:
                    # ä½¿ç”¨ ISO8601 æ ¼å¼æ”¯æŒæ··åˆæ—¥æœŸæ—¶é—´æ ¼å¼
                    df[time_column] = pd.to_datetime(df[time_column], format="ISO8601")
                except Exception as e:
                    logger.error(f"Warning: Failed to convert datetime column: {e}")
            # timestampåˆ—ä¿æŒåŸæ ·ï¼ˆæ•°å€¼ç±»å‹ï¼‰

        return df

    except Exception as e:
        logger.error(f"Warning: Error loading CSV from {csv_path}: {e}")
        return pd.DataFrame()


def _try_get_cached_data(
    cache, parquet_path: Path, start_date: str, end_date: str, logger
) -> Optional[pd.DataFrame]:
    """å°è¯•ä»ç¼“å­˜è·å–æ•°æ®"""
    cached_df = cache.get(parquet_path, start_date, end_date)
    if cached_df is not None:
        logger.debug(f"ä»ç¼“å­˜åŠ è½½ Parquet: {parquet_path.name}")
    return cached_df


def _get_or_create_metadata(cache, parquet_path: Path, use_cache: bool, logger) -> dict:
    """è·å–æˆ–åˆ›å»ºParquetå…ƒæ•°æ®"""
    metadata = None
    if use_cache:
        metadata = cache.get_parquet_metadata(parquet_path)

    if metadata is None:
        import pyarrow.parquet as pq

        parquet_file = pq.ParquetFile(parquet_path)
        metadata = {
            "num_rows": parquet_file.metadata.num_rows,
            "num_row_groups": parquet_file.metadata.num_row_groups,
            "columns": [col for col in parquet_file.schema.names],
            "schema": str(parquet_file.schema),
        }
        if use_cache:
            cache.put_parquet_metadata(parquet_path, metadata)
        logger.debug(
            f"Parquet å…ƒæ•°æ®: {metadata['num_rows']} è¡Œ, {metadata['num_row_groups']} è¡Œç»„"
        )

    return metadata


def _detect_and_set_time_index(df: pd.DataFrame) -> pd.DataFrame:
    """æ£€æµ‹å¹¶è®¾ç½®æ—¶é—´ç´¢å¼•"""
    time_col = None
    for col in ["timestamp", "datetime", "time"]:
        if col in df.columns:
            time_col = col
            break

    if time_col:
        if time_col == "timestamp":
            df.index = pd.to_datetime(df[time_col], unit="ms")
        else:
            df.index = pd.to_datetime(df[time_col])
        df.index.name = "timestamp"
        df = df.drop(columns=[time_col])

    return df


def _cache_result(
    cache, parquet_path: Path, start_date: str, end_date: str, df: pd.DataFrame
) -> None:
    """ç¼“å­˜ç»“æœæ•°æ®"""
    cache.put(parquet_path, start_date, end_date, df)


def _try_get_cached_parquet(
    cache, parquet_path: Path, start_date: str, end_date: str, logger
) -> pd.DataFrame | None:
    """å°è¯•ä»ç¼“å­˜è·å–Parquetæ•°æ®"""
    cached_df = _try_get_cached_data(cache, parquet_path, start_date, end_date, logger)
    if cached_df is not None:
        return cached_df
    return None


def _read_and_validate_parquet(parquet_path: Path) -> pd.DataFrame:
    """è¯»å–å¹¶éªŒè¯Parquetæ–‡ä»¶"""
    df = pd.read_parquet(parquet_path)

    if df.empty:
        raise DataLoadError(f"Parquet file is empty: {parquet_path}")

    return df


def _process_parquet_data(
    df: pd.DataFrame, parquet_path: Path, start_date: str | None, end_date: str | None
) -> pd.DataFrame:
    """å¤„ç†Parquetæ•°æ®ï¼ˆè®¾ç½®ç´¢å¼•ã€è¿‡æ»¤èŒƒå›´ï¼‰"""
    # æ£€æµ‹å¹¶è®¾ç½®æ—¶é—´ç´¢å¼•
    df = _detect_and_set_time_index(df)

    # æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤
    df = _filter_by_date_range(df, start_date, end_date)

    if len(df) == 0:
        raise DataLoadError(
            f"No data in date range [{start_date} to {end_date}] for {parquet_path.name}"
        )

    return df


def load_ohlcv_parquet(
    parquet_path: Union[str, Path],
    start_date: str | None = None,
    end_date: str | None = None,
    use_cache: bool = True,
) -> pd.DataFrame:
    """
    åŠ è½½ OHLCV Parquet æ•°æ®ï¼Œæ”¯æŒå…ƒæ•°æ®ç¼“å­˜å’Œæ—¶é—´èŒƒå›´è¿‡æ»¤

    ç›¸æ¯” CSVï¼ŒParquet æä¾›ï¼š
    - æ›´å¿«çš„è¯»å–é€Ÿåº¦ï¼ˆåˆ—å¼å­˜å‚¨ï¼‰
    - æ›´å°çš„æ–‡ä»¶å¤§å°ï¼ˆå‹ç¼©ï¼‰
    - å†…ç½®æ•°æ®ç±»å‹ä¿¡æ¯

    Parameters
    ----------
    parquet_path : Union[str, Path]
        Parquet æ–‡ä»¶è·¯å¾„
    start_date : str | None, optional
        å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    end_date : str | None, optional
        ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    use_cache : bool, optional
        æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ True

    Returns
    -------
    pd.DataFrame
        åŠ è½½å¹¶å¤„ç†åçš„ OHLCV æ•°æ®ï¼Œä»¥æ—¶é—´ä¸ºç´¢å¼•

    Raises
    ------
    DataLoadError
        å½“æ•°æ®åŠ è½½æˆ–å¤„ç†å¤±è´¥æ—¶
    """
    import logging

    logger = logging.getLogger(__name__)
    parquet_path = Path(parquet_path)

    if not parquet_path.exists():
        raise DataLoadError(f"Parquet file not found: {parquet_path}")

    # å°è¯•ä»ç¼“å­˜è·å–
    cache = get_cache()
    if use_cache:
        cached_df = _try_get_cached_parquet(
            cache, parquet_path, start_date or "", end_date or "", logger
        )
        if cached_df is not None:
            return cached_df

    try:
        # è·å–æˆ–åˆ›å»ºå…ƒæ•°æ®
        _get_or_create_metadata(cache, parquet_path, use_cache, logger)

        # è¯»å–å¹¶éªŒè¯
        df = _read_and_validate_parquet(parquet_path)

        # å¤„ç†æ•°æ®
        df = _process_parquet_data(df, parquet_path, start_date, end_date)

        # ç¼“å­˜ç»“æœ
        if use_cache:
            _cache_result(cache, parquet_path, start_date or "", end_date or "", df)

        logger.info(f"âœ… åŠ è½½ Parquet: {parquet_path.name} ({len(df)} è¡Œ)")
        return df

    except Exception as e:
        raise DataLoadError(f"Error loading Parquet file {parquet_path}: {e}")


def load_ohlcv_auto(
    file_path: Union[str, Path],
    start_date: str | None = None,
    end_date: str | None = None,
    use_cache: bool = True,
    **kwargs,
) -> pd.DataFrame:
    """
    è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼å¹¶åŠ è½½ OHLCV æ•°æ®ï¼ˆCSV æˆ– Parquetï¼‰

    ä¼˜å…ˆä½¿ç”¨ Parquet æ ¼å¼ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

    Parameters
    ----------
    file_path : Union[str, Path]
        æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆ.csv æˆ– .parquetï¼‰
    start_date : str | None, optional
        å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    end_date : str | None, optional
        ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
    use_cache : bool, optional
        æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ True
    **kwargs
        ä¼ é€’ç»™å…·ä½“åŠ è½½å‡½æ•°çš„é¢å¤–å‚æ•°

    Returns
    -------
    pd.DataFrame
        åŠ è½½çš„ OHLCV æ•°æ®

    Raises
    ------
    DataLoadError
        å½“æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒæˆ–åŠ è½½å¤±è´¥æ—¶
    """
    file_path = Path(file_path)

    if not file_path.exists():
        raise DataLoadError(f"File not found: {file_path}")

    suffix = file_path.suffix.lower()

    if suffix == ".parquet":
        return load_ohlcv_parquet(
            file_path, start_date=start_date, end_date=end_date, use_cache=use_cache
        )
    elif suffix == ".csv":
        return load_ohlcv_csv(
            file_path, start_date=start_date, end_date=end_date, use_cache=use_cache, **kwargs
        )
    else:
        raise DataLoadError(f"Unsupported file format: {suffix}. Expected .csv or .parquet")
