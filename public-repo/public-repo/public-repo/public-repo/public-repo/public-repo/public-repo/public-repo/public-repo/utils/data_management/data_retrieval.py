"""
æ•°æ®è·å–å·¥å…·æ¨¡å—

ç»Ÿä¸€çš„æ•°æ®è·å–æ¥å£ï¼Œæ”¯æŒ OHLCVã€OI å’Œ Funding Rate æ•°æ®ã€‚
"""

import logging
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Literal

import ccxt
import pandas as pd
from tqdm import tqdm

from backtest.tui_manager import get_tui, is_tui_enabled
from core.adapter import DataConfig
from utils.network import retry_fetch
from utils.symbol_parser import parse_timeframe, resolve_symbol_and_type
from utils.time_helpers import get_ms_timestamp

logger = logging.getLogger(__name__)

# ç½‘ç»œè¯·æ±‚é…ç½®
MAX_ITERATIONS = 1000  # æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯

# å¹¶å‘ä¸‹è½½é…ç½®
# å¯é€šè¿‡ç¯å¢ƒå˜é‡ NAUTILUS_MAX_WORKERS æ§åˆ¶å¹¶å‘æ•°é‡
# é»˜è®¤å€¼: 10 (é€‚åˆå¤§å¤šæ•°ç½‘ç»œç¯å¢ƒ)
# å»ºè®®èŒƒå›´: 5-20 (è¿‡é«˜å¯èƒ½è§¦å‘äº¤æ˜“æ‰€ API é™æµ)
DEFAULT_MAX_WORKERS = int(os.getenv("NAUTILUS_MAX_WORKERS", "10"))


def fetch_ohlcv_data(exchange, symbol, timeframe, since, limit_ms, source="auto"):
    """
    è·å–OHLCVæ•°æ®ï¼Œæ”¯æŒå¤šæ•°æ®æº

    Args:
        source: æ•°æ®æº (ccxt/auto)ï¼Œautoä¼šåœ¨ccxtå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å…¶ä»–æº
    """
    if source == "auto":
        try:
            return _fetch_ohlcv_ccxt(exchange, symbol, timeframe, since, limit_ms)
        except (IOError, ValueError, KeyError, ConnectionError) as e:
            logger.warning(f"CCXT fetch failed for {symbol}, falling back to DataFetcher: {e}")
            from .data_fetcher import DataFetcher

            fetcher = DataFetcher()
            df = fetcher.fetch_ohlcv(symbol, timeframe, since, limit_ms, source="auto")
            return df[df["timestamp"] < limit_ms] if not df.empty else df
    else:
        return _fetch_ohlcv_ccxt(exchange, symbol, timeframe, since, limit_ms)


def _get_exchange_limit_param(exchange) -> int:
    """è·å–äº¤æ˜“æ‰€ç‰¹å®šçš„æ•°æ®é™åˆ¶å‚æ•°"""
    return 100 if exchange.id == "okx" else 1000


def _filter_new_ohlcv_data(ohlcv: list, current_since: int) -> list:
    """è¿‡æ»¤å‡ºæ–°çš„OHLCVæ•°æ®ï¼ˆæ—¶é—´æˆ³>=current_sinceï¼‰"""
    return [x for x in ohlcv if x[0] >= current_since]


def _convert_ohlcv_to_dataframe(all_ohlcv: list, limit_ms: int) -> pd.DataFrame:
    """å°†OHLCVæ•°æ®åˆ—è¡¨è½¬æ¢ä¸ºDataFrameå¹¶è¿‡æ»¤æ—¶é—´èŒƒå›´"""
    if not all_ohlcv:
        return pd.DataFrame()
    df = pd.DataFrame(all_ohlcv, columns=["timestamp", "open", "high", "low", "close", "volume"])
    return df[df["timestamp"] < limit_ms]


def _fetch_ohlcv_ccxt(exchange, symbol, timeframe, since, limit_ms):
    all_ohlcv = []
    current_since = since
    iteration_count = 0

    while current_since < limit_ms and iteration_count < MAX_ITERATIONS:
        iteration_count += 1
        limit_param = _get_exchange_limit_param(exchange)
        ohlcv = retry_fetch(
            exchange.fetch_ohlcv,
            symbol,
            timeframe,
            since=current_since,
            limit=limit_param,
        )

        if not ohlcv:
            break

        new_data = _filter_new_ohlcv_data(ohlcv, current_since)
        if not new_data:
            break

        all_ohlcv.extend(new_data)
        current_since = all_ohlcv[-1][0] + 1

        if current_since >= limit_ms:
            break

        time.sleep(exchange.rateLimit / 1000)

    if iteration_count >= MAX_ITERATIONS:
        logger.warning(f"Reached max iterations ({MAX_ITERATIONS}) for {symbol}")

    return _convert_ohlcv_to_dataframe(all_ohlcv, limit_ms)


def _fetch_single_symbol(
    raw_symbol: str,
    exchange_id: str,
    timeframe: str,
    start_ms: int,
    end_ms: int,
    base_dir: Path,
    start_date: str,
    end_date: str,
    agg,
    period: int,
    source: str,
    is_single: bool,
) -> tuple:
    """
    è·å–å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®

    Returns:
        (DataConfig | None, status): DataConfigå¯¹è±¡å’ŒçŠ¶æ€ä¿¡æ¯
        status: 'fetched', 'cached', 'skipped', 'error'
    """
    # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„ exchange å®ä¾‹ä»¥é¿å…çº¿ç¨‹å®‰å…¨é—®é¢˜
    exchange_class = getattr(ccxt, exchange_id.lower())
    exchange = exchange_class({"enableRateLimit": True, "options": {"defaultType": "future"}})

    try:
        ccxt_symbol, market_type = resolve_symbol_and_type(raw_symbol)
    except Exception as e:
        tui = get_tui()
        if is_tui_enabled():
            tui.add_log(f"Failed to parse symbol {raw_symbol}: {e}, skipping", "WARNING")
        else:
            logger.warning(f"Failed to parse symbol {raw_symbol}: {e}, skipping")
        return None, "skipped"

    safe_symbol = raw_symbol.replace("/", "")
    filename = f"{exchange.id}-{safe_symbol}-{timeframe}-{start_date}_{end_date}.csv"
    relative_path = f"{safe_symbol}/{filename}"
    output_path = base_dir / "data" / "raw" / relative_path

    # æ£€æŸ¥ç¼“å­˜
    if output_path.exists() and output_path.stat().st_size > 10 * 1024:
        config = DataConfig(
            csv_file_name=relative_path,
            bar_aggregation=agg,
            bar_period=period,
            label="main" if is_single else "universe",
        )
        return config, "cached"

    # ä¸‹è½½æ•°æ®
    try:
        df = fetch_ohlcv_data(exchange, ccxt_symbol, timeframe, start_ms, end_ms, source=source)
        if not df.empty:
            df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms")
            output_path.parent.mkdir(parents=True, exist_ok=True)
            df.to_csv(output_path, index=False)

            config = DataConfig(
                csv_file_name=relative_path,
                bar_aggregation=agg,
                bar_period=period,
                label="main" if is_single else "universe",
            )
            return config, "fetched"
        else:
            return None, "error"
    except Exception as e:
        logger.warning(f"Error fetching {raw_symbol}: {e}")
        return None, "error"


def batch_fetch_ohlcv(
    symbols: List[str],
    start_date: str,
    end_date: str,
    timeframe: str,
    exchange_id: str,
    base_dir: Path,
    extra: bool = False,
    source: str = "auto",
    max_workers: int = None,
) -> List[DataConfig]:
    """
    æ‰¹é‡æŠ“å– OHLCV æ•°æ®å¹¶è¿”å› DataConfig åˆ—è¡¨

    Args:
        max_workers: å¹¶å‘ä¸‹è½½çš„æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡ NAUTILUS_MAX_WORKERS è¯»å–ï¼ˆé»˜è®¤10ï¼‰
    """
    if max_workers is None:
        max_workers = DEFAULT_MAX_WORKERS

    start_ms = get_ms_timestamp(start_date)
    end_ms = get_ms_timestamp(end_date)
    agg, period = parse_timeframe(timeframe)

    configs = []
    total = len(symbols)
    fetched_count = 0
    cached_count = 0
    skipped_count = 0
    is_single = len(symbols) == 1

    tui = get_tui()
    use_tui = is_tui_enabled()

    if use_tui:
        # ä½¿ç”¨ TUI æ˜¾ç¤ºè¿›åº¦
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_symbol = {
                executor.submit(
                    _fetch_single_symbol,
                    raw_symbol,
                    exchange_id,
                    timeframe,
                    start_ms,
                    end_ms,
                    base_dir,
                    start_date,
                    end_date,
                    agg,
                    period,
                    source,
                    is_single,
                ): raw_symbol
                for raw_symbol in symbols
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_symbol):
                raw_symbol = future_to_symbol[future]
                try:
                    config, status = future.result()
                    if config:
                        configs.append(config)

                    if status == "fetched":
                        fetched_count += 1
                    elif status == "cached":
                        cached_count += 1
                    elif status == "skipped":
                        skipped_count += 1

                    tui.update_progress(description=f"Downloading {raw_symbol}...")
                    tui.update_stat("fetched", fetched_count)
                    tui.update_stat("cached", cached_count)
                    tui.update_stat("skipped", skipped_count)
                except Exception as e:
                    logger.error(f"Error processing {raw_symbol}: {e}")
                    tui.add_log(f"Error: {raw_symbol}", "ERROR")
                    skipped_count += 1
                    tui.update_stat("skipped", skipped_count)
    else:
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰
        with tqdm(
            total=total,
            desc="ğŸ“¥ Fetching data",
            unit="symbol",
            ncols=80,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]",
        ) as pbar:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
                future_to_symbol = {
                    executor.submit(
                        _fetch_single_symbol,
                        raw_symbol,
                        exchange_id,
                        timeframe,
                        start_ms,
                        end_ms,
                        base_dir,
                        start_date,
                        end_date,
                        agg,
                        period,
                        source,
                        is_single,
                    ): raw_symbol
                    for raw_symbol in symbols
                }

                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in as_completed(future_to_symbol):
                    raw_symbol = future_to_symbol[future]
                    try:
                        config, status = future.result()
                        if config:
                            configs.append(config)

                        if status == "fetched":
                            fetched_count += 1
                        elif status == "cached":
                            cached_count += 1
                        elif status == "skipped":
                            skipped_count += 1

                        pbar.set_postfix_str(f"{raw_symbol}", refresh=True)
                        pbar.update(1)
                    except Exception as e:
                        logger.error(f"Error processing {raw_symbol}: {e}")
                        skipped_count += 1
                        pbar.update(1)

    logger.info(
        f"Data retrieval complete: {len(configs)}/{total} symbols "
        f"(fetched: {fetched_count}, cached: {cached_count}, skipped: {skipped_count})"
    )

    return configs


def _build_binance_oi_params(symbol: str, period: str) -> dict:
    """æ„å»ºBinance OI APIè¯·æ±‚å‚æ•°"""
    return {
        "symbol": symbol.replace("/", "").replace(":USDT", ""),
        "period": period,
        "limit": 500,
    }


def _parse_binance_oi_item(item: dict, start_ms: int, end_ms: int) -> dict:
    """è§£æå•ä¸ªBinance OIæ•°æ®é¡¹"""
    ts = int(item["timestamp"])
    if start_ms <= ts <= end_ms:
        return {
            "timestamp": ts,
            "open_interest": float(item["sumOpenInterest"]),
            "open_interest_value": float(item["sumOpenInterestValue"]),
        }
    return None


def _log_binance_oi_data_range(all_data: list, start_ms: int):
    """è®°å½•Binance OIæ•°æ®èŒƒå›´ä¿¡æ¯"""
    from datetime import datetime

    oldest_ts = min(item["timestamp"] for item in all_data)
    newest_ts = max(item["timestamp"] for item in all_data)

    logger.info(
        f"â„¹ï¸  Binance OI: Got {len(all_data)} records from "
        f"{datetime.fromtimestamp(oldest_ts / 1000).date()} to "
        f"{datetime.fromtimestamp(newest_ts / 1000).date()}"
    )

    if oldest_ts > start_ms:
        logger.warning(
            f"âš ï¸  Warning: Requested data from {datetime.fromtimestamp(start_ms / 1000).date()}, "
            f"but API only provides recent ~21 days"
        )


def _finalize_binance_oi_dataframe(all_data: list) -> pd.DataFrame:
    """å°†Binance OIæ•°æ®åˆ—è¡¨è½¬æ¢ä¸ºDataFrame"""
    if not all_data:
        return pd.DataFrame()

    df = pd.DataFrame(all_data)
    df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms")
    return df


def fetch_binance_oi_history(
    exchange: ccxt.binance,
    symbol: str,
    start_ms: int,
    end_ms: int,
    period: str = "1h",
) -> pd.DataFrame:
    """è·å– Binance æ°¸ç»­åˆçº¦æŒä»“é‡å†å²æ•°æ®"""
    all_data = []

    try:
        params = _build_binance_oi_params(symbol, period)
        response = retry_fetch(
            exchange.fapiDataGetOpenInterestHist,
            params=params,
        )

        if not response:
            logger.warning(f"âš ï¸  Binance OI API: No data returned for {symbol}")
            return pd.DataFrame()

        for item in response:
            parsed_item = _parse_binance_oi_item(item, start_ms, end_ms)
            if parsed_item:
                all_data.append(parsed_item)

        if all_data:
            _log_binance_oi_data_range(all_data, start_ms)

    except Exception as e:
        logger.error(f"âŒ Error fetching Binance OI data: {e}")
        return pd.DataFrame()

    return _finalize_binance_oi_dataframe(all_data)


def _build_okx_oi_params(inst_id: str, period: str, current_end: int) -> dict:
    """æ„å»ºOKX OI APIè¯·æ±‚å‚æ•°"""
    return {
        "instId": inst_id,
        "period": period,
        "end": str(current_end),
        "limit": "100",
    }


def _parse_okx_oi_item(item: dict, start_ms: int) -> dict:
    """è§£æå•ä¸ªOKX OIæ•°æ®é¡¹"""
    ts = int(item["ts"])
    if ts >= start_ms:
        return {
            "timestamp": ts,
            "open_interest": float(item["oi"]),
            "open_interest_value": float(item["oiVol"]),
        }
    return None


def _process_okx_oi_response(response: dict, start_ms: int) -> tuple[list, bool, int]:
    """
    å¤„ç†OKX OI APIå“åº”

    Returns:
        tuple: (æ•°æ®åˆ—è¡¨, æ˜¯å¦ç»§ç»­, ä¸‹ä¸€ä¸ªç»“æŸæ—¶é—´æˆ³)
    """
    if not response or "data" not in response:
        return [], False, 0

    data_list = response["data"]
    if not data_list:
        return [], False, 0

    parsed_data = []
    for item in data_list:
        parsed_item = _parse_okx_oi_item(item, start_ms)
        if parsed_item:
            parsed_data.append(parsed_item)

    should_continue = len(data_list) >= 100
    next_end = int(data_list[-1]["ts"]) - 1 if should_continue else 0

    return parsed_data, should_continue, next_end


def _finalize_okx_oi_dataframe(all_data: list) -> pd.DataFrame:
    """å°†OKX OIæ•°æ®åˆ—è¡¨è½¬æ¢ä¸ºDataFrame"""
    if not all_data:
        return pd.DataFrame()

    df = pd.DataFrame(all_data)
    df = df.sort_values("timestamp").reset_index(drop=True)
    df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms")
    return df


def fetch_okx_oi_history(
    exchange: ccxt.okx, symbol: str, start_ms: int, end_ms: int, period: str = "1H"
) -> pd.DataFrame:
    """è·å– OKX æ°¸ç»­åˆçº¦æŒä»“é‡å†å²æ•°æ®"""
    all_data = []
    current_end = end_ms
    iteration_count = 0
    inst_id = symbol.replace("/", "-")

    while current_end > start_ms and iteration_count < MAX_ITERATIONS:
        iteration_count += 1
        try:
            params = _build_okx_oi_params(inst_id, period, current_end)
            response = retry_fetch(
                exchange.publicGetRubikStatContractsOpenInterestHistory,
                params=params,
            )

            parsed_data, should_continue, next_end = _process_okx_oi_response(response, start_ms)
            all_data.extend(parsed_data)

            if not should_continue:
                break

            current_end = next_end
            time.sleep(exchange.rateLimit / 1000)

        except Exception as e:
            logger.error(f"Warning: Error fetching OKX OI data: {e}")
            break

    if iteration_count >= MAX_ITERATIONS:
        logger.warning(f"Reached max iterations ({MAX_ITERATIONS}) for OKX OI {symbol}")

    return _finalize_okx_oi_dataframe(all_data)


def _build_binance_funding_params(clean_symbol: str, current_start: int, end_ms: int) -> dict:
    """æ„å»ºBinance Funding Rate APIè¯·æ±‚å‚æ•°"""
    return {
        "symbol": clean_symbol,
        "startTime": current_start,
        "endTime": end_ms,
        "limit": 1000,
    }


def _parse_binance_funding_item(item: dict) -> dict:
    """è§£æå•ä¸ªBinance Funding Rateæ•°æ®é¡¹"""
    return {
        "timestamp": int(item["fundingTime"]),
        "funding_rate": float(item["fundingRate"]),
    }


def _calculate_next_funding_start(all_data: list) -> int:
    """è®¡ç®—ä¸‹ä¸€æ¬¡èµ„é‡‘è´¹ç‡æŸ¥è¯¢çš„èµ·å§‹æ—¶é—´ï¼ˆ8å°æ—¶åï¼‰"""
    return all_data[-1]["timestamp"] + 8 * 3600 * 1000


def _finalize_binance_funding_dataframe(all_data: list) -> pd.DataFrame:
    """å°†Binance Funding Rateæ•°æ®åˆ—è¡¨è½¬æ¢ä¸ºDataFrame"""
    if not all_data:
        return pd.DataFrame()

    df = pd.DataFrame(all_data)
    df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms")
    df["funding_rate_annual"] = df["funding_rate"] * 3 * 365 * 100
    return df


def fetch_binance_funding_rate_history(
    exchange: ccxt.binance, symbol: str, start_ms: int, end_ms: int
) -> pd.DataFrame:
    """è·å– Binance æ°¸ç»­åˆçº¦èµ„é‡‘è´¹ç‡å†å²æ•°æ®"""
    all_data = []
    current_start = start_ms
    iteration_count = 0
    clean_symbol = symbol.replace("/", "").replace(":USDT", "")

    while current_start < end_ms and iteration_count < MAX_ITERATIONS:
        iteration_count += 1
        try:
            params = _build_binance_funding_params(clean_symbol, current_start, end_ms)
            response = retry_fetch(exchange.fapiPublicGetFundingRate, params=params)

            if not response:
                break

            for item in response:
                all_data.append(_parse_binance_funding_item(item))

            if len(response) < 1000:
                break

            current_start = _calculate_next_funding_start(all_data)
            time.sleep(exchange.rateLimit / 1000)

        except Exception as e:
            logger.error(f"Warning: Error fetching funding rate data: {e}")
            break

    if iteration_count >= MAX_ITERATIONS:
        logger.warning(f"Reached max iterations ({MAX_ITERATIONS}) for Binance Funding {symbol}")

    return _finalize_binance_funding_dataframe(all_data)


def _build_okx_funding_params(inst_id: str, current_end: int) -> dict:
    """æ„å»ºOKX Funding Rate APIè¯·æ±‚å‚æ•°"""
    return {
        "instId": inst_id,
        "before": str(current_end),
        "limit": "100",
    }


def _parse_okx_funding_item(item: dict, start_ms: int) -> dict:
    """è§£æå•ä¸ªOKX Funding Rateæ•°æ®é¡¹"""
    ts = int(item["fundingTime"])
    if ts >= start_ms:
        return {
            "timestamp": ts,
            "funding_rate": float(item["fundingRate"]),
            "realized_rate": float(item.get("realizedRate", 0)),
        }
    return None


def _process_okx_funding_response(response: dict, start_ms: int) -> tuple[list, bool, int]:
    """
    å¤„ç†OKX Funding Rate APIå“åº”

    Returns:
        tuple: (æ•°æ®åˆ—è¡¨, æ˜¯å¦ç»§ç»­, ä¸‹ä¸€ä¸ªç»“æŸæ—¶é—´æˆ³)
    """
    if not response or "data" not in response:
        return [], False, 0

    data_list = response["data"]
    if not data_list:
        return [], False, 0

    parsed_data = []
    for item in data_list:
        parsed_item = _parse_okx_funding_item(item, start_ms)
        if parsed_item:
            parsed_data.append(parsed_item)

    should_continue = len(data_list) >= 100
    next_end = int(data_list[-1]["fundingTime"]) - 1 if should_continue else 0

    return parsed_data, should_continue, next_end


def _finalize_okx_funding_dataframe(all_data: list) -> pd.DataFrame:
    """å°†OKX Funding Rateæ•°æ®åˆ—è¡¨è½¬æ¢ä¸ºDataFrame"""
    if not all_data:
        return pd.DataFrame()

    df = pd.DataFrame(all_data)
    df = df.sort_values("timestamp").reset_index(drop=True)
    df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms")
    df["funding_rate_annual"] = df["funding_rate"] * 3 * 365 * 100
    return df


def fetch_okx_funding_rate_history(
    exchange: ccxt.okx, symbol: str, start_ms: int, end_ms: int
) -> pd.DataFrame:
    """è·å– OKX æ°¸ç»­åˆçº¦èµ„é‡‘è´¹ç‡å†å²æ•°æ®"""
    all_data = []
    current_end = end_ms
    iteration_count = 0
    inst_id = symbol.replace("/", "-")

    while current_end > start_ms and iteration_count < MAX_ITERATIONS:
        iteration_count += 1
        try:
            params = _build_okx_funding_params(inst_id, current_end)
            response = retry_fetch(exchange.publicGetPublicFundingRateHistory, params=params)

            parsed_data, should_continue, next_end = _process_okx_funding_response(
                response, start_ms
            )
            all_data.extend(parsed_data)

            if not should_continue:
                break

            current_end = next_end
            time.sleep(exchange.rateLimit / 1000)

        except Exception as e:
            logger.error(f"Warning: Error fetching OKX funding rate data: {e}")
            break

    if iteration_count >= MAX_ITERATIONS:
        logger.warning(f"Reached max iterations ({MAX_ITERATIONS}) for OKX Funding {symbol}")

    return _finalize_okx_funding_dataframe(all_data)


def _initialize_exchange(exchange_id: str):
    """åˆå§‹åŒ–äº¤æ˜“æ‰€è¿æ¥"""
    exchange_class = getattr(ccxt, exchange_id.lower())
    exchange = exchange_class({"enableRateLimit": True, "options": {"defaultType": "swap"}})
    exchange.load_markets()
    return exchange


def _resolve_and_validate_symbol(
    raw_symbol: str, exchange, exchange_id: str
) -> tuple[str, str, str] | None:
    """è§£æå¹¶éªŒè¯äº¤æ˜“å¯¹ç¬¦å·"""
    try:
        ccxt_symbol, market_type = resolve_symbol_and_type(raw_symbol)
    except Exception as e:
        tui = get_tui()
        if is_tui_enabled():
            tui.add_log(f"Failed to parse symbol {raw_symbol}: {e}, skipping", "WARNING")
        else:
            logger.warning(f"\n[WARNING] Failed to parse symbol {raw_symbol}: {e}, skipping")
        return None

    if ccxt_symbol not in exchange.markets:
        logger.warning(f"\n[WARNING] {raw_symbol} not found in {exchange_id.upper()}, skipping")
        return None

    safe_symbol = raw_symbol.replace("/", "")
    return ccxt_symbol, market_type, safe_symbol


def _fetch_oi_data(
    exchange_id: str, exchange, ccxt_symbol: str, start_ms: int, end_ms: int, oi_period: str
):
    """è·å–OIæ•°æ®"""
    if exchange_id == "binance":
        return fetch_binance_oi_history(exchange, ccxt_symbol, start_ms, end_ms, oi_period)
    elif exchange_id == "okx":
        return fetch_okx_oi_history(exchange, ccxt_symbol, start_ms, end_ms, oi_period.upper())
    return None


def _fetch_funding_data(exchange_id: str, exchange, ccxt_symbol: str, start_ms: int, end_ms: int):
    """è·å–Funding Rateæ•°æ®"""
    if exchange_id == "binance":
        return fetch_binance_funding_rate_history(exchange, ccxt_symbol, start_ms, end_ms)
    elif exchange_id == "okx":
        return fetch_okx_funding_rate_history(exchange, ccxt_symbol, start_ms, end_ms)
    return None


def _save_data_to_csv(df, file_path: Path, results_key: str, results: dict):
    """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
    if not df.empty:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(file_path, index=False)
        results[results_key].append(str(file_path))


def _should_fetch_file(file_path: Path) -> bool:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦è·å–æ–‡ä»¶"""
    return not (file_path.exists() and file_path.stat().st_size > 1024)


def _process_oi_data(
    exchange_id: str,
    exchange,
    ccxt_symbol: str,
    safe_symbol: str,
    start_date: str,
    end_date: str,
    start_ms: int,
    end_ms: int,
    oi_period: str,
    base_dir: Path,
    results: dict,
    enable_oi: bool,
):
    """å¤„ç†OIæ•°æ®è·å–"""
    if not enable_oi:
        return

    oi_filename = f"{exchange_id}-{safe_symbol}-oi-{oi_period}-{start_date}_{end_date}.csv"
    oi_path = base_dir / "data" / "raw" / safe_symbol / oi_filename

    if _should_fetch_file(oi_path):
        oi_df = _fetch_oi_data(exchange_id, exchange, ccxt_symbol, start_ms, end_ms, oi_period)
        if oi_df is not None:
            _save_data_to_csv(oi_df, oi_path, "oi_files", results)


def _process_funding_data(
    exchange_id: str,
    exchange,
    ccxt_symbol: str,
    safe_symbol: str,
    start_date: str,
    end_date: str,
    start_ms: int,
    end_ms: int,
    base_dir: Path,
    results: dict,
):
    """å¤„ç†Funding Rateæ•°æ®è·å–"""
    funding_filename = f"{exchange_id}-{safe_symbol}-funding-{start_date}_{end_date}.csv"
    funding_path = base_dir / "data" / "raw" / safe_symbol / funding_filename

    if _should_fetch_file(funding_path):
        funding_df = _fetch_funding_data(exchange_id, exchange, ccxt_symbol, start_ms, end_ms)
        if funding_df is not None:
            _save_data_to_csv(funding_df, funding_path, "funding_files", results)


def _print_fetch_results(results: dict, enable_oi: bool):
    """æ‰“å°è·å–ç»“æœ"""
    if enable_oi:
        logger.info(
            f"OI/Funding data retrieval complete: "
            f"{len(results['oi_files'])} OI files, "
            f"{len(results['funding_files'])} Funding files"
        )
    else:
        logger.info(
            f"Funding data retrieval complete: "
            f"{len(results['funding_files'])} Funding files "
            f"(OI data fetching disabled)"
        )


def _fetch_single_symbol_oi_funding(
    raw_symbol: str,
    exchange_id: str,
    start_date: str,
    end_date: str,
    start_ms: int,
    end_ms: int,
    oi_period: str,
    base_dir: Path,
    enable_oi: bool,
) -> dict:
    """
    è·å–å•ä¸ªäº¤æ˜“å¯¹çš„ OI å’Œ Funding Rate æ•°æ®

    Returns:
        dict: {"oi_files": [...], "funding_files": [...]}
    """
    # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„ exchange å®ä¾‹
    exchange = _initialize_exchange(exchange_id)
    results = {"oi_files": [], "funding_files": []}

    symbol_info = _resolve_and_validate_symbol(raw_symbol, exchange, exchange_id)
    if symbol_info is None:
        return results

    ccxt_symbol, market_type, safe_symbol = symbol_info

    try:
        _process_oi_data(
            exchange_id,
            exchange,
            ccxt_symbol,
            safe_symbol,
            start_date,
            end_date,
            start_ms,
            end_ms,
            oi_period,
            base_dir,
            results,
            enable_oi,
        )

        _process_funding_data(
            exchange_id,
            exchange,
            ccxt_symbol,
            safe_symbol,
            start_date,
            end_date,
            start_ms,
            end_ms,
            base_dir,
            results,
        )
    except Exception as e:
        logger.warning(f"Error fetching OI/Funding for {raw_symbol}: {e}")

    return results


def batch_fetch_oi_and_funding(
    symbols: List[str],
    start_date: str,
    end_date: str,
    exchange_id: Literal["binance", "okx"] = "binance",
    base_dir: Path = Path("."),
    oi_period: str = "1h",
    max_workers: int = None,
) -> dict:
    """
    æ‰¹é‡è·å– OI å’Œ Funding Rate æ•°æ®ï¼ˆæ”¯æŒå¹¶å‘ï¼‰

    æ³¨æ„ï¼šOI æ•°æ®è·å–å·²ä¸´æ—¶ç¦ç”¨
    åŸå› ï¼šç¬¬ä¸‰æ–¹ API ä»…æä¾› 21 å¤©å†å²æ•°æ®ï¼Œä¸ç¬¦åˆé•¿æœŸå›æµ‹éœ€æ±‚
    è®¡åˆ’ï¼šå°†é€šè¿‡æœ¬åœ°æœåŠ¡æŒç»­é‡‡é›†å’Œå­˜å‚¨ OI æ•°æ®

    Args:
        max_workers: å¹¶å‘ä¸‹è½½çš„æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡ NAUTILUS_MAX_WORKERS è¯»å–ï¼ˆé»˜è®¤10ï¼‰
    """
    if max_workers is None:
        max_workers = DEFAULT_MAX_WORKERS

    ENABLE_OI_FETCH = False

    start_ms = get_ms_timestamp(start_date)
    end_ms = get_ms_timestamp(end_date)
    results = {"oi_files": [], "funding_files": []}

    desc = (
        f"ğŸ“Š Fetching Funding from {exchange_id.upper()}"
        if not ENABLE_OI_FETCH
        else f"ğŸ“Š Fetching OI/Funding from {exchange_id.upper()}"
    )

    with tqdm(total=len(symbols), desc=desc, unit="symbol") as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_symbol = {
                executor.submit(
                    _fetch_single_symbol_oi_funding,
                    raw_symbol,
                    exchange_id,
                    start_date,
                    end_date,
                    start_ms,
                    end_ms,
                    oi_period,
                    base_dir,
                    ENABLE_OI_FETCH,
                ): raw_symbol
                for raw_symbol in symbols
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_symbol):
                raw_symbol = future_to_symbol[future]
                try:
                    symbol_results = future.result()
                    results["oi_files"].extend(symbol_results["oi_files"])
                    results["funding_files"].extend(symbol_results["funding_files"])

                    pbar.set_postfix_str(f"{raw_symbol}", refresh=True)
                    pbar.update(1)
                except Exception as e:
                    logger.error(f"Error processing {raw_symbol}: {e}")
                    pbar.update(1)

    _print_fetch_results(results, ENABLE_OI_FETCH)
    return results
